[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R ile Veri Dönüşümü",
    "section": "",
    "text": "Önsöz\nEkonomik ve sosyal kalkınmayı teşvik etmek amacıyla uygulanan devlet yardımları, kamu kaynaklarının etkin ve verimli kullanımını sağlamayı hedefleyen kritik politika araçlarıdır. Ancak bu yardımların gerçekten beklenen etkileri yaratıp yaratmadığını belirlemek, yalnızca sağlanan desteklerin miktarını değerlendirmekle değil, bu desteklerin ekonomi ve toplum üzerindeki somut sonuçlarını bilimsel yöntemlerle ölçmekle mümkündür. Etki değerlendirme yöntemleri, bu doğrultuda karar alıcıların bilinçli ve veri temelli politikalar geliştirmesine olanak sağlayan güçlü bir enstrüman olarak öne çıkmaktadır.\nBu çalışma, devlet yardımlarının etki değerlendirme süreçlerinde kullanılan yöntemlere teorik ve uygulamalı bir temel sunmayı amaçlamaktadır. Etki değerlendirme süreçlerinin güvenilir ve tutarlı sonuçlar üretebilmesi için iki temel unsur büyük önem taşımaktadır:\n\nKullanılan analiz yöntemlerinin bilimsel sağlamlığı\nVeri setlerinin doğruluğu, eksiksizliği ve tutarlılığı\n\nBu bağlamda, etki değerlendirme süreci yalnızca istatistiksel analizleri içermekle kalmayıp, aynı zamanda veri manipülasyonu, veri temizleme, eksik veri yönetimi, aykırı değer analizi ve veri dönüşümleri gibi konular da süreç içinde kritik rol oynamaktadır. Eksik veya yanlış verilerle yürütülen bir analiz, politika yapıcıları yanlış yönlendirebilmekte ve kamu kaynaklarının verimsiz kullanımına neden olabilmektedir.\nSon yıllarda, etki değerlendirme alanındaki en önemli gelişmelerden biri, istatistiksel analizlerin programlama dilleri ve dinamik raporlama sistemleriyle entegre edilmesi olmuştur. Bu çerçevede, çalışmada R programlama dili ve Quarto platformu kullanılarak uygulamalı bir rehber oluşturulmuştur. R, sunduğu esneklik, geniş analiz olanakları ve güçlü veri işleme yetenekleriyle etki değerlendirme süreçlerinde yaygın olarak kullanılan bir araç haline gelmiştir. Çalışma, R içerisinde yer alan bir platform olan Quarto’nun kitap formatı özelliğiyle derlenmiş olup, hem okunması kolay bir kaynak hem de uygulamalı bir rehber olarak tasarlanmıştır.\nBu çalışma, Başkanlığımız ve diğer kamu kurumlarında yürütülen etki değerlendirme çalışmalarına destek sağlamayı hedeflemektedir. Etki değerlendirme sürecinde veri yönetimi gibi temel zorluklara yönelik çözüm önerileri sunarak, veri temelli karar alma süreçlerine katkı sağlamayı amaçlamaktadır.\nGünümüzde, etki değerlendirme yalnızca uygulanan politikaların başarısını ölçmek için değil, gelecekte uygulanacak politikaların daha iyi tasarlanması için de kritik bir rol oynamaktadır. Bu çalışma ile devlet yardımlarının etkisini etkisini değerlendirmeden önce veri yönetimi ile alakalı çalışmalara katkı sunmak hedeflemektedir.\nStrateji ve Bütçe Başkanlığı\nDevlet Yardımları Genel Müdürlüğü",
    "crumbs": [
      "Önsöz"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Giriş",
    "section": "",
    "text": "1.1 Etki Değerlendirmede Verinin Önemi\nEtki değerlendirmesi, belirli bir politika, program veya müdahalenin hedeflenen sonuçlara ulaşıp ulaşmadığını belirlemek amacıyla kullanılan sistematik bir analiz sürecidir. Kamu politikalarından sosyal programlara, ekonomik teşviklerden eğitim reformlarına kadar geniş bir yelpazede kullanılan etki değerlendirmesi, uygulanan müdahalelerin etkisini bilimsel yöntemlerle ölçerek politika yapıcıların bilinçli kararlar almasına olanak tanır.\nBu sürecin temel taşı veridir. Doğru, güvenilir ve kapsamlı veri olmadan, yapılan değerlendirmelerin bilimsel temeli zayıf kalır ve yanlış yönlendirmelere yol açabilir. Verinin toplanması, temizlenmesi, analiz edilmesi ve yorumlanması, etki değerlendirme süreçlerinin başarılı bir şekilde yürütülmesini sağlar.\nÖrneğin, bir devlet destek programının istihdam üzerindeki etkisini ölçmek için destek alan ve almayan firmaların karşılaştırılması gerekir. Ancak, bu firmalara ait eksik veya yanlış veriler kullanılırsa, programın gerçek etkisi olduğundan farklı yorumlanabilir ve politika yapıcılar yanlış çıkarımlar yapabilir. Benzer şekilde, eğitimde yapılan reformların öğrenci başarısı üzerindeki etkisini anlamak için kullanılan veriler eksik, hatalı veya yanlı olduğunda, değerlendirme süreci yanıltıcı hale gelebilir.\nEtki değerlendirmesinde veri, yalnızca mevcut bir politikanın değerlendirilmesi için değil, aynı zamanda gelecekte uygulanacak politikaların tasarlanması için de kritik öneme sahiptir. Politika yapıcılar, geçmiş verileri analiz ederek benzer müdahalelerin hangi koşullarda daha etkili olduğunu belirleyebilir ve gelecekte daha iyi planlanmış projeler oluşturabilir.\nBu nedenle, etki değerlendirme süreçlerinde veri toplama, işleme, temizleme ve analiz etme aşamalarının dikkatli bir şekilde yürütülmesi gerekir. Kaliteli veri olmadan yapılan değerlendirmeler, yanlış politika kararlarına ve kamu kaynaklarının verimsiz kullanımına yol açabilir.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Giriş</span>"
    ]
  },
  {
    "objectID": "intro.html#etki-değerlendirmede-verinin-önemi",
    "href": "intro.html#etki-değerlendirmede-verinin-önemi",
    "title": "1  Giriş",
    "section": "",
    "text": "1.1.1 Etki Değerlendirmesinde Verinin Rolü\nEtki değerlendirmesinde kullanılan veri, bir politikanın ya da programın öncesi ve sonrası durumlarını karşılaştırmak, neden-sonuç ilişkilerini ortaya koymak ve karar vericilere bilimsel temellere dayalı öneriler sunmak için kritik bir araçtır. Verinin doğru kullanımı sayesinde, politika yapıcılar hizmetlerin etkinliğini artırabilir, kaynakları verimli yönetebilir ve hedef grupların ihtiyaçlarını daha iyi karşılayabilir.\nÖrneğin, bir devlet destek programının istihdam üzerindeki etkisini ölçmek için iş gücü piyasası verilerine ihtiyaç vardır. Destek alan firmaların ve almayan firmaların karşılaştırılması, programın gerçekten istihdamı artırıp artırmadığını anlamamıza yardımcı olur.\n\n\n1.1.2 Etki Değerlendirmede Veri Türleri ve Kaynakları\nEtki değerlendirmesi için kullanılan veriler farklı kaynaklardan ve formatlardan gelebilir. Başlıca veri türleri şunlardır:\n\n1.1.2.1 Kesitsel ve Panel Veri\n\nKesitsel Veri: Tek bir zaman dilimindeki bireyleri, firmaları veya bölgeleri analiz eder.\nPanel Veri: Aynı birimleri farklı zaman dilimlerinde gözlemleyerek zaman içindeki değişimleri analiz etmeye olanak tanır.\n\n\n\n1.1.2.2 Nicel ve Nitel Veri\n\nNicel Veriler: Sayısal olarak ifade edilen, ölçülebilir değişkenlerdir (örneğin, gelir, istihdam oranı, üretim miktarı).\nNitel Veriler: Katılımcı görüşleri, anket yanıtları veya mülakat sonuçları gibi kalitatif bilgileri içerir.\n\n\n\n\n1.1.3 Etki Değerlendirme Modelleri ve Verinin Kullanımı\nEtki değerlendirme sürecinde kullanılan yöntemler, verinin nasıl toplandığını ve analiz edildiğini belirler. Kullanılan yöntemler, verinin zaman içindeki değişimini ve müdahalelerin etkisini anlamamıza yardımcı olur.\n\nDifference-in-Differences (DiD): Müdahale öncesi ve sonrası verileri karşılaştırarak etkiyi belirler.\nPropensity Score Matching (PSM): Müdahale alan ve almayan bireyleri benzer özelliklere göre eşleştirerek daha doğru karşılaştırmalar yapar.\nInstrumental Variables (IV): Nedenselliği ölçmek için kullanılan yöntemlerden biridir.\n\nBu yöntemlerin sağlıklı çalışabilmesi için doğru veri toplama, temizleme ve analiz süreçlerinin uygulanması gerekmektedir.\n\n\n1.1.4 Neden Veri Olmazsa Etki Değerlendirme Yapılamaz?\nEtki değerlendirmesi veriye dayalı bir süreçtir ve doğru kararlar alabilmek için güvenilir, eksiksiz ve analiz edilebilir veri setlerine ihtiyaç vardır. Eğer veri eksik, hatalı veya yanlı ise, politika değerlendirmeleri yanlış sonuçlar verebilir ve bu da yanlış politika kararlarına yol açabilir.\nVeri olmadan etki değerlendirme yapılamaz çünkü:\n\nMüdahalenin etkisini ölçmek için karşılaştırmalar yapılamaz.\nPolitika önerileri bilimsel dayanağa sahip olmaz.\nKaynakların verimli kullanımı garanti edilemez.\nSonuçların doğruluğu sorgulanır ve politika yapıcılar yanlış yönlendirilebilir.\n\nBu nedenle, veri toplama, temizleme, analiz etme ve raporlama süreçlerinin doğru yönetilmesi, etkili bir etki değerlendirmesi için kritik öneme sahiptir.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Giriş</span>"
    ]
  },
  {
    "objectID": "intro.html#etki-değerlendirmede-veri-seti-nasıl-olmalı",
    "href": "intro.html#etki-değerlendirmede-veri-seti-nasıl-olmalı",
    "title": "1  Giriş",
    "section": "1.2 Etki Değerlendirmede Veri Seti Nasıl Olmalı?",
    "text": "1.2 Etki Değerlendirmede Veri Seti Nasıl Olmalı?\nEtki değerlendirmesi, belirli bir politika, program veya müdahalenin beklenen sonuçları üretip üretmediğini anlamak için kullanılan analitik bir süreçtir. Bu sürecin başarısı, kullanılan veri setinin doğruluğuna, kapsamına ve analiz için uygunluğuna bağlıdır. Eğer veri seti eksik, tutarsız veya hatalı ise, analizler yanlış yönlendirme yapabilir ve alınan politika kararları gerçek durumu yansıtmayabilir.\nİyi bir etki değerlendirme veri seti, yalnızca mevcut bir politikayı veya müdahaleyi analiz etmekle kalmaz, aynı zamanda gelecekteki karar süreçlerine de rehberlik eder. Doğru yapılandırılmış bir veri seti, karar alıcıların ve araştırmacıların müdahale öncesi ve sonrası durumları kıyaslamasına, neden-sonuç ilişkilerini analiz etmesine ve politika önerilerini bilimsel temellere dayandırmasına olanak tanır.\nAncak, etki değerlendirme sürecinde kullanılan veri setleri çeşitli eksiklikler, hatalar ve metodolojik zorluklar içerebilir. Eksik veriler, aykırı değerler, yanlış kodlanmış değişkenler, zaman uyumsuzluğu ve örneklem seçim yanlılığı gibi sorunlar analizlerin güvenilirliğini ciddi şekilde etkileyebilir. Bu nedenle, veri setinin etki değerlendirmesi için hazırlanma süreci titizlikle yürütülmelidir.\nBu bölümde, etki değerlendirme için ideal bir veri setinin nasıl olması gerektiği, hangi bileşenleri içermesi gerektiği ve hangi kalite standartlarını karşılaması gerektiği ayrıntılı olarak ele alınacaktır. Ayrıca, etki değerlendirme yöntemleriyle uyumlu veri yapılarının nasıl oluşturulması gerektiği ve veri setlerinin güvenilir analizler için nasıl hazırlanması gerektiği açıklanacaktır.\n\n1.2.1 Veri Setinin Temel Yapısı\nEtki değerlendirmesi için kullanılan veri setleri genellikle gözlemler (satırlar) ve değişkenlerden (sütunlar) oluşur. Her satır, bir birey, firma, bölge veya politika müdahalesine tabi olan bir birimi temsil eder. Her sütun ise bu birime ilişkin farklı değişkenleri içerir.\nVeri setinde şu temel bileşenler bulunmalıdır:\n\nKimlik Bilgileri (ID değişkenleri): Özniteliklerin eşleşmesini sağlayan benzersiz bir kimlik numarası (örn. birey ID, firma ID, bölge kodu vb.).\nBağımlı Değişken (Sonuç Değişkeni): Politika veya müdahalenin etkilediği değişken (örn. istihdam, gelir, eğitim seviyesi vb.).\nBağımsız Değişkenler: Etkiyi belirleyen veya kontrol değişkeni olarak kullanılan faktörler (örn. yaş, sektör, hanehalkı büyüklüğü, ekonomik göstergeler vb.).\nMüdahale Değişkeni (Tedavi/Müdahale Değişkeni): Müdahaleye maruz kalıp kalmadığını gösteren değişken (örn. 1 = müdahale aldı, 0 = almadı).\nZaman Değişkeni: Etki değerlendirmesi zaman serileri veya panel veri ile yapılıyorsa tarih veya yıl değişkeni.\n\nÖrnek bir etki değerlendirme veri seti (Panel veri yapısında):\n\n\n\nID\nYıl\nMüdahale\nGelir\nEğitim Yılı\nSektör\nBölge\n\n\n\n\n101\n2018\n0\n5000\n12\nHizmet\nİstanbul\n\n\n101\n2019\n1\n6000\n12\nHizmet\nİstanbul\n\n\n102\n2018\n0\n4500\n10\nTarım\nAnkara\n\n\n102\n2019\n1\n5500\n10\nTarım\nAnkara\n\n\n\nBu yapıda müdahale değişkeni, bireyin politikadan önce ve sonra nasıl değiştiğini anlamamıza yardımcı olur.\n\n\n1.2.2 Veri Setinin Kalite Kriterleri\nEtki değerlendirme veri setlerinin yüksek kaliteli olması için şu kriterlere uyması gerekir:\n\nEksiksiz ve Tutarlı Olmalı: Tüm birimler ve zaman dilimleri için eksiksiz veri içermeli, tutarsız girişlerden kaçınılmalıdır.\nTemizlenmiş Olmalı: Eksik veriler tamamlanmalı, aykırı değerler kontrol edilmeli ve veriler doğru formatta saklanmalıdır.\nTekilleştirilmiş Olmalı: Aynı birime ait birden fazla satır veya çakışan kayıtlar olmamalıdır.\nStandartlaştırılmış Olmalı: Değişken isimleri ve formatları tutarlı olmalı, aynı türdeki veriler uyumlu formatta olmalıdır (örn. tarih formatı, kategorik değişkenlerin kodlanması).\nAnaliz İçin Uygun Olmalı: Kullanılacak yöntem (Difference-in-Differences, Propensity Score Matching vb.) için gerekli değişkenleri içermelidir.\n\n\n\n1.2.3 Veri Setinin Türü ve Yapısı\nEtki değerlendirmesi için kullanılan veri setleri genellikle iki ana yapıya sahiptir:\n\n1.2.3.1 Kesitsel Veri (Cross-Sectional Data)\n\nTek bir zaman noktasındaki bireyleri veya firmaları içerir.\nPolitikaların kısa vadeli etkilerini ölçmek için uygundur.\nÖrnek: 2023 yılı için girişimcilik destek programına katılan firmalar ve katılmayan firmaların performansları.\n\nÖrnek Kesitsel Veri Seti:\n\n\n\nFirma ID\nMüdahale\nGelir\nÇalışan Sayısı\nSektör\n\n\n\n\n201\n1\n75000\n15\nÜretim\n\n\n202\n0\n60000\n12\nTicaret\n\n\n\n\n\n1.2.3.2 Panel Veri (Panel Data)\n\nAynı bireyleri veya firmaları farklı zaman dilimlerinde içerir.\nDifference-in-Differences (DiD) gibi yöntemler için gereklidir.\nPolitikanın uzun vadeli etkisini incelemek için uygundur.\n\nÖrnek Panel Veri Seti (Zaman içindeki değişimi gösterir):\n\n\n\nFirma ID\nYıl\nMüdahale\nGelir\nÇalışan Sayısı\n\n\n\n\n201\n2021\n0\n50000\n10\n\n\n201\n2022\n1\n60000\n12\n\n\n202\n2021\n0\n55000\n11\n\n\n202\n2022\n0\n58000\n11\n\n\n\nPanel veri kullanıldığında müdahaleden önce ve sonra bireylerin/firma performanslarının nasıl değiştiği karşılaştırılabilir.\n\n\n\n1.2.4 Etki Değerlendirme Yöntemleri için Veri Yapıları\nEtki değerlendirmesinde kullanılan yöntemlere göre veri setinin özellikleri değişebilir:\n\nPropensity Score Matching (PSM) → Müdahale ve karşılaştırma grubu arasında benzer özelliklere sahip gözlemler bulunmalı.\nDifference-in-Differences (DiD) → Zaman içinde gözlemlenen aynı bireyler veya firmalar olmalı.\nRegression Discontinuity (RD) → Kesikli bir değişken üzerinden belirlenen bir eşik değeri ile müdahale alan ve almayan grupların ayrımı yapılmalı.\n\nHer yöntem için veri setinin doğru formatta olması analiz sürecinin güvenilirliği açısından kritik öneme sahiptir.\n\n\n1.2.5 Etki Değerlendirme için İdeal Veri Seti\nEtki değerlendirme veri seti, aşağıdaki özellikleri taşımalıdır:\n\nZengin Değişken Yapısı: Etkiyi ölçmek için bağımlı değişken, müdahale değişkeni ve kontrol değişkenleri içermelidir.\n\nDoğru Veri Formatı: Panel veri veya kesitsel veri analiz yöntemine uygun şekilde düzenlenmelidir.\n\nEksiksiz ve Temiz: Eksik veriler doldurulmalı, aykırı değerler düzeltilmeli ve tutarsız girişler kaldırılmalıdır.\n\nZaman Boyutunu İçermeli: Müdahale öncesi ve sonrası gözlemler mümkünse bulunmalıdır.\n\nÖzetle, iyi yapılandırılmış, eksiksiz ve doğru bir veri seti, etki değerlendirme süreçlerinin güvenilirliğini artırır ve politika yapıcılara doğru yönlendirmeler sunar.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Giriş</span>"
    ]
  },
  {
    "objectID": "intro.html#etki-değerlendirmede-veri-seti-ile-ilgili-karşılaşılan-sorunlar-ve-doğurduğu-sonuçlar",
    "href": "intro.html#etki-değerlendirmede-veri-seti-ile-ilgili-karşılaşılan-sorunlar-ve-doğurduğu-sonuçlar",
    "title": "1  Giriş",
    "section": "1.3 Etki Değerlendirmede Veri Seti ile İlgili Karşılaşılan Sorunlar ve Doğurduğu Sonuçlar",
    "text": "1.3 Etki Değerlendirmede Veri Seti ile İlgili Karşılaşılan Sorunlar ve Doğurduğu Sonuçlar\nEtki değerlendirme sürecinde kullanılan veri setleri, analizlerin doğruluğunu, geçerliliğini ve güvenilirliğini belirleyen en kritik bileşenlerden biridir. Verinin eksik, hatalı veya yanlış yapılandırılmış olması, değerlendirme sürecini yanıltıcı hale getirebilir ve politika yapıcıların yanlış sonuçlara ulaşmasına neden olabilir. Bu durum, etkisiz veya gereksiz politikaların sürdürülmesine ya da etkili bir müdahalenin gereğinden az desteklenmesine yol açabilir.\nEtki değerlendirme sürecinde karşılaşılan veri seti sorunları veri manipülasyonu, veri dönüşümü, veri şekillendirme, metin manipülasyonu ve fonksiyonlarla veri işleme aşamalarında yapılan hatalardan kaynaklanabilir. Bu aşamalarda dikkat edilmesi gereken en önemli unsurlar, veri setinin eksiksiz, tutarlı, doğru formatlanmış ve analiz için uygun yapıda olmasıdır. Aksi halde, yapılan modellemeler güvenilirliğini kaybedebilir ve yanlış politika sonuçlarına neden olabilir.\nVeri setleriyle ilgili yaşanan başlıca sorunlar; eksik veriler, aykırı değerler, örneklem seçim yanlılığı, zaman uyumsuzluğu, yanlış veri eşleşmeleri ve eksik tanımlanmış değişkenler olarak öne çıkmaktadır. Bu tür hatalar, veri analizlerinin yanıltıcı sonuçlar üretmesine, nedensellik ilişkilerinin hatalı yorumlanmasına ve yanlış politika önerilerine neden olabilir.\nBu bölümde, etki değerlendirme sürecinde veri setleriyle ilgili yaygın olarak karşılaşılan sorunlar detaylı bir şekilde ele alınacak ve bu sorunların değerlendirme süreçleri üzerindeki etkileri tartışılacaktır. Aynı zamanda, veri kalitesini artırmaya yönelik bazı temel çözümler ve iyi uygulama yöntemleri de incelenecektir.\n\n1.3.1 Veri Hazırlama, Dönüştürme ve Şekillendirme ile İlgili Genel Sorunlar\nEtki değerlendirme analizlerinde kullanılan veri setleri, ham veri olarak toplandıktan sonra çeşitli işlemlerden geçirilerek analiz edilebilir hale getirilmelidir. Ancak, yanlış veya eksik veri manipülasyonu, hatalı veri dönüşümleri, yanlış veri birleştirme işlemleri ve eksik veri temizliği gibi faktörler, sonuçların güvenilirliğini zayıflatabilir.\n\nSütun Seçimi (select()) ve Satır Filtreleme (filter()) yanlış yapıldığında, analiz için gerekli olan değişkenler yanlışlıkla veri setinden çıkarılabilir veya örneklemde önemli bir kayma yaşanabilir.\nSatırların sıralanması (arrange()), zaman serisi analizlerinde doğru yapılmazsa, etki değerlendirme sonuçları hatalı olabilir.\nYeni değişkenler oluşturulurken (mutate()) veya istatistiksel özetler hesaplanırken (summarise()) yanlış hesaplamalar, yanıltıcı bulgular üretebilir.\nVeri kaynaklarının birleştirilmesi (merge()) yanlış yapılırsa, aynı bireyler veya firmalar için tutarsız bilgiler içeren bir veri seti oluşabilir.\nMetinsel verilerin düzensizliği (yanlış kategorik değişkenler, eksik etiketler) analizleri bozabilir ve yanlış sınıflandırmalara yol açabilir.\n\nSonuçları:\n\nYanlış veri seçimi ve filtreleme, politika etkisinin yanlış hesaplanmasına neden olabilir.\nEksik veya yanlış dönüştürülen veriler, analiz sonuçlarını çarpıtabilir.\nYanlış eşleşmeler ve eksik gözlemler nedeniyle model hatalı tahminlerde bulunabilir.\nEtki değerlendirmesi hatalı yapıldığında, karar alıcılar yanlış yönlendirilebilir.\n\n\n\n1.3.2 Eksik Verilerle Çalışma ve Olası Sorunlar\nEksik veriler, etki değerlendirme süreçlerinde en yaygın karşılaşılan sorunlardan biridir. Veri setlerinde eksik değerlerin bulunması, analizlerin güvenilirliğini azaltabilir ve sonuçların yanlış yorumlanmasına yol açabilir.\n\n1.3.2.1 Eksik Verilerin Kaynakları ve Yaygın Nedenleri\nEksik veriler, farklı nedenlerden dolayı ortaya çıkabilir:\n\nYanıt Eksiklikleri: Anketlerde bazı sorulara verilen yanıtların eksik olması.\nİdari Kayıtlardaki Boşluklar: Kamu veritabanlarında bazı bireyler veya firmalar hakkında eksik bilgiler bulunması.\nÖrneklemden Kaynaklanan Eksiklikler: Belirli bir grup bireyin veya firmanın değerlendirme dışında kalması.\nZaman Uyum Sorunları: Verinin belirli dönemlerde eksik olması.\n\n\n\n1.3.2.2 Eksik Verilerin Etkileri\nEksik veriler analiz sürecinde ciddi sorunlara yol açabilir:\n\nYanlılık Oluşabilir: Eksik veriler sistematik bir biçimde belirli bir grupta toplanıyorsa (örneğin, düşük gelirli bireylerin anketleri tam doldurmaması), analizde yanlılık meydana gelebilir.\nÖrneklem Temsiliyetini Bozabilir: Veri setindeki eksik değerler nedeniyle analiz edilen örneklem, evreni tam olarak temsil etmeyebilir.\nModel Performansını Düşürebilir: Eksik değerler yanlış tahminlere yol açabilir.\n\nSonuçları:\n\nEksik veri sorunları uygun yöntemlerle ele alınmazsa, analiz sonuçları ciddi şekilde yanlı hale gelebilir.\nEksik verilerin yanlış doldurulması, model performansını düşürebilir.\nVerinin kalitesini artırmak için uygun eksik veri yöntemleri uygulanmalıdır.\n\n\n\n\n1.3.3 Aykırı Değerler ile Çalışma ve Olası Sorunlar\nAykırı değerler, veri setinde diğer gözlemlerden büyük ölçüde farklılık gösteren gözlemlerdir. Etki değerlendirme sürecinde, aykırı değerlerin varlığı analizleri yanıltıcı hale getirebilir.\n\n1.3.3.1 Aykırı Değerlerin Kaynakları\n\nÖlçüm Hataları: Veri girişlerinde hata yapılması sonucu ortaya çıkabilir.\nDoğal Aykırılıklar: Bazı bireyler veya firmalar gerçekten aşırı uç değerlere sahip olabilir.\nYanlış Veri Kodlaması: Negatif maaş değerleri gibi mantıksız veriler bulunabilir.\n\n\n\n1.3.3.2 Aykırı Değerlerin Etkileri\n\nModel Katsayılarını Çarpıtabilir: Regresyon analizinde uç değerler tahminleri bozabilir.\nOrtalama ve Standart Sapmayı Etkileyebilir: Veri setinin genel istatistikleri yanıltıcı olabilir.\nYanlış Politikaların Önerilmesine Yol Açabilir: Yanlış analiz sonuçları, etkisiz veya hatalı politikaların önerilmesine neden olabilir.\n\nSonuçları:\n\nAykırı değerler düzgün ele alınmazsa, analiz sonuçları yanıltıcı olabilir.\nYanlış politika etkileri hesaplanabilir.\nUygun istatistiksel yöntemlerle aykırı değerler kontrol edilmelidir.\n\n\n\n\n1.3.4 Veri Setinin Yanıltıcı veya Eksik Tanımlanması\nVeri setinde bazı değişkenlerin eksik tanımlanması veya yanlış şekilde kategorize edilmesi, analiz süreçlerini olumsuz etkileyebilir.\n\n1.3.4.1 Yanlış Kategorik Değişken Tanımları\n\nÖrneğin, “Bölgesel Kalkınma Programı”na dahil olan şehirlerin yanlış kategorize edilmesi, analiz sonuçlarını bozabilir.\n\n\n\n1.3.4.2 Zaman Uyum Sorunları\n\nMüdahale etkisini ölçmek için yeterli zaman geçmeden yapılan analizler, yanlış politika sonuçları çıkarılmasına neden olabilir.\n\nSonuçları:\n\nVerinin yanlış tanımlanması, analizlerin yanlış yorumlanmasına yol açabilir.\nPolitikalar hakkında hatalı çıkarımlar yapılabilir.\nEtki değerlendirme süreçlerinde veri yapısının doğru tanımlanması sağlanmalıdır.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Giriş</span>"
    ]
  },
  {
    "objectID": "intro.html#etki-değerlendirme-için-veri-setlerinin-hazır-hale-getirilmesi",
    "href": "intro.html#etki-değerlendirme-için-veri-setlerinin-hazır-hale-getirilmesi",
    "title": "1  Giriş",
    "section": "1.4 Etki Değerlendirme İçin Veri Setlerinin Hazır Hale Getirilmesi",
    "text": "1.4 Etki Değerlendirme İçin Veri Setlerinin Hazır Hale Getirilmesi\nEtki değerlendirme sürecinde kullanılan veri setlerinin doğru, eksiksiz ve analiz için uygun hale getirilmesi, elde edilecek sonuçların güvenilirliği açısından kritik bir öneme sahiptir. Veri setinde yer alan eksik gözlemler, hatalı girişler, aykırı değerler veya yanlış kodlanmış değişkenler giderilmezse, analiz sonuçları yanıltıcı olabilir ve politika yapıcıların yanlış yönlendirilmesine neden olabilir.\nEtki değerlendirme için uygun bir veri seti oluşturmak, sadece veriyi toplamakla sınırlı değildir. Verinin kalite kontrol süreçlerinden geçirilmesi, eksik veya hatalı verilerin düzeltilmesi ve analiz yöntemlerine uygun şekilde dönüştürülmesi gerekir. Eksik verilerin doldurulması, aykırı değerlerin analiz edilmesi, veri manipülasyonu ve standardizasyon gibi işlemler, veri setinin analize hazır hale getirilmesi için mutlaka uygulanmalıdır.\nEtki değerlendirme modelleri (Difference-in-Differences, Propensity Score Matching, Regression Discontinuity vb.) belirli veri yapıları gerektirdiğinden, veri setinin analiz yöntemlerine uygun hale getirilmesi de büyük önem taşır. Örneğin, panel veri kullanan Difference-in-Differences yöntemi için zaman boyutunun eksiksiz olması gerekirken, Propensity Score Matching yöntemi için müdahale ve kontrol gruplarının benzer özellikler taşıması sağlanmalıdır.\nBu nedenle, veri setlerini analize uygun hale getirmek için aşağıdaki işlemler titizlikle uygulanmalıdır:\n\nVeri manipülasyonu ve dönüştürme işlemleri (sütun seçimi, veri standardizasyonu, zaman serisi uygunluğu vb.)\nEksik verilerin belirlenmesi ve uygun şekilde doldurulması\nAykırı değerlerin tespiti ve uygun yöntemlerle düzeltilmesi\nFarklı veri kaynaklarının tutarlı hale getirilmesi ve birleştirilmesi\nDoğru değişken tanımlamaları yapılarak veri setinin analiz yöntemine uygun hale getirilmesi\n\nBu adımların her biri, analizin güvenilirliğini artırmak ve politika etkilerini doğru ölçmek için kritik rol oynamaktadır. Eksik veya hatalı veriler düzeltilmeden yapılan analizler, politika yapıcıların hatalı kararlar almasına neden olabilir. Bu bölümde, veri setinin etki değerlendirme sürecine uygun hale getirilmesi için atılması gereken temel adımlar detaylı bir şekilde ele alınacaktır.\n\n1.4.1 Veri Manipülasyonu ve Dönüştürme İşlemleri\nVeri setlerinin analiz öncesinde uygun hale getirilmesi için sütun seçimi, satır filtreleme, sıralama ve veri dönüştürme işlemlerinin doğru yapılması gerekmektedir.\n\nGereksiz Sütunların Çıkarılması: select() fonksiyonu ile analizde kullanılmayacak değişkenler veri setinden temizlenmelidir.\nHedef Grupların Seçilmesi: filter() fonksiyonu ile yalnızca analiz için gerekli bireyler veya firmalar veri setinde bırakılmalıdır.\nZaman Serisi Uygunluğu: arrange() fonksiyonu ile zaman içindeki değişimler doğru bir sıraya konulmalıdır.\nVeri Standartlaştırma ve Yeniden Kodlama: mutate() ve rename() fonksiyonları kullanılarak değişken isimleri ve kategorik değişkenler düzenlenmelidir.\n\nEtki Değerlendirme İçin Önemi:\n\nMüdahale ve kontrol gruplarının doğru tanımlanmasını sağlar.\nModelin tutarlı bir veri seti üzerinden çalışmasına olanak tanır.\nZamansal tutarsızlıkları ve yanlış filtreleme hatalarını önler.\n\n\n\n1.4.2 Eksik Verilerle Başa Çıkma ve Tamamlama Süreci\nEksik veriler, analizlerin güvenilirliğini düşüren en büyük sorunlardan biridir. Eksik veri oranı yüksekse, analiz yanlı hale gelebilir ve yanlış politika kararları alınabilir.\n\nEksik Veri Analizi: Eksik veri yüzdesi belirlenmeli ve sistematik bir desen olup olmadığı incelenmelidir. naniar, VIM gibi paketler ile eksik veri görselleştirilebilir.\nEksik Verileri Doldurma Yöntemleri:\n\nOrtalama, Medyan veya Mod ile Doldurma (Basit ama her zaman güvenilir değil)\nRegresyon Tabanlı İmputasyon\nÇoklu İmputasyon (MICE Paketi ile)\nK En Yakın Komşu (KNN) Yöntemi\n\nEksik Verilerin Yönetimi: Eğer eksik veriler belirli bir gruba özgü ise (örneğin düşük gelirli bireylerde daha fazla eksik veri olması gibi), analiz sırasında bu eksiklik dikkate alınmalıdır.\n\nEtki Değerlendirme İçin Önemi:\n\nEksik verilerin uygun şekilde doldurulması, analizlerin güvenilirliğini artırır.\nÖrneklemde sistematik veri kaybı olup olmadığı belirlenebilir.\nEksik verilerin yanlış doldurulması önlenerek modelin hatalı tahminler yapması engellenir.\n\n\n\n1.4.3 Aykırı Değerleri Tespit Etme ve Düzenleme\nAykırı değerler, analiz sonuçlarını çarpıtabilir ve yanlış politika önerilerine yol açabilir. Bu nedenle aykırı değerlerin tespit edilmesi ve uygun şekilde ele alınması gerekmektedir.\n\nAykırı Değerleri Belirleme:\n\nİstatistiksel yöntemlerle tespit edilmesi: Z-skoru, IQR yöntemi, Boxplot kullanımı.\nGörselleştirme ile inceleme: boxplot(), ggplot2 kullanımı.\n\nAykırı Değerlerle Baş Etme Yöntemleri:\n\nVeri Setinden Çıkarma: Aşırı uç değerler tamamen kaldırılabilir. Ancak dikkatli olunmalıdır, çünkü bazı aykırı değerler gerçekte politika etkisini yansıtan değerler olabilir.\nWinsorization: Aykırı değerlerin belirli bir sınıra çekilmesi.\nRobust Modeller Kullanma: Aykırı değerlere duyarlı olmayan analiz yöntemleri tercih edilmelidir.\n\n\nEtki Değerlendirme İçin Önemi:\n\nAykırı değerler kontrol edilmezse, müdahale etkisi yanlış hesaplanabilir.\nBazı grupların aşırı etkili görünüp gerçekte farklı sonuçlar doğurması önlenir.\nPolitikaların uzun vadeli etkileri daha doğru bir şekilde hesaplanır.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Giriş</span>"
    ]
  },
  {
    "objectID": "intro.html#sonuç",
    "href": "intro.html#sonuç",
    "title": "1  Giriş",
    "section": "1.5 Sonuç",
    "text": "1.5 Sonuç\nEtki değerlendirme sürecinin sağlıklı bir şekilde yürütülebilmesi, kullanılan verinin kalitesiyle doğrudan ilişkilidir. Eksik, hatalı veya düzensiz bir veri seti ile yapılan analizler, yanlış sonuçlara yol açarak politika yapıcıları yanıltabilir ve kaynakların etkisiz kullanılmasına neden olabilir. Bu nedenle, veri setlerinin manipülasyon, dönüşüm, temizleme ve bütünleştirme işlemlerinden geçirilerek analiz için uygun hale getirilmesi gerekmektedir.\nEksik verilerin uygun yöntemlerle tamamlanması, aykırı değerlerin kontrol edilmesi, zaman uyumsuzluklarının giderilmesi ve veri setinin standardize edilmesi, değerlendirme sürecinin doğruluğunu artıracaktır. Ayrıca, veri türüne ve kullanılan yönteme bağlı olarak veri setinin kesitsel veya panel veri formatında düzenlenmesi, nedensellik ilişkilerinin daha sağlıklı kurulmasına olanak tanıyacaktır.\nEtki değerlendirme süreçlerinde doğru veriyle çalışmak, müdahale ve kontrol gruplarının karşılaştırılmasını kolaylaştırır, nedensel ilişkileri daha iyi anlamamızı sağlar ve daha güvenilir politika önerileri geliştirilmesine katkıda bulunur. Ancak, veri setleri hazırlanırken yukarıda ele alınan hataların önlenmesi büyük önem taşımaktadır.\nBu çalışma, yukarıda sayılan konuların önemli bir kısmında yardımcı olacak niteliktedir. Veri setlerinin etki değerlendirme sürecine uygun hale getirilmesine yönelik adımlar, eksik veri yönetimi, aykırı değer analizi, veri bütünleştirme ve standardizasyon gibi kritik konulara değinilerek kapsamlı bir yol haritası sunulmuştur. Bu sayede, politika yapıcılar ve araştırmacılar, daha sağlıklı analizler yaparak karar alma süreçlerinde güvenilir bilgiye dayalı değerlendirmeler gerçekleştirebileceklerdir.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Giriş</span>"
    ]
  },
  {
    "objectID": "veri_manipulasyonu.html",
    "href": "veri_manipulasyonu.html",
    "title": "\n2  Veri Manipülasyonu\n",
    "section": "",
    "text": "2.1 Temel Veri Manipülasyonu İşlemleri",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Veri Manipülasyonu</span>"
    ]
  },
  {
    "objectID": "veri_manipulasyonu.html#temel-veri-manipülasyonu-işlemleri",
    "href": "veri_manipulasyonu.html#temel-veri-manipülasyonu-işlemleri",
    "title": "\n2  Veri Manipülasyonu\n",
    "section": "",
    "text": "2.1.1 Sütun Seçimi: select()\n\nVeri analizi sırasında, genellikle yalnızca belirli sütunlarla çalışmak ya da analiz için gereksiz sütunları veri setinden çıkarmak gerekebilir. R dilinde, sütun seçimi ve yönetimi için en yaygın kullanılan araçlardan biri, dplyr paketinin sunduğu select() fonksiyonudur. Bu fonksiyon, kolay ve esnek bir şekilde sütunları seçmeyi, sıralamayı veya hariç tutmayı mümkün kılar.\nselect() fonksiyonunu kullanırken yalnızca sütun isimlerini belirterek seçim yapabilirsiniz. Ayrıca, sütun seçim işlemini kolaylaştırmak için çeşitli yardımcı fonksiyonlar da kullanılabilir. Bu yardımcı fonksiyonlar, sütun adlarını desenlere, pozisyonlara veya belirli kurallara göre seçmeyi sağlar.\n\nÖrnek Veri: starwars Veri Seti\n\nBu eğitimde, sütun seçimi işlemlerini öğrenirken, dplyr paketinde yer alan starwars veri setinin ilk 5 satırı ve ilk 6 sütunundan oluşan bir alt kümesini kullanacağız. Bu veri seti, sütun seçim işlemlerini göstermek için çeşitli veri türlerini ve isimlendirme desenlerini içeren harika bir örnek sağlar.\nAşağıdaki adımlarda, hem belirli sütunları seçme hem de sütunları hariç tutma işlemlerini nasıl gerçekleştireceğimizi öğrenirken yukarıda bahsedilen yardımcı fonksiyonları pratikte göreceğiz.\n\n# Gerekli paketin yüklenmesi\n# install.packages(\"dplyr\")\nlibrary(dplyr)\n\n# Örnek veri seti oluşturma\ndf &lt;- starwars[1:5, 1:6] \n# starwars veri setinin ilk 5 satırını ve ilk 6 sütununu seçerek bir alt küme\n# oluşturur ve bunu df adlı bir nesneye atar.\n\n# Veri setini görüntüleme\ndf # Bundan sonra df veri seti kullanılacaktır.\n\n# A tibble: 5 × 6\n  name           height  mass hair_color skin_color  eye_color\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;    \n1 Luke Skywalker    172    77 blond      fair        blue     \n2 C-3PO             167    75 &lt;NA&gt;       gold        yellow   \n3 R2-D2              96    32 &lt;NA&gt;       white, blue red      \n4 Darth Vader       202   136 none       white       yellow   \n5 Leia Organa       150    49 brown      light       brown    \n\n\n\nStarwars Veri Seti\nstarwars veri seti, Star Wars evrenindeki karakterlerin fiziksel özelliklerini ve kimlik bilgilerini içeren bir veri setidir. Bu veri seti, çeşitli değişkenlerle karakterlerin boy, kilo, saç rengi gibi fiziksel özelliklerini ve göz rengi gibi detaylarını sunar. Seçilen veri seti (ilk 5 satır ve ilk 6 sütun), aşağıdaki değişkenleri içerir:\n\n\nname: Karakterin adı.\n\nheight: Boy ölçümlerini içerir (santimetre).\n\nmass: Kilo ölçümlerini içerir (kilogram).\n\nhair_color: Saç rengini içerir.\n\nskin_color: Ten rengini içerir.\n\neye_color: Göz rengini içerir.\n\n\nselect() fonksiyonu, bir veri çerçevesinden belirli sütunları seçmek veya hariç tutmak için kullanılır. Sütun seçimi, hem sütun isimleri hem de sütunların konum/indeks bilgileri kullanılarak yapılabilir.\n\n\n\n\n\n\nR’da Köşeli Parantez\n\n\n\nR’de [ işareti, bir nesne içinden belirli elemanları seçmek için kullanılan bir alt kümeleme operatörüdür. Veri çerçeveleri, matrisler, vektörler ve listeler üzerinde alt kümeleme yapmak için kullanılır.\n\nVektörlerde Alt Kümeleme:\n\n\n# Bir vektör oluşturma\nv &lt;- c(10, 20, 30, 40)\n\n# Tek bir eleman seçme\nv[2]  # Sonuç: 20 (2. eleman)\n\n[1] 20\n\n# Birden fazla eleman seçme\nv[c(1, 3)]  # Sonuç: 10, 30 (1. ve 3. eleman)\n\n[1] 10 30\n\n\n\nVeri Çerçevelerinde Alt Kümeleme:\n\n\n# Bir örnek data frame oluşturma\ndf_ornek &lt;- data.frame(\n  a = 1:3, # Birinci sütun: 1, 2, 3\n  b = 4:6  # İkinci sütun: 4, 5, 6\n)\n\n# 1. satıra erişim\ndf_ornek[1, ]  # 1. satır\n\n  a b\n1 1 4\n\n# \"a\" sütununa erişim\ndf_ornek[, \"a\"]  # \"a\" sütunu\n\n[1] 1 2 3\n\n# İlk 2 satır ve \"a\", \"b\" sütunlarına erişim\ndf_ornek[1:2, c(\"a\", \"b\")]  # İlk 2 satır, \"a\" ve \"b\" sütunları\n\n  a b\n1 1 4\n2 2 5\n\n\n\n\n\n2.1.1.1 İsimle Seçim (By Name)\nBir veri seti üzerinde çalışırken, select fonksiyonu içinde seçmek istediğiniz sütunları belirtebilirsiniz. Sütun isimlerini tırnak işaretleriyle (\") veya tırnak işareti olmadan yazabilirsiniz. Tek bir sütun ya da birden fazla sütunu seçmeniz mümkündür.\n\nBelirli Sütunların Seçilmesi\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Star Wars veri setinden alt küme oluşturma\ndf &lt;- starwars[1:5, 1:6] \n\n# Belirli sütunların seçimi\ndf_2 &lt;- df %&gt;%\n  select(name, height) \n# Yalnızca \"name\" ve \"height\" sütunlarını seçer.\n\n# Yeni veri setini görüntüleme\ndf_2 \n\n# A tibble: 5 × 2\n  name           height\n  &lt;chr&gt;           &lt;int&gt;\n1 Luke Skywalker    172\n2 C-3PO             167\n3 R2-D2              96\n4 Darth Vader       202\n5 Leia Organa       150\n\n\n\n\n\n\n\n\nPipe Operatörü Nedir ve Nasıl Çalışır?\n\n\n\nPipe operatörü (%&gt;%), R programlama dilinde bir işlemin sonucunu otomatik olarak bir sonraki işleme girdi olarak aktarır. Bu operatör, özellikle veri manipülasyonu işlemlerini daha okunabilir, düzenli ve anlaşılır hale getirmek için kullanılır. Pipe, bir işlemden gelen veriyi diğerine “aktararak” adım adım bir işlem zinciri oluşturmayı sağlar.\nNeden Kullanılır?\n\n\nKod Okunabilirliğini Artırır: Pipe operatörüyle yazılmış kodda, işlemler sırasıyla ve kolayca takip edilebilir. Bu, özellikle uzun ve karmaşık veri işleme süreçlerinde büyük bir avantaj sağlar.\n\nAra Değişkenleri Ortadan Kaldırır: Pipe kullanımı, her işlem sonucu için ayrı bir değişken tanımlama ihtiyacını ortadan kaldırır, böylece kod daha sade hale gelir.\n\nAdım Adım İşlem Zinciri Kurar: Birden fazla işlemi arka arkaya uygulamak gerektiğinde pipe operatörü ile bu işlemler kolayca zincirlenir.\n\nNasıl Çalışır?\nPipe operatörü, bir nesneyi (örneğin bir veri çerçevesini) bir fonksiyonun girdisi olarak aktarır. Bu sayede kod, “bu işlemden gelen sonucu şu işleme aktar” şeklinde yazılır. Her işlem bir öncekinin sonucunu alır ve yeni bir işlem yapar.\nÖzetle, pipe operatörü veriyi işlemler arasında taşımak için kullanılır ve kod yazımını hem daha kısa hem de daha anlaşılır hale getirir.\n\n\n\nBelirli Sütun Aralığının Seçilmesi\n\nSütunlar arasında bir dizi seçmek için : operatörünü kullanabilirsiniz. Örneğin, height sütunundan başlayarak skin_color sütununa kadar (her iki sütun da dahil) olan sütunları seçmek isterseniz, şu şekilde yazabilirsiniz:\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6] \n\n# 'height' sütunundan 'skin_color' sütununa kadar olan sütunları seçme\ndf_2 &lt;- df %&gt;%\n  select(height:skin_color) \n# Bu ifade, df veri setinden \"height\" sütunundan başlayıp \"skin_color\" sütununa kadar\n# (her iki sütun dahil) olan sütunları seçer.\n\n# Seçilen sütunları görüntüleme\ndf_2 \n\n# A tibble: 5 × 4\n  height  mass hair_color skin_color \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1    172    77 blond      fair       \n2    167    75 &lt;NA&gt;       gold       \n3     96    32 &lt;NA&gt;       white, blue\n4    202   136 none       white      \n5    150    49 brown      light      \n\n\n\n2.1.1.2 İndeks ile Seçim (By Index)\n\nBelirli Sütunların İndeks Numarası ile Seçilmesi\n\nSütunlar indeks (konum) numaralarına göre de seçilebilir. Bunun için select fonksiyonu içinde istenilen sütun numaralarını belirtmeniz yeterlidir. Aşağıdaki örnek, birinci, beşinci ve altıncı sütunları seçmektedir:\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6] \n\n# Belirli sütunları indeks numarasına göre seçme\ndf_2 &lt;- df %&gt;%\n  select(1, 5, 6) \n# Bu ifade, df veri setinden birinci, beşinci ve altıncı sütunları\n# indeks numaralarına göre seçer.\n\n# Seçilen sütunları görüntüleme\ndf_2\n\n# A tibble: 5 × 3\n  name           skin_color  eye_color\n  &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;    \n1 Luke Skywalker fair        blue     \n2 C-3PO          gold        yellow   \n3 R2-D2          white, blue red      \n4 Darth Vader    white       yellow   \n5 Leia Organa    light       brown    \n\n\n\n2.1.1.3 Sütunları Hariç Tutma (Drop Columns)\n\nBelirli Bir Sütunu Hariç Tutma\n\nselect fonksiyonu, belirli sütunları hariç tutmak (çıkarmak) için de kullanılabilir. Bunun için, sütun isimlerinin önüne - sembolünü eklemek yeterlidir. Aşağıdaki örnekte, mass sütunu hariç tüm sütunlar seçilmektedir:\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6] \n\n# Belirli bir sütunu hariç tutma\ndf_2 &lt;- df %&gt;%\n  select(-mass) \n# Bu ifade, df veri setinden \"mass\" sütununu hariç tutar \n# ve geri kalan tüm sütunları seçer.\n\n# Seçilen sütunları görüntüleme\ndf_2\n\n# A tibble: 5 × 5\n  name           height hair_color skin_color  eye_color\n  &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;    \n1 Luke Skywalker    172 blond      fair        blue     \n2 C-3PO             167 &lt;NA&gt;       gold        yellow   \n3 R2-D2              96 &lt;NA&gt;       white, blue red      \n4 Darth Vader       202 none       white       yellow   \n5 Leia Organa       150 brown      light       brown    \n\n\n\nBirden Fazla Sütunu Hariç Tutma\n\nBirden fazla sütunu çıkarmak istediğiniz durumlarda, her sütun adının önüne - ekleyebilir ya da sütun adlarını içeren bir vektörün önüne - koyabilirsiniz.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6] \n\n# Birden fazla sütunu hariç tutma\ndf_2 &lt;- df %&gt;%\n  select(-height, -mass, -hair_color) \n# Alternatif olarak: select(-c(height, mass, hair_color))\n\n# Seçilen sütunları görüntüleme\ndf_2\n\n# A tibble: 5 × 3\n  name           skin_color  eye_color\n  &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;    \n1 Luke Skywalker fair        blue     \n2 C-3PO          gold        yellow   \n3 R2-D2          white, blue red      \n4 Darth Vader    white       yellow   \n5 Leia Organa    light       brown    \n\n\n\n2.1.1.4 Sütunları Seçmek veya Çıkarmak İçin Yardımcı Fonksiyonlar\n\nBelirli Bir Kelimeyi İçeren Sütunları Seçme\n\nBelirli desenlere veya koşullara dayalı olarak sütunları seçmek için çeşitli yardımcı fonksiyonlar bulunmaktadır. Bu fonksiyonlar şunları içerir:\n\n\ncontains: Belirli bir kelimeyi içeren sütunları seçer.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6]\n\n# \"color\" kelimesini içeren sütunları seçme\ndf_contains_color &lt;- df %&gt;%\n  select(contains(\"color\")) # \"color\" kelimesini içeren sütunları seçer.\n\n# Sonucu görüntüleme\ndf_contains_color\n\n# A tibble: 5 × 3\n  hair_color skin_color  eye_color\n  &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;    \n1 blond      fair        blue     \n2 &lt;NA&gt;       gold        yellow   \n3 &lt;NA&gt;       white, blue red      \n4 none       white       yellow   \n5 brown      light       brown    \n\n\n\nBelirli Bir Metin ile Başlayan Sütunları Seçme\nstarts_with: Belirli bir metinle başlayan sütunları seçer.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6]\n\n# \"h\" harfiyle başlayan sütunları seçme\ndf_starts_with_h &lt;- df %&gt;%\n  select(starts_with(\"h\")) # \"h\" harfiyle başlayan sütunları seçer.\n\n# Sonucu görüntüleme\ndf_starts_with_h\n\n# A tibble: 5 × 2\n  height hair_color\n   &lt;int&gt; &lt;chr&gt;     \n1    172 blond     \n2    167 &lt;NA&gt;      \n3     96 &lt;NA&gt;      \n4    202 none      \n5    150 brown     \n\n\n\nBelirli Bir Metin ile Biten Sütunları Seçme\nends_with: Belirli bir metinle biten sütunları seçer.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6]\n\n# \"t\" harfiyle biten sütunları seçme\ndf_ends_with_t &lt;- df %&gt;%\n  select(ends_with(\"t\")) # \"t\" harfiyle biten sütunları seçer.\n\n# Sonucu görüntüleme\ndf_ends_with_t\n\n# A tibble: 5 × 1\n  height\n   &lt;int&gt;\n1    172\n2    167\n3     96\n4    202\n5    150\n\n\n\nSon Sütunu Seçme\nlast_col: Son sütunu seçer.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6]\n\n# Son sütunu seçme\ndf_last_col &lt;- df %&gt;%\n  select(last_col()) # Veri setindeki son sütunu seçer.\n\n# Sonucu görüntüleme\ndf_last_col\n\n# A tibble: 5 × 1\n  eye_color\n  &lt;chr&gt;    \n1 blue     \n2 yellow   \n3 red      \n4 yellow   \n5 brown    \n\n\n\nBelirli Kelimeleri İçeren Sütunları Regex ile Seçme\nmatches: Regex desenine uyan sütunları seçer.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6]\n\n# \"name\" veya \"mass\" kelimelerini içeren sütunları seçme\ndf_matches_name_mass &lt;- df %&gt;%\n  select(matches(\"name|mass\")) \n# \"name\" veya \"mass\" kelimelerini içeren sütunları regex (düzenli ifade) ile seçer.\n\n# Sonucu görüntüleme\ndf_matches_name_mass\n\n# A tibble: 5 × 2\n  name            mass\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Luke Skywalker    77\n2 C-3PO             75\n3 R2-D2             32\n4 Darth Vader      136\n5 Leia Organa       49\n\n\n\nNumara Aralığı ile Sütun Seçme\nnum_range: Numara aralığına göre sütun seçimi yapar.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6] \n\n# Numara aralığına göre sütun seçme\ndf_num_range &lt;- df %&gt;%\n  select(num_range(\"col\", 1:2)) \n# \"col1\", \"col2\" gibi sütun isimlerine uyan numara aralığını seçer.\n# Ancak bu veri setinde \"col1\" veya \"col2\" isimli sütunlar olmadığı için \n# sonuç boş olacaktır.\n\n# Sonucu görüntüleme\ndf_num_range\n\n# A tibble: 5 × 0\n\n\n\nTüm Sütunları Seçme\neverything: Veri setindeki tüm sütunları seçer.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6]\n\n# Tüm sütunları seçme\ndf_everything &lt;- df %&gt;%\n  select(everything()) # Veri setindeki tüm sütunları seçer.\n\n# Sonucu görüntüleme\ndf_everything\n\n# A tibble: 5 × 6\n  name           height  mass hair_color skin_color  eye_color\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;    \n1 Luke Skywalker    172    77 blond      fair        blue     \n2 C-3PO             167    75 &lt;NA&gt;       gold        yellow   \n3 R2-D2              96    32 &lt;NA&gt;       white, blue red      \n4 Darth Vader       202   136 none       white       yellow   \n5 Leia Organa       150    49 brown      light       brown    \n\n\n\nYalnızca Sayısal Sütunları Seçme\nwhere: Belirli bir koşulu sağlayan sütunları seçer.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6] \n\n# Yalnızca sayısal sütunları seçme\ndf_numeric_cols &lt;- df %&gt;%\n  select(where(is.numeric)) # Sadece sayısal veri tipine sahip sütunları seçer.\n\n# Sonucu görüntüleme\ndf_numeric_cols\n\n# A tibble: 5 × 2\n  height  mass\n   &lt;int&gt; &lt;dbl&gt;\n1    172    77\n2    167    75\n3     96    32\n4    202   136\n5    150    49\n\n\n\nSütunları Belirli Bir Sıralamayla Düzenleme\n\nselect() fonksiyonu yalnızca sütunları seçmek için değil, aynı zamanda sütunların sıralamasını değiştirmek için de kullanılabilir. Sütunları belirli bir düzene göre sıralamak istediğinizde, sütun isimlerini veya yardımcı fonksiyonları sırayla belirterek veri setinizin sütun yapısını yeniden düzenleyebilirsiniz.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri seti\ndf &lt;- starwars[1:5, 1:6]\n\n# Sütunları sıralama\ndf_sorted &lt;- df %&gt;%\n  select(\n    name,                       # İlk olarak 'name' sütunu\n    ends_with(\"_color\"),        # Sonra sonu \"_color\" ile biten sütunlar\n    everything()                # En son diğer tüm sütunlar\n  )\n\n# Sonucu görüntüleme\ndf_sorted\n\n# A tibble: 5 × 6\n  name           hair_color skin_color  eye_color height  mass\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n1 Luke Skywalker blond      fair        blue         172    77\n2 C-3PO          &lt;NA&gt;       gold        yellow       167    75\n3 R2-D2          &lt;NA&gt;       white, blue red           96    32\n4 Darth Vader    none       white       yellow       202   136\n5 Leia Organa    brown      light       brown        150    49\n\n\n\n2.1.2 Satır Filtreleme: filter\n\nfilter fonksiyonu, bir veri çerçevesindeki satırları belirli bir veya birden fazla koşula göre alt kümeye ayırmak için kullanılır. Bu fonksiyon, hem karşılaştırma hem de mantıksal operatörlerle esnek bir şekilde çalışarak, veri setinden yalnızca belirli kriterlere uyan satırları seçmeyi sağlar.\n\n*Örnek Veri: women Veri Seti**\n\nBu eğitimde, filtreleme işlemlerini öğrenirken R içinde yer alan women veri setini kullanacağız. Bu veri seti, kadınlara ait boy ve kilo ölçümlerini içeren iki sayısal sütundan oluşur. Boy ve kilo arasındaki ilişkileri incelemek ve koşullara göre filtreleme işlemlerini göstermek için ideal bir örnek teşkil eder.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri çerçevesi\ndf &lt;- as_tibble(women) \n# women veri setini tibble formatına çevirir ve df nesnesine atar.\n\n# Veri çerçevesini görüntüleme\nhead(df, 10) \n\n# A tibble: 10 × 2\n   height weight\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1     58    115\n 2     59    117\n 3     60    120\n 4     61    123\n 5     62    126\n 6     63    129\n 7     64    132\n 8     65    135\n 9     66    139\n10     67    142\n\n\n\nWomen Veri Seti\nwomen veri seti, kadınlara ait boy ve kilo ölçümlerini içeren bir veri setidir. Bu veri seti, iki sayısal değişken ile kadınların fiziksel özelliklerini sunar.\n\n\nheight: Kadınların boy ölçümlerini içerir (inç cinsinden).\n\nweight: Kadınların kilo ölçümlerini içerir (pound cinsinden).\n\n\n\n2.1.2.1 Tek Bir Koşula Dayalı Satır Filtreleme\nfilter fonksiyonu, bir veri çerçevesindeki satırları belirli bir koşula göre alt kümeye ayırmak için kullanılır. Bu fonksiyon sayesinde, değerlerin belirli bir değere eşit olup olmadığını, daha büyük veya küçük olduğunu, ya da belirli bir aralıkta olup olmadığını kontrol ederek veri filtreleme işlemi yapılabilir.\n\nR’deki Karşılaştırma Operatörleri\n\nAşağıdaki tablo, R’deki karşılaştırma operatörlerini ve açıklamalarını içerir:\n\n\nKarşılaştırma Operatörü\nAçıklama\n\n\n\n&gt;\nDaha büyük\n\n\n&lt;\nDaha küçük\n\n\n&gt;=\nDaha büyük veya eşit\n\n\n&lt;=\nDaha küçük veya eşit\n\n\n==\nEşit\n\n\n!=\nEşit değil\n\n\n\n\n*Belirli Bir Koşula Göre Satırları Filtreleme**\n\nAşağıdaki örnekte, women veri setinden height sütununda değeri 68’den büyük olan satırları filtreliyoruz. Bu işlem, yalnızca boyu belirtilen değerden daha büyük olan kadınlara ait bilgileri seçmek için kullanılır.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# height sütununda 68'den büyük olan değerleri filtreleme\ndf_2 &lt;- df %&gt;%\n  filter(height &gt; 68) \n# height sütununda  68'den büyük olan değerlerin bulunduğu satırları seçer.\n\n# Filtrelenmiş veri çerçevesini görüntüleme\ndf_2\n\n# A tibble: 4 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     69    150\n2     70    154\n3     71    159\n4     72    164\n\n\n\nOrtalama Değere Göre Satırları Filtreleme\n\nfilter fonksiyonu, yalnızca sabit bir değere değil, aynı zamanda bir fonksiyonun çıktısına dayalı olarak da satırları filtreleyebilir. Örneğin, bir sütunun değerlerini, o sütunun ortalamasıyla karşılaştırarak filtreleme yapılabilir.\nAşağıdaki örnekte, height sütununda değeri sütunun ortalamasına eşit veya daha düşük olan satırlar filtrelenmektedir.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# height sütununda ortalamaya eşit veya daha düşük olan değerleri filtreleme\ndf_2 &lt;- df %&gt;%\n  filter(height &lt;= mean(height)) \n# height sütununda, değeri sütunun ortalamasına eşit veya daha düşük olan\n# satırları seçer.\n\n# Filtrelenmiş veri çerçevesini görüntüleme\ndf_2\n\n# A tibble: 8 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     58    115\n2     59    117\n3     60    120\n4     61    123\n5     62    126\n6     63    129\n7     64    132\n8     65    135\n\n\n\n2.1.2.2 Mantıksal Operatörler ve Fonksiyonlarla Satır Filtreleme\nfilter fonksiyonu, mantıksal operatörler veya TRUE ya da FALSE döndüren fonksiyonlarla birlikte kullanılarak daha karmaşık filtreleme işlemleri yapabilir. Aşağıdaki tabloda, R’de sık kullanılan mantıksal operatörler ve fonksiyonlar açıklanmaktadır:\n\n\nOperatör/Fonksiyon\nAçıklama\n\n\n\n!\nMantıksal değil (‘NOT’)\n\n\n%in%\nBelirtilen kümenin içinde\n\n\n!(x %in% y)\nBelirtilen kümenin içinde olmayanlar\n\n\nis.na()\nDeğer NA olanlar\n\n\n!is.na()\nDeğer NA olmayanlar\n\n\ngrepl()\nBelirtilen bir deseni içerenler\n\n\n!grepl()\nBelirtilen bir deseni içermeyenler\n\n\n\n\nBelirli Değerlere Göre Satırları Filtreleme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\ndf &lt;- as_tibble(women)\n\n# 'height' sütununda değeri 65, 70 veya 72 olan satırları seçme\ndf_2 &lt;- df %&gt;%\n  filter(height %in% c(65, 70, 72)) \n# height sütununda 65, 70 veya 72 olan satırları filtreler.\n\n# Filtrelenmiş veriyi görüntüleme\ndf_2\n\n# A tibble: 3 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     65    135\n2     70    154\n3     72    164\n\n\n\nBelirli Değerlere Sahip Olmayan Satırları Filtreleme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\ndf &lt;- as_tibble(women) \n\n# 'height' değeri 65, 70 veya 72 olmayan satırları seçme\ndf_2 &lt;- df %&gt;%\n  filter(!(height %in% c(65, 70, 72))) \n# Bu ifade, height sütununda değeri 65, 70 veya 72 olmayan satırları seçer.\n\n# Filtrelenmiş veriyi görüntüleme\ndf_2\n\n# A tibble: 12 × 2\n   height weight\n    &lt;dbl&gt;  &lt;dbl&gt;\n 1     58    115\n 2     59    117\n 3     60    120\n 4     61    123\n 5     62    126\n 6     63    129\n 7     64    132\n 8     66    139\n 9     67    142\n10     68    146\n11     69    150\n12     71    159\n\n\n\nBelirli Bir Rakamı veya Deseni İçeren Satırları Filtreleme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# height sütununda \"5\" rakamını içeren satırları filtreleme\ndf_2 &lt;- df %&gt;%\n  filter(grepl(\"5\", height)) \n# Bu ifade, height sütununda \"5\" rakamını içeren satırları seçer.\n\n# Filtrelenmiş veriyi görüntüleme\ndf_2\n\n# A tibble: 3 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     58    115\n2     59    117\n3     65    135\n\n\n\n2.1.2.3 Birden Fazla Koşula Dayalı Satır Filtreleme\nBir veri çerçevesinde satırları filtrelerken birden fazla koşul kullanılabilir. Örneğin, belirli bir aralıkta bulunan değerleri seçmek veya tarih aralığında veri filtrelemek gibi işlemler yapılabilir. Bunun için mantıksal operatörler kullanılır.\n\n2.1.2.4 R’deki Mantıksal Operatörler ve Açıklamaları\n\n\n\n\n\n\nMantıksal Operatör\nAçıklama\n\n\n\n&\nEleman bazında mantıksal “VE” (AND)\n\n\n\n|\nEleman bazında mantıksal “VEYA” (OR)\n\n\n\nxor()\nEleman bazında kapsamlı mantıksal !(x | y)\n\n\n\n\n\nİki Koşulu Aynı Anda Sağlayan Satırları Filtreleme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# 'height' değeri 65'ten büyük VE 68'den küçük olan satırları filtreleme\ndf_2 &lt;- df %&gt;%\n  filter(height &gt; 65 & height &lt; 68) \n# Bu ifade, height sütununda değeri 65'ten büyük ve 68'den küçük olan \n# satırları seçer.\n\n# Filtrelenmiş veriyi görüntüleme\ndf_2\n\n# A tibble: 2 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     66    139\n2     67    142\n\n\n\nÇoklu Koşul ile Satırları Filtreleme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# 'height' sütunu 65'ten büyük ve 'weight' sütunu 150'den küçük veya \n# eşit olan satırları filtreleme\ndf_2 &lt;- df %&gt;%\n  filter(height &gt; 65 & weight &lt;= 150) \n# Bu ifade, height sütunu 65'ten büyük ve weight sütunu 150'den küçük veya \n# eşit olan satırları seçer.\n\n# Filtrelenmiş veriyi görüntüleme\ndf_2\n\n# A tibble: 4 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     66    139\n2     67    142\n3     68    146\n4     69    150\n\n\n\nMantıksal “VEYA” Koşulu ile Satırları Filtreleme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# 'height' sütunu 65'ten büyük VEYA 'weight' sütunu 150'den büyük veya \n# eşit olan satırları filtreleme\ndf_2 &lt;- df %&gt;%\n  filter(height &gt; 65 | weight &gt;= 150) \n# Bu ifade, height sütunu 65'ten büyük VEYA weight sütunu 150'den büyük veya \n# eşit olan satırları seçer.\n\n# Filtrelenmiş veriyi görüntüleme\ndf_2\n\n# A tibble: 7 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     66    139\n2     67    142\n3     68    146\n4     69    150\n5     70    154\n6     71    159\n7     72    164\n\n\n\n2.1.2.5 Satır Numarasına Göre Filtreleme: slice\n\nfilter fonksiyonuna benzer bir işlev de slice fonksiyonudur. Bu fonksiyon, satırları indekslerine/pozisyonlarına göre filtrelemeye olanak tanır. Girdi olarak bir sıra veya indekslerin bulunduğu bir vektör (tam sayı değerleri) alır. Kullanımı aşağıda gösterilmiştir.\n\nBelirli Satırları Seçme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women)\n\n# İlk 3 satırı seçme\ndf_2 &lt;- df %&gt;%\n  slice(1:3) # Bu ifade, df veri setinin ilk 3 satırını seçer.\n\n# Filtrelenmiş veriyi görüntüleme\ndf_2\n\n# A tibble: 3 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     58    115\n2     59    117\n3     60    120\n\n\n\nİlk N Satırı Seçme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women)\n\n# İlk 3 satırı seçme\ndf_slice_head &lt;- df %&gt;%\n  slice_head(n = 3) # İlk 3 satır seçilir.\n\n# Sonucu görüntüleme\ndf_slice_head\n\n# A tibble: 3 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     58    115\n2     59    117\n3     60    120\n\n\n\nSon N Satırı Seçme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women)\n\n# Son 3 satırı seçme\ndf_slice_tail &lt;- df %&gt;%\n  slice_tail(n = 3) # Son 3 satır seçilir.\n\n# Sonucu görüntüleme\ndf_slice_tail\n\n# A tibble: 3 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     70    154\n2     71    159\n3     72    164\n\n\n\nRastgele Satırlar Seçme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# Rastgele 3 satırı seçme\ndf_slice_sample &lt;- df %&gt;%\n  slice_sample(n = 3) # Rastgele 3 satır seçilir.\n\n# Sonucu görüntüleme\ndf_slice_sample\n\n# A tibble: 3 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     66    139\n2     69    150\n3     68    146\n\n\n\nBelirli Bir Sütuna Göre En Küçük Satırları Seçme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# height sütununa göre en küçük 2 satırı seçme\ndf_slice_min &lt;- df %&gt;%\n  slice_min(height, n = 2) # height sütunundaki en küçük 2 satır seçilir.\n\n# Sonucu görüntüleme\ndf_slice_min\n\n# A tibble: 2 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     58    115\n2     59    117\n\n\n\nBelirli Bir Sütuna Göre En Büyük Satırları Seçme\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# women veri setini tibble formatına çevirme\ndf &lt;- as_tibble(women) \n\n# weight sütununa göre en büyük 2 satırı seçme\ndf_slice_max &lt;- df %&gt;%\n  slice_max(weight, n = 2) # weight sütunundaki en büyük 2 satır seçilir.\n\n# Sonucu görüntüleme\ndf_slice_max\n\n# A tibble: 2 × 2\n  height weight\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     72    164\n2     71    159\n\n\n\n2.1.3 Satırların Sıralanması: arrange()\n\narrange, dplyr paketinde kullanılan ve bir veri çerçevesindeki satırları bir veya daha fazla sütunun değerlerine göre yeniden sıralamayı sağlayan bir fonksiyondur. Varsayılan olarak, satırları artan düzende sıralar. Azalan sıralama yapmak için desc fonksiyonu kullanılır.\nstarwars Veri Seti\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\ndf &lt;- starwars[1:10, c(1, 2, 3, 11)] \n# starwars veri setinin ilk 10 satırını ve 1, 2, 3 ve 11. sütunlarını seçerek bir alt küme oluşturur ve bunu df adlı bir nesneye atar.\n\n# Veriyi görüntüleme\ndf \n\n# A tibble: 10 × 4\n   name               height  mass species\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1 Luke Skywalker        172    77 Human  \n 2 C-3PO                 167    75 Droid  \n 3 R2-D2                  96    32 Droid  \n 4 Darth Vader           202   136 Human  \n 5 Leia Organa           150    49 Human  \n 6 Owen Lars             178   120 Human  \n 7 Beru Whitesun Lars    165    75 Human  \n 8 R5-D4                  97    32 Droid  \n 9 Biggs Darklighter     183    84 Human  \n10 Obi-Wan Kenobi        182    77 Human  \n\n# Bundan sonra df veri seti kullanılacaktır.\n\n\nStarwars Veri Seti\nstarwars veri seti, Star Wars evrenindeki karakterlerin fiziksel özelliklerini ve kimlik bilgilerini içeren bir veri setidir. Bu veri seti, çeşitli değişkenlerle karakterlerin boy, kilo, cinsiyet gibi fiziksel özelliklerini ve isim gibi kimlik detaylarını sunar. Seçilen veri seti (ilk 10 satır ve 1, 2, 3, 11. sütunlar), aşağıdaki değişkenleri içerir:\n\n\nname: Karakterin adı.\n\nheight: Boy ölçümlerini içerir (santimetre).\n\nmass: Kilo ölçümlerini içerir (kilogram).\n\ngender: Karakterin cinsiyet bilgisi.\n\n\n\n2.1.3.1 Tek Bir Sütuna Göre Sıralama\n\nBir Sütuna Göre Artan Sıralama\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# starwars veri setinden alt küme oluşturma\ndf &lt;- starwars[1:10, c(1, 2, 3, 11)] \n\n# 'height' sütununa göre artan (ASCENDING) sıralama\ndf_2 &lt;- df %&gt;%\n  arrange(height) # height sütununa göre satırları artan sırayla yeniden düzenler.\n\n# Sıralanmış veriyi görüntüleme\ndf_2# Sıralama sonucunda elde edilen yeni veri çerçevesi.\n\n# A tibble: 10 × 4\n   name               height  mass species\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1 R2-D2                  96    32 Droid  \n 2 R5-D4                  97    32 Droid  \n 3 Leia Organa           150    49 Human  \n 4 Beru Whitesun Lars    165    75 Human  \n 5 C-3PO                 167    75 Droid  \n 6 Luke Skywalker        172    77 Human  \n 7 Owen Lars             178   120 Human  \n 8 Obi-Wan Kenobi        182    77 Human  \n 9 Biggs Darklighter     183    84 Human  \n10 Darth Vader           202   136 Human  \n\n\ndesc(): Bir sütunu azalan sırada sıralamak için kullanılır.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# starwars veri setinden alt küme oluşturma\ndf &lt;- starwars[1:10, c(1, 2, 3, 11)] \n\n# 'height' sütununa göre AZALAN sıralama\ndf_2 &lt;- df %&gt;%\n  arrange(desc(height)) \n# desc(): height sütununu azalan sıraya göre sıralamak için kullanılır.\n\n# Sıralanmış veri seti\ndf_2 \n\n# A tibble: 10 × 4\n   name               height  mass species\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1 Darth Vader           202   136 Human  \n 2 Biggs Darklighter     183    84 Human  \n 3 Obi-Wan Kenobi        182    77 Human  \n 4 Owen Lars             178   120 Human  \n 5 Luke Skywalker        172    77 Human  \n 6 C-3PO                 167    75 Droid  \n 7 Beru Whitesun Lars    165    75 Human  \n 8 Leia Organa           150    49 Human  \n 9 R5-D4                  97    32 Droid  \n10 R2-D2                  96    32 Droid  \n\n\n\nBelirli Bir Sütunun Belirli Bir Karakterine Göre Sıralama\n\nsubstr(x, start, stop): Bir metinden belirli bir başlangıç ve bitiş pozisyonu arasındaki karakterleri döndürür.\nx: İşlem yapılacak metin veya karakter vektörü (örneğin, bir sütun adı).\n\n\nstart: Metnin hangi pozisyondan başlayacağını belirtir (dahil).\n\nstop: Metnin hangi pozisyonda duracağını belirtir (dahil).\n\nEğer x “Star Wars” ise:\n\n\nsubstr(x, 1, 4) → \"Star\" (İlk 4 karakter).\n\nsubstr(x, 6, 9) → \"Wars\" (6. ve 9. karakterler arası).\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# starwars veri setinden alt küme oluşturma\ndf &lt;- starwars[1:10, c(1, 2, 3, 11)] \n\n# 'name' sütununu adın ilk harfine göre sıralama\ndf_2 &lt;- df %&gt;%\n  arrange(substr(name, 1, 2)) \n# substr(): 'name' sütununun ilk iki harfine göre sıralama yapar.\n\n# Veriyi görüntüleme\ndf_2\n\n# A tibble: 10 × 4\n   name               height  mass species\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1 Beru Whitesun Lars    165    75 Human  \n 2 Biggs Darklighter     183    84 Human  \n 3 C-3PO                 167    75 Droid  \n 4 Darth Vader           202   136 Human  \n 5 Leia Organa           150    49 Human  \n 6 Luke Skywalker        172    77 Human  \n 7 Obi-Wan Kenobi        182    77 Human  \n 8 Owen Lars             178   120 Human  \n 9 R2-D2                  96    32 Droid  \n10 R5-D4                  97    32 Droid  \n\n\n\n2.1.3.2 Birden Fazla Sütuna Göre Satırları Sıralama\nSatırlar birden fazla sütuna göre de sıralanabilir. Bu durumda sıralama sırasıyla gerçekleşir: önce birinci sütun, ardından ikinci sütun ve devam eder. Aşağıdaki örnek, satırların height ve mass değişkenlerine göre sıralanmasını göstermektedir.\n\nBirden Fazla Sütuna Göre Sıralama\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# starwars veri setinden alt küme oluşturma\ndf &lt;- starwars[1:10, c(1, 2, 3, 11)] \n\n# 'height' ve ardından 'mass' sütununa göre sıralama\ndf_2 &lt;- df %&gt;%\n  arrange(height, mass) \n# Önce height sütununa, ardından mass sütununa göre artan sıralama yapar.\n\n# Veriyi görüntüleme\ndf_2\n\n# A tibble: 10 × 4\n   name               height  mass species\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1 R2-D2                  96    32 Droid  \n 2 R5-D4                  97    32 Droid  \n 3 Leia Organa           150    49 Human  \n 4 Beru Whitesun Lars    165    75 Human  \n 5 C-3PO                 167    75 Droid  \n 6 Luke Skywalker        172    77 Human  \n 7 Owen Lars             178   120 Human  \n 8 Obi-Wan Kenobi        182    77 Human  \n 9 Biggs Darklighter     183    84 Human  \n10 Darth Vader           202   136 Human  \n\n\n\nBirden Fazla Sütuna Göre Artan Sıralama\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# starwars veri setinden alt küme oluşturma\ndf &lt;- starwars[1:10, c(1, 2, 3, 11)] \n\n# 'mass' ve ardından 'height' sütununa göre sıralama\ndf_2 &lt;- df %&gt;%\n  arrange(mass, height)\n# Önce mass sütununa, ardından height sütununa göre artan sıralama yapar.\n\n# Veriyi görüntüleme\ndf_2\n\n# A tibble: 10 × 4\n   name               height  mass species\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1 R2-D2                  96    32 Droid  \n 2 R5-D4                  97    32 Droid  \n 3 Leia Organa           150    49 Human  \n 4 Beru Whitesun Lars    165    75 Human  \n 5 C-3PO                 167    75 Droid  \n 6 Luke Skywalker        172    77 Human  \n 7 Obi-Wan Kenobi        182    77 Human  \n 8 Biggs Darklighter     183    84 Human  \n 9 Owen Lars             178   120 Human  \n10 Darth Vader           202   136 Human  \n\n\n\n2.1.4 Sütun Adlarını Yeniden Adlandırma: rename()\n\nR’de dplyr paketinin rename() fonksiyonu, bir veri çerçevesindeki sütun isimlerini değiştirmek için kullanılır. Bu fonksiyon, belirli sütunlara yeni isimler atamanıza olanak tanır.\nAyrıca, rename_with() fonksiyonu, sütunları bir fonksiyon kullanarak toplu halde yeniden adlandırmanıza olanak sağlar.\ndplyr paketindeki band_instruments veri setini kullanacağız. Bu veri seti, name ve plays adlı iki sütunu içermektedir.\n\nİlk sütunu “First Name” olarak yeniden adlandırmak istediğinizi düşünüyorsanız, aşağıdaki komutu çalıştırabilirsiniz:\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# 'name' sütununu 'First Name' olarak yeniden adlandırma\ndf_2 &lt;- band_instruments %&gt;%\n  rename(\"First Name\" = name)\n\n# Veriyi görüntüleme\ndf_2\n\n# A tibble: 3 × 2\n  `First Name` plays \n  &lt;chr&gt;        &lt;chr&gt; \n1 John         guitar\n2 Paul         bass  \n3 Keith        guitar\n\n\n\nSütun Adını İndeks ile Yeniden Adlandırma\n\nSütunları indeks numarasına göre de yeniden adlandırabilirsiniz. Aşağıdaki örnek, veri setinin ikinci sütununun nasıl yeniden adlandırılacağını göstermektedir.\n\nlibrary(tidyverse)\n\n# İkinci sütunu 'Second column' olarak yeniden adlandırma\ndf_2 &lt;- band_instruments %&gt;%\n  rename(\"Second column\" = 2)\n\n# Veriyi görüntüleme\ndf_2\n\n# A tibble: 3 × 2\n  name  `Second column`\n  &lt;chr&gt; &lt;chr&gt;          \n1 John  guitar         \n2 Paul  bass           \n3 Keith guitar         \n\n\n\nBirden Fazla Sütun Adını Yeniden Adlandırma\nBirden fazla sütunu aynı anda yeniden adlandırmak mümkündür. Bunun için, fonksiyona new_name = old_name ifadeleri eklenir ve bu ifadeler virgülle ayrılır. Aşağıdaki örnek, name sütununu Member, plays sütununu ise Instrument olarak yeniden adlandırmaktadır.\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# 'name' sütununu 'Member', 'plays' sütununu 'Instrument' olarak adlandırma\ndf_2 &lt;- band_instruments %&gt;%\n  rename(\"Member\" = name,\n         \"Instrument\" = plays)\n\n# Veriyi görüntüleme\ndf_2\n\n# A tibble: 3 × 2\n  Member Instrument\n  &lt;chr&gt;  &lt;chr&gt;     \n1 John   guitar    \n2 Paul   bass      \n3 Keith  guitar",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Veri Manipülasyonu</span>"
    ]
  },
  {
    "objectID": "veri_manipulasyonu.html#veri-dönüştürme",
    "href": "veri_manipulasyonu.html#veri-dönüştürme",
    "title": "\n2  Veri Manipülasyonu\n",
    "section": "\n2.2 Veri Dönüştürme",
    "text": "2.2 Veri Dönüştürme\n\n2.2.1 Yeni Değişkenler Oluşturma ve Düzenleme: mutate()\n\nmutate(), R’de dplyr paketinde kullanılan bir fonksiyondur ve bir veri çerçevesinde yeni sütunlar oluşturmak veya mevcut sütunları değiştirmek için kullanılır. Veri çerçevesinin orijinal yapısını korur ve sonuçları yeni sütunlar olarak saklar.\n\n2.2.1.1 mutate() Fonksiyonu Sözdizimi\n\n\n.data Veri çerçevesi\n\n\n... Yeni sütunlar (örneğin, yeni_sütun = işlem)\n\n\n.by = NULL, Gruplama değişkenleri (isteğe bağlı)\n\n\n.keep = c(\"all\", \"used\", \"unused\", \"none\"), Hangi sütunların tutulacağı\n\n\n.before = NULL, Yeni sütunları belirli bir sütundan önce yerleştirme\n\n\n.after = NULL Yeni sütunları belirli bir sütundan sonra yerleştirme\n\n\n2.2.1.2 Yeni Sütun Oluşturma\nBir veri çerçevesine yeni bir sütun eklemek için, yeni sütunun adını (örneğin, Var3) ve yeni sütunun değerlerini hesaplamak için bir ifadeyi belirtmeniz yeterlidir. Aşağıdaki örnekte, yeni sütunun değeri, diğer iki sütunun toplamı (Var1 + Var2) olarak hesaplanmıştır.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- data.frame(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütun: 'Var3', 'Var1' ve 'Var2'nin toplamı\ndf_2 &lt;- df %&gt;%\n  mutate(Var3 = Var1 + Var2) \n# Yeni sütun ekler: Var3, Var1 ve Var2 sütunlarının toplamı olarak hesaplanır.\n\n# Yeni sütun eklenmiş veri seti\ndf_2\n\n  Var1 Var2 Var3\n1   32   39   71\n2   34    1   35\n3   15   29   44\n4   12    3   15\n5   42   35   77\n\n\n\nYeni Sütun Olarak Karekök Hesaplama\n\nBir veri çerçevesine yeni bir sütun eklemek için mevcut bir sütuna bir fonksiyon uygulayabilirsiniz. Aşağıdaki örnek, bir sütunun karekökünü hesaplayarak yeni bir sütun (Sqrt_Var1) oluşturmayı göstermektedir.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- data.frame(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütun: 'Sqrt_Var1', 'Var1' sütununun karekökü\ndf_2 &lt;- df %&gt;%\n  mutate(Sqrt_Var1 = sqrt(Var1)) \n# Yeni sütun ekler: Sqrt_Var1, Var1 sütununun karekökü olarak hesaplanır.\n\n# Veriyi görüntüleme\ndf_2 # Yeni sütun eklenmiş veri seti.\n\n  Var1 Var2 Sqrt_Var1\n1   32   39  5.656854\n2   34    1  5.830952\n3   15   29  3.872983\n4   12    3  3.464102\n5   42   35  6.480741\n\n\n\nBirden Fazla Yeni Sütun Eklemek ve Koşullu Değer Atamak\n\nmutate fonksiyonuna birden fazla ifade ekleyerek aynı anda birden fazla sütun oluşturabilirsiniz. Bunun için ifadeleri virgülle ayırmanız yeterlidir.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- data.frame(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütunlar: 'Var3', 'Var4' ve 'Var5'\ndf_2 &lt;- df %&gt;%\n  mutate(\n    Var3 = Var1 + Var2,             # Var1 ve Var2'nin toplamı\n    Var4 = cumsum(Var1),            # Var1'in kümülatif toplamı\n    Var5 = if_else(Var1 &gt; Var2, TRUE, FALSE) \n    # Var1, Var2'den büyükse TRUE, değilse FALSE\n  )\n\n# Veriyi görüntüleme\ndf_2 # Birden fazla sütun eklenmiş veri seti.\n\n  Var1 Var2 Var3 Var4  Var5\n1   32   39   71   32 FALSE\n2   34    1   35   66  TRUE\n3   15   29   44   81 FALSE\n4   12    3   15   93  TRUE\n5   42   35   77  135  TRUE\n\n\n\n\n\n\n\n\nif_else fonksyonu\n\n\n\nif_else() fonksiyonu R programlama dilinde koşullu ifadeler oluşturmak için kullanılan bir fonksiyondur. Temelde, bir koşulun doğru olup olmamasına göre farklı değerler döndürür. Bu, daha geleneksel if ve else yapılarının vektörleştirilmiş bir karşılığıdır ve özellikle veri manipülasyonu ve vektörler üzerinde işlem yaparken çok daha verimli olabilir.\n\nif_else(condition, true_value, false_value)\n\n\n\ncondition: Mantıksal bir vektör veya ifade. Her bir eleman için TRUE veya FALSE değerini döndürmelidir.\n\ntrue_value: condition vektöründeki ilgili eleman TRUE ise döndürülecek değer. Bu bir vektör olabilir.\n\nfalse_value: condition vektöründeki ilgili eleman FALSE ise döndürülecek değer. Bu da bir vektör olabilir.\n\nNasıl Çalışır?\nif_else() fonksiyonu, condition vektöründeki her bir elemanı teker teker kontrol eder. Eğer ilgili eleman TRUE ise, true_value vektöründeki aynı konumdaki değeri döndürür. Eğer ilgili eleman FALSE ise, false_value vektöründeki aynı konumdaki değeri döndürür.\n\n\n\nBelirli Sütunlarda İşlem Yapmak ve Yeni Sütunlar Oluşturmak: across() Kullanımı\n\nacross() fonksiyonu, mutate ile birlikte kullanılarak belirli sütunlara fonksiyonlar uygulamayı sağlar. Aynı zamanda yardımcı fonksiyonlar (contains, starts_with, vb.) ile sütun seçiminde esneklik sunar.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\ndf &lt;- tibble(Var1 = c(1, 4, 9), Var2 = c(16, 25, 36), Var3 = c(49, 64, 81))\n\n# \"1\" içeren sütunlara karekök uygulama ve yeni sütunlar oluşturma\ndf_2 &lt;- df %&gt;%\n   mutate(\n      across(\n         .cols = contains(\"Var\"), # İsimlerinde \"Var\" geçen sütunları seçer\n         .fns = sqrt,           # Karekök fonksiyonunu uygular\n         .names = \"{.col}_sqrt\")) # Yeni sütun isimleri: Eski isim + \"_sqrt\"\n\n# Veriyi görüntüleme\ndf_2 # Yeni sütunlar eklenmiş veri seti.\n\n# A tibble: 3 × 6\n   Var1  Var2  Var3 Var1_sqrt Var2_sqrt Var3_sqrt\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     1    16    49         1         4         7\n2     4    25    64         2         5         8\n3     9    36    81         3         6         9\n\n\n\n2.2.1.3 Mevcut Sütunları Güncelleme\n\nMevcut Bir Sütunun Değerlerini Güncelleme\n\nmutate() fonksiyonu, mevcut sütunları güncellemek veya üzerinde işlem yapmak için de kullanılabilir. Bunu gerçekleştirmek için, eski_sütun_adı = ifade sözdizimini kullanmanız yeterlidir.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\ndf &lt;- tibble(Var1 = c(10, 20, 30), Var2 = c(5, 10, 15))\n\n# 'Var1' sütununun değerlerini ikiye katlama\ndf_2 &lt;- df %&gt;%\n  mutate(Var1 = Var1 * 2) # Var1 sütununun yeni değerleri Var1 * 2\n\n# Veriyi görüntüleme\ndf_2\n\n# A tibble: 3 × 2\n   Var1  Var2\n  &lt;dbl&gt; &lt;dbl&gt;\n1    20     5\n2    40    10\n3    60    15\n\n\n\nBirden Fazla Yeni Sütun Eklemek ve Koşullu Değer Atamak\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(123)\ndf &lt;- tibble(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütunlar: 'Var3', 'Var4' ve 'Var5'\ndf_2 &lt;- df %&gt;%\n  mutate(\n    Var1 = Var1 + Var2,             # Var1 ve Var2'nin toplamı\n    Var2 = cumsum(Var1),            # Var1'in kümülatif toplamı\n    Var3 = if_else(Var1 &gt;= Var2, \"Yes\", \"No\") \n    # Var1, Var2'den büyükse \"Yes\", değilse \"No\"\n  )\n\n# Veriyi görüntüleme\ndf_2 # Birden fazla sütun eklenmiş veri seti.\n\n# A tibble: 5 × 3\n   Var1  Var2 Var3 \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n1    81    81 Yes  \n2    58   139 No   \n3    51   190 No   \n4    17   207 No   \n5    67   274 No   \n\n\n\nBelirli Sütunlarda İşlem Yapmak\n\nacross() fonksiyonunu kullanarak belirli sütunları seçebilir ve bunlara özel bir fonksiyon uygulayabilirsiniz. Yeni sütun oluşturmadan, mevcut sütunların değerlerini doğrudan değiştirebilirsiniz.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- tibble(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# 'Var1' hariç tüm sütunlara logaritma işlemi uygulama\ndf_2 &lt;- df %&gt;%\n  mutate(across(!Var1, log)) # Var1 dışındaki tüm sütunlara log uygulanır\n\n# Veriyi görüntüleme\ndf_2 # Güncellenmiş veri seti.\n\n# A tibble: 5 × 2\n   Var1  Var2\n  &lt;int&gt; &lt;dbl&gt;\n1    32  3.66\n2    34  0   \n3    15  3.37\n4    12  1.10\n5    42  3.56\n\n\n\n2.2.1.4 Yeni Sütunların Konumu\nVarsayılan olarak, mutate fonksiyonu yeni sütunları veri çerçevesinin sonuna ekler. Ancak, .before veya .after argümanlarını kullanarak yeni sütunun başka bir sütuna göre konumunu belirleyebilirsiniz.\n\nYeni Sütunlar Eklemek ve Belirli Pozisyonlara Yerleştirmek\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- data.frame(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütunlar ekleniyor\ndf_2 &lt;- df %&gt;%\n  mutate(Var3 = Var1 / Var2, .before = Var2) %&gt;%  \n  # Yeni sütun: Var3, Var2'den önce ekleniyor\n  mutate(Var4 = Var1 * Var2, .before = Var1) %&gt;%  \n  # Yeni sütun: Var4, ilk sütundan önce ekleniyor\n  mutate(Var5 = (Var1 * Var2) / 2, .after = Var4) \n  # Yeni sütun: Var5, Var4'ten sonra ekleniyor\n\n# Veriyi görüntüleme\ndf_2\n\n  Var4  Var5 Var1       Var3 Var2\n1 1248 624.0   32  0.8205128   39\n2   34  17.0   34 34.0000000    1\n3  435 217.5   15  0.5172414   29\n4   36  18.0   12  4.0000000    3\n5 1470 735.0   42  1.2000000   35\n\n\n\n2.2.1.5 Sütunları saklama veya çıkarma\nYeni sütunlar bir veri çerçevesine eklendiğinde, varsayılan olarak diğer tüm sütunlar korunur. Ancak .keep argümanı sayesinde bu davranış değiştirilebilir. .keep varsayılan olarak \"all\" değerine sahiptir, ancak aşağıdaki şekilde ayarlanabilir:\n\n\"all\": Tüm sütunları korur (varsayılan).\n\"used\": Sadece mutate içinde kullanılan sütunları korur.\n\"unused\": mutate içinde kullanılmayan sütunları korur.\n\"none\": Eski tüm sütunları siler, sadece yeni sütunlar kalır.\nKullanılan Sütunları Saklamak .keep = \"used\"\n\nAşağıdaki örnek, sadece kullanılan sütunları saklamak için .keep = \"used\" ayarının nasıl kullanılacağını göstermektedir:\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- data.frame(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütun ekleme ve sadece kullanılan sütunu ('Var1') ve \n# yeni sütunu ('Var3') koruma\ndf_2 &lt;- df %&gt;%\n  mutate(Var3 = Var1 * 2, \n         .keep = \"used\")\n\ndf_2\n\n  Var1 Var3\n1   32   64\n2   34   68\n3   15   30\n4   12   24\n5   42   84\n\n\n\nYalnızca Kullanılmayan Sütunları Saklamak .keep = \"unused\"\n\nTam tersi, yalnızca yeni sütunu ve kullanılmayan sütunları korumaktır. Bunun için .keep = \"unused\" ayarı kullanılabilir. Bu durumda, mutate içinde kullanılan sütunlar hariç tutulur.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- data.frame(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütun ekleme ve sadece yeni sütunu ('Var3') ve kullanılmayan \n# sütunu ('Var2') koruma\ndf_2 &lt;- df %&gt;%\n  mutate(Var3 = Var1 * 2, .keep = \"unused\")\n\ndf_2\n\n  Var2 Var3\n1   39   64\n2    1   68\n3   29   30\n4    3   24\n5   35   84\n\n\n\nYalnızca Yeni Sütunu Saklamak .keep = \"none\"\n\nSon olarak, orijinal veri çerçevesindeki tüm sütunları kaldırmak için .keep = \"none\" kullanabilirsiniz.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(8)\ndf &lt;- data.frame(Var1 = sample(1:50, 5), Var2 = sample(1:50, 5))\n\n# Yeni sütun ekleme ve yalnızca yeni sütunu ('Var3') saklama\ndf_2 &lt;- df %&gt;%\n  mutate(Var3 = Var1 * 2, .keep = \"none\")\n\ndf_2\n\n  Var3\n1   64\n2   68\n3   30\n4   24\n5   84\n\n\n\n2.2.2 İstatistiksel Özetler Oluşturma: summarise()\n\nsummarise (veya summarize) fonksiyonu, verileri toplulaştırmak ve özetlemek için kullanılır. Bu fonksiyon, özellikle verileri gruplara ayırarak her grup için istatistiksel özetler veya hesaplamalar yapmak açısından oldukça faydalıdır. Belirtilen özet istatistiklerle birlikte yeni bir veri çerçevesi oluşturur ve her grup için tek bir satır döndürür.\n\n2.2.2.1 Sözdizimi\n\n# summarise(data, new_column = function(column))\n\nArgümanlar:\n\n\ndata: Özetleme yapmak istediğiniz veri çerçevesi veya gruplandırılmış veri.\n\nnew_column: Yeni oluşturulacak sütunun adı.\n\nfunction(column): Belirli bir sütuna uygulanacak fonksiyon (örneğin: sum, mean, max).\n\n2.2.2.2 Verinin İstatistiksel Özetleri\nBir veri setinden belirli değişkenlerin istatistiksel özetlerini içeren yeni bir veri çerçevesi oluşturabilirsiniz. summarise fonksiyonu ile kullanılabilecek en faydalı fonksiyonlar aşağıda açıklanmıştır:\n\n2.2.3 Verinin İstatistiksel Özetleri\nBir veri setinden belirli değişkenlerin istatistiksel özetlerini içeren yeni bir veri çerçevesi oluşturabilirsiniz. summarise fonksiyonu ile kullanılabilecek en faydalı fonksiyonlar aşağıda açıklanmıştır:\nKullanışlı Fonksiyonlar\n\n\nFonksiyon\nAçıklama\n\n\n\nmean()\nDeğerlerin ortalaması\n\n\nmedian()\nDeğerlerin medyanı\n\n\nsd(), var()\nDeğerlerin standart sapması ve varyansı\n\n\nquantile()\nDeğerlerin çeyrek dilimleri\n\n\nIQR()\nDeğerlerin interçeyrek aralığı\n\n\nmin(), max()\nMinimum ve maksimum değer\n\n\nfirst()\nİlk değer\n\n\nlast()\nSon değer\n\n\nnth()\nN. sıradaki değer\n\n\nn()\nHer gruptaki eleman sayısı\n\n\nn_distinct()\nBenzersiz değerlerin sayısı\n\n\n\n\nVeri Seti Üzerinde Özetleme İşlemi\n\nAşağıdaki örnekte, orijinal veri setindeki sayısal sütunların ortalamalarını hesaplayarak yeni bir veri çerçevesi oluşturmayı gösteriyoruz.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(9)\ndf &lt;- data.frame(group = sample(c(\"G1\", \"G2\"), 5, replace = TRUE),\n                 x = sample(1:50, 5), y = sample(1:50, 5))\n\n# Yeni sütunlar: 'mean_x' ve 'mean_y'\ndf_2 &lt;- df %&gt;%\n   summarise(mean_x = mean(x), # x sütununun ortalaması\n             mean_y = mean(y))  # y sütununun ortalaması\n\n# Veriyi görüntüleme\ndf_2 # x ve y sütunlarının ortalaması alınmış veri seti.\n\n  mean_x mean_y\n1   21.6   38.2\n\n\nSonuçta elde edilen çıktı, giriş fonksiyonu tarafından döndürülen değerler kadar satır içerecektir.\n\n2.2.3.1 Gruplara Göre Veriyi Özetleme (group_by)\n\nsummarise fonksiyonu, group_by ile birlikte kullanıldığında, her grup için istatistiksel özetler oluşturmak açısından oldukça etkili bir yöntem sunar. Bu kombinasyon, veri setini belirli bir değişkene göre gruplandırarak, her grup için özelleştirilmiş özet istatistiklerin elde edilmesini sağlar.\nAşağıdaki örnek, group değişkenine göre gruplandırılmış veri seti üzerinde, her sütunun ortalama değerini hesaplayarak özet bir veri çerçevesi oluşturmaktadır.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(9)\ndf &lt;- data.frame(group = sample(c(\"G1\", \"G2\"), 5, replace = TRUE),\n                 x = sample(1:50, 5),\n                 y = sample(1:50, 5))\n\n# Gruplara göre 'x' ve 'y' sütunlarının toplamları\ndf_2 &lt;- df %&gt;%\n   group_by(group) %&gt;%\n   summarise(mean_x = sum(x), # x sütununun toplamı\n             mean_y = sum(y))  # y sütununun toplamı\n\n# Veriyi görüntüleme\ndf_2 # Gruplara göre özetlenmiş veri seti.\n\n# A tibble: 2 × 3\n  group mean_x mean_y\n  &lt;chr&gt;  &lt;int&gt;  &lt;int&gt;\n1 G1        52     97\n2 G2        56     94\n\n\n\nÇoklu Gruplara Göre Veri Setini Özetleme\n\nAyrıca, birden fazla kategorik değişkene göre gruplama yapabilirsiniz. Bu durumda, fonksiyon her grup ve alt grup için istatistiksel özetler hesaplar. Varsayılan olarak, çıktı, birinci kategorik değişkene göre gruplandırılır ve bu durum bir mesaj ile belirtilir.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(9)\ndf &lt;- data.frame(group = sample(c(\"G1\", \"G2\"), 5, replace = TRUE),\n                 group_2 = sample(c(\"G3\", \"G4\"), 5, replace = TRUE),\n                 x = sample(1:50, 5),\n                 y = sample(1:50, 5))\n\n# Gruplara göre 'x' ve 'y' sütunlarının toplamları\ndf_2 &lt;- df %&gt;%\n   group_by(group, group_2) %&gt;% # 'group' ve 'group_2' sütunlarına göre gruplama\n   summarise(sum_x = sum(x),    # x sütununun toplamını hesaplar\n             sum_y = sum(y))    # y sütununun toplamını hesaplar\n\n# Veriyi görüntüleme\ndf_2 # Gruplara göre özetlenmiş veri seti.\n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group group_2 sum_x sum_y\n  &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n1 G1    G3         74    86\n2 G1    G4         18    30\n3 G2    G3         48    22\n4 G2    G4         37    35\n\n\n\n\n.groups argümanı\n\n.groups argümanı isteğe bağlıdır ve gruplama işlemi sonrası grupların nasıl ele alınacağını kontrol etmek için kullanılır. Aşağıdaki değerlerden birini alabilir:\n\n\n\"drop_last\": Eğer birden fazla gruplama seviyesi varsa, son gruplama seviyesi kaldırılır, ancak diğerleri korunur. Örneğin, iki kategorik değişkenle gruplama yaptıysanız, birinci değişkene göre gruplama devam eder.\n\n\"drop\": Tüm gruplama seviyelerini kaldırır. Sonuç, gruplandırılmamış, düz bir veri çerçevesi olur.\n\n\"keep\": Orijinal gruplama yapısını korur. Özetleme işlemi tamamlandıktan sonra veri, hala gruplandırılmış halde kalır.\n\n\"rowwise\": Her satırı kendi başına bir grup olarak ele alır. Bu, satır bazında işlem yapmak için kullanılır ve genellikle özelleştirilmiş hesaplamalarda faydalıdır.\n\n\n\nGruplama Sonrası Gruplama Seviyelerini Kaldırma .groups\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(9)\ndf &lt;- data.frame(group = sample(c(\"G1\", \"G2\"), 5, replace = TRUE),\n                 group_2 = sample(c(\"G3\", \"G4\"), 5, replace = TRUE),\n                 x = sample(1:50, 5),\n                 y = sample(1:50, 5))\n\n# Gruplara göre 'x' ve 'y' sütunlarının toplamları, tüm gruplama seviyeleri kaldırılır\ndf_2 &lt;- df %&gt;%\n   group_by(group, group_2) %&gt;% # 'group' ve 'group_2' sütunlarına göre gruplama\n   summarise(sum_x = sum(x),    # x sütununun toplamı\n             sum_y = sum(y),    # y sütununun toplamı\n             .groups = \"drop\")  # Tüm gruplama seviyelerini kaldırır\n\n# Veriyi görüntüleme\ndf_2 # Gruplama bilgisi olmadan özetlenmiş veri seti.\n\n# A tibble: 4 × 4\n  group group_2 sum_x sum_y\n  &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n1 G1    G3         74    86\n2 G1    G4         18    30\n3 G2    G3         48    22\n4 G2    G4         37    35\n\n\n\nAynı işlem ungroup() fonksyonu ile de yapılabilir:\n\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(9)\ndf &lt;- data.frame(group = sample(c(\"G1\", \"G2\"), 5, replace = TRUE),\n                 group_2 = sample(c(\"G3\", \"G4\"), 5, replace = TRUE),\n                 x = sample(1:50, 5),\n                 y = sample(1:50, 5))\n\n# Gruplara göre 'x' ve 'y' sütunlarının toplamları, ardından gruplama kaldırılır\ndf_2 &lt;- df %&gt;%\n   group_by(group, group_2) %&gt;% # 'group' ve 'group_2' sütunlarına göre gruplama\n   summarise(sum_x = sum(x),     # x sütununun toplamını hesaplar\n             sum_y = sum(y)) %&gt;% # y sütununun toplamını hesaplar\n   ungroup()                     # Gruplama kaldırılır\n\n# Veriyi görüntüleme\ndf_2 # Gruplama bilgisi kaldırılmış özetlenmiş veri seti.\n\n# A tibble: 4 × 4\n  group group_2 sum_x sum_y\n  &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n1 G1    G3         74    86\n2 G1    G4         18    30\n3 G2    G3         48    22\n4 G2    G4         37    35\n\n\n\n2.2.3.2 Birden Fazla Sütunu Özetleme\nBirden fazla sütunu manuel olarak belirtmek yerine, summarise ve across kombinasyonunu kullanarak bir koşula göre sütunları seçerek özetler oluşturabilirsiniz. Sütunları seçmek için kullanılan yardımcı fonksiyonların listesine göz atabilirsiniz.\nAşağıdaki örnekte, group sütunu hariç tüm sütunların varyansı hesaplanmakta ve sonuç sütunları, orijinal sütun adlarına “var” eklenerek yeniden adlandırılmaktadır.\n\n# Gerekli paketin yüklenmesi\nlibrary(tidyverse)\n\n# Örnek veri\nset.seed(9)\ndf &lt;- data.frame(group = sample(c(\"G1\", \"G2\"), 5, replace = TRUE),\n                 x = sample(1:50, 5), \n                 y = sample(1:50, 5))\n\n# Varyans hesaplama: 'group' hariç tüm sütunların varyansı\ndf_2 &lt;- df %&gt;% \n  summarise(\n    across(!group,  # 'group' sütunu hariç diğer sütunlar\n           var,     # Varyans hesaplamak için 'var' fonksiyonu\n           .names = \"{.col}_var\")) # Sütun adlarına \"_var\" ekler\n\n# Veriyi görüntüleme\ndf_2 # 'group' dışındaki sütunların varyansını içeren veri seti.\n\n  x_var y_var\n1 254.3 149.2",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Veri Manipülasyonu</span>"
    ]
  },
  {
    "objectID": "veri_manipulasyonu.html#veri-şekillendirme",
    "href": "veri_manipulasyonu.html#veri-şekillendirme",
    "title": "\n2  Veri Manipülasyonu\n",
    "section": "\n2.3 Veri Şekillendirme",
    "text": "2.3 Veri Şekillendirme\n\n2.3.1 Veri Çerçevelerini Birleştirme: merge()\n\nR’deki merge fonksiyonu, iki veri çerçevesini ortak sütunlar veya satır isimleri aracılığıyla birleştirmeye olanak tanır. Bu fonksiyon, sol birleşim (left join), iç birleşim (inner join), sağ birleşim (right join) veya tam birleşim (full join) gibi farklı veri tabanı (SQL) birleşim türlerini gerçekleştirebilir. Bu eğitimde, R’nin temel fonksiyonlarını kullanarak veri setlerini birleştirmenin çeşitli yöntemlerini örneklerle öğrenebilirsiniz.\n\n2.3.1.1 R’deki merge() Fonksiyonu\n\nlibrary(tidyverse)\n\n# merge(x, y, ...)\n\nArgümanlar\n\n\nx, y: Birleştirilecek veri çerçeveleri veya dönüştürülebilecek diğer objeler.\n\nby: Birleştirme işlemi için kullanılacak ortak sütunların adları. Varsayılan olarak, her iki veri çerçevesinde bulunan sütun adlarının kesişimi kullanılır.\n\nby.x, by.y: Sırasıyla x ve y veri çerçevelerinden birleştirme için kullanılacak sütunların adları. Özel sütunlar belirtmek için kullanılır.\n\nall: TRUE olarak ayarlandığında hem all.x hem de all.y TRUE olur ve tam birleşim (full join) gerçekleştirilir.\n\nall.x, all.y: all.x = TRUE: x’deki tüm satırları korur, y’de eşleşmeyenler için eksik değerler (NA) eklenir (sol birleşim - left join). all.y = TRUE: y’deki tüm satırları korur, x’de eşleşmeyenler için eksik değerler (NA) eklenir (sağ birleşim - right join).\n\nsort: TRUE ise çıktı, birleştirme için kullanılan sütunlara göre sıralanır. Varsayılan değer TRUE’dur.\n\nsuffixes: Ortak sütun isimlerini ayırt etmek için kullanılan ekler. Varsayılan olarak c(\".x\", \".y\") olarak ayarlanmıştır.\n\nno.dups: TRUE ise, tekrarlanan sütun adlarını önlemek için daha fazla ekler ekler.\n\nincomparables: Eşleştirilemeyen değerlerle nasıl başa çıkılacağını belirtir. Varsayılan olarak NULL’dır.\n\nÖrnek Veri Çerçevesi Oluşturma - 1\n\n# Örnek veri çerçevesi oluşturma\ndata1 &lt;- data.frame(ID = 1:2,            # ID sütunu, 1 ve 2 değerleri\n                    X1 = c(\"a1\", \"a2\"),  # X1 sütunu, \"a1\" ve \"a2\" değerleri\n                    stringsAsFactors = FALSE) # 'stringsAsFactors' parametresi,\n# karakter sütunlarının faktör yerine karakter olarak saklanmasını sağlar.\n\n# Veri çerçevesini görüntüleme\ndata1\n\n  ID X1\n1  1 a1\n2  2 a2\n\n\nÖrnek Veri Çerçevesi Oluşturma - 2\n\n# İkinci veri çerçevesi oluşturma\ndata2 &lt;- data.frame(ID = 2:3,            # ID sütunu, 2 ve 3 değerleri\n                    X2 = c(\"b1\", \"b2\"),  # X2 sütunu, \"b1\" ve \"b2\" değerleri\n                    stringsAsFactors = FALSE) # 'stringsAsFactors' parametresi,\n# karakter sütunlarının faktör yerine karakter olarak saklanmasını sağlar.\n\n# Veri çerçevesini görüntüleme\ndata2\n\n  ID X2\n1  2 b1\n2  3 b2\n\n\n. . .\n\n\n2.3.1.2 Inner Join - İç Birleştirme İşlemi\nInner join işlemi ile veri çerçevelerini birleştirmek için, birleştirilecek veri çerçevelerinin adlarını (data1 ve data2) ve birleştirme işleminin gerçekleştirileceği ortak sütunun adını (ID sütunu) belirtmek yeterlidir. Bu işlem, iki veri çerçevesinin kesişim kümesini oluşturarak her iki çerçevede de bulunan ortak öğeleri içeren bir yapı sağlar.\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# İç birleştirme işlemi: 'inner_join' kullanımı\ninner_join(data1, data2, by = \"ID\") # 'ID' sütununa göre birleştirme\n\n  ID X1 X2\n1  2 a2 b1\n\n\n\n\n\ninner_join, dplyr paketinin bir fonksiyonudur ve iki veri çerçevesini iç birleştirme (inner join) mantığıyla birleştirir.\n\nİç birleştirme (inner join), yalnızca her iki veri çerçevesinde de ortak olan satırları birleştirir. Ortak bir sütun üzerinden eşleşmeyen satırlar sonuçta yer almaz.\n\nby = \"ID\": Birleştirme işleminin ID sütunu üzerinden yapılacağını belirtir.\nSonuç olarak, sadece ID sütununda ortak olan satırlar birleştirilir ve diğerleri dışlanır.\n\n\n\nInner join, her iki veri çerçevesinde ortak olan satırları birleştirir. Bu işlem sonucunda, her iki veri çerçevesinde de bulunan ID 2 tutulur. Ortaya çıkan sonuç veri çerçevesi, ID 2’ye ait ortak sütunları içerir; bunlar, ilk veri çerçevesinden X1 sütunundaki a2 değeri ve ikinci veri çerçevesinden X2 sütunundaki b1 değeridir. Sonuç olarak, inner join işlemi yalnızca iki veri çerçevesinin ortak öğelerini birleştirir ve diğer öğeleri dışarıda bırakır. Bu yöntem, veri çerçevelerini karşılaştırarak kesişim kümesini oluşturmaya olanak tanır.\n\n2.3.1.3 Left Join - Sol Birleştirme İşlemi\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# Sol birleştirme işlemi: 'left_join' kullanımı\nleft_join(data1, data2, by = \"ID\") # 'ID' sütununa göre birleştirme\n\n  ID X1   X2\n1  1 a1 &lt;NA&gt;\n2  2 a2   b1\n\n\n\n\n\nleft_join fonksiyonu, dplyr paketinin bir fonksiyonudur ve iki veri çerçevesini sol birleştirme mantığıyla birleştirir.\n\nSol birleştirme (left join), birinci veri çerçevesindeki tüm satırları tutar ve ikinci veri çerçevesinden sadece eşleşen değerleri ekler. Eğer ikinci veri çerçevesinde bir eşleşme yoksa, eksik değerler NA olarak atanır.\n\nby = \"ID\": Birleştirme işleminin ID sütunu üzerinden yapılacağını belirtir.\n\n\n\nLeft join, sol veri çerçevesindeki tüm satırları tutar ve sağ veri çerçevesinden yalnızca eşleşen değerleri ekler. Eşleşme yoksa eksik değerler (NA) atanır. Inner join ise, yalnızca her iki veri çerçevesinde ortak olan satırları tutar ve diğer satırları dışlar.\n\n2.3.1.4 Right Join - Sağ Birleştirme İşlemi\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# Sağ birleştirme işlemi: 'right_join' kullanımı\nright_join(data1, data2, by = \"ID\") # 'ID' sütununa göre sağ birleştirme\n\n  ID   X1 X2\n1  2   a2 b1\n2  3 &lt;NA&gt; b2\n\n\n\n\n\nright_join, dplyr paketinin bir fonksiyonudur ve sağ birleştirme (right join) işlemini gerçekleştirir.\n\nSağ Birleştirme (Right Join), sağ veri çerçevesindeki tüm satırları tutar ve sol veri çerçevesinden yalnızca eşleşen değerleri ekler. Eğer sol veri çerçevesinde eşleşen bir satır yoksa, bu satır için eksik değerler (NA) atanır.\n\nby = \"ID\": Birleştirmenin ID sütunu üzerinden yapılacağını belirtir.\n\n\n\nRight join, sağ veri çerçevesindeki tüm satırları korurken sol veri çerçevesinden yalnızca eşleşen değerleri ekler ve eşleşme olmayan satırlar için eksik değerler (NA) atar. Left join ile farkı, Right join’in sağ veri çerçevesini referans alması, Left join’in ise sol veri çerçevesini referans almasıdır. Inner join ile farkı ise, Right join sağ veri çerçevesindeki tüm satırları tutarken, Inner join yalnızca her iki veri çerçevesinde ortak olan satırları döndürmesidir; bu nedenle Inner join eksik değer içermezken, Right join eksik değerler barındırabilir.\n\n2.3.1.5 Full Join - Tam Birleştirme İşlemi\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# Tam birleştirme işlemi: 'full_join' kullanımı\nfull_join(data1, data2, by = \"ID\") # 'ID' sütununa göre tam birleştirme\n\n  ID   X1   X2\n1  1   a1 &lt;NA&gt;\n2  2   a2   b1\n3  3 &lt;NA&gt;   b2\n\n\n\n\n\nfull_join, dplyr paketinin bir fonksiyonudur ve tam birleştirme (full join) işlemini gerçekleştirir.\n\nTüm Birleştirme (full_join), iki veri çerçevesindeki tüm satırları tutar. Her iki veri çerçevesinde de eşleşen satırlar birleştirilir; eşleşmeyen satırlar ise eksik değerler (NA) ile tamamlanır.\n\nby = \"ID\": Birleştirme işleminin ID sütunu üzerinden yapılacağını belirtir.\n\n\n\nFull join, her iki veri çerçevesindeki tüm satırları sonuç veri çerçevesine dahil eder. Ortak olan satırlar birleştirilir, eşleşmeyen satırlar ise eksik değerlerle (NA) tamamlanır. Bu yöntem, iki veri çerçevesinin birleşim kümesini oluşturur. Inner join ise yalnızca her iki veri çerçevesinde ortak olan satırları içerir ve eşleşmeyen satırları dışlar. Bu yöntem, iki veri çerçevesinin kesişim kümesini oluşturur. Full join eksik değerler barındırırken, Inner join sadece ortak değerleri içerdiği için eksik değer içermez.\n\n2.3.1.6 Semi Join - Yarı Birleştirme İşlemi\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# Yarı birleştirme işlemi: 'semi_join' kullanımı\nsemi_join(data1, data2, by = \"ID\") # 'ID' sütununa göre yarı birleştirme\n\n  ID X1\n1  2 a2\n\n\n\n\n\nsemi_join, dplyr paketinin bir fonksiyonudur ve yarı birleştirme (semi join) işlemini gerçekleştirir.\n\nSemi Join, yalnızca birinci veri çerçevesindeki (data1) satırları döndürür, ancak bu satırlar ikinci veri çerçevesiyle (data2) eşleşen satırlardır.\n\nby = \"ID\": Eşleşmenin ID sütunu üzerinden yapılacağını belirtir.\nEşleşen satırlara yalnızca birinci veri çerçevesindeki sütunlar eklenir, ikinci veri çerçevesinin sütunları eklenmez.\n\n\n\nSemi join ve Inner join arasındaki temel fark, sonuç veri çerçevesinin içeriğidir. Inner join, her iki veri çerçevesinde ortak olan satırları birleştirir ve sonuç veri çerçevesine her iki veri çerçevesinin sütunlarını ekler. Semi join ise sadece birinci veri çerçevesindeki ortak satırları döndürür ve ikinci veri çerçevesinin sütunlarını sonuçta dahil etmez. Semi join, eşleşen satırları birinci veri çerçevesi perspektifinden filtrelemek için kullanılırken, Inner join, iki veri çerçevesinin kesişimini oluşturur.\n\n2.3.1.7 Anti Join - Anti Birleştirme İşlemi\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# Anti birleştirme işlemi: 'anti_join' kullanımı\nanti_join(data1, data2, by = \"ID\") # 'ID' sütununa göre anti join işlemi\n\n  ID X1\n1  1 a1\n\n\n\n\n\nanti_join, dplyr paketinin bir fonksiyonudur ve ters birleştirme (anti join) işlemini gerçekleştirir.\n\nAnti Join, birinci veri çerçevesindeki (data1) ve ikinci veri çerçevesindeki (data2) satırları karşılaştırır ve yalnızca ikinci veri çerçevesiyle eşleşmeyen birinci veri çerçevesi satırlarını döndürür.\n\nby = \"ID\": Eşleşmenin ID sütunu üzerinden yapılacağını belirtir.\n\n\n\n\n2.3.2 Veri Eklemeler: bind_rows(), bind_cols()\n\n\n2.3.2.1 Satır Ekleme: bind_rows()\n\nbind_rows(), birden fazla veri çerçevesini satır bazında birleştirmek için kullanılan bir fonksiyondur. Bu fonksiyon, sütun isimlerini eşleştirerek çalışır ve eksik sütunlar varsa bu sütunlara eksik değerler (NA) atar. rbind() ile farkı, bind_rows()’un sütun sıralarına bağlı kalmadan sütun isimlerini dikkate alarak birleştirme yapabilmesidir. Ayrıca, bind_rows(), sütunları eksik olan veri çerçevelerinde hata vermek yerine eksik değerler ekleyerek birleştirmeyi tamamlar. Buna karşın, rbind(), veri çerçevelerindeki sütun isimlerinin ve sıralarının birebir aynı olmasını gerektirir; aksi halde hata verir. Bu nedenle, bind_rows(), esneklik ve kullanım kolaylığı açısından genellikle daha avantajlıdır.\n\n# Örnek veri çerçevesi 1: data1 oluşturma\ndata1 &lt;- data.frame(\n  x1 = 1:5,          # Sayılar 1'den 5'e kadar\n  x2 = letters[1:5]  # Harfler 'a' ile 'e' arasında\n)\ndata1\n\n  x1 x2\n1  1  a\n2  2  b\n3  3  c\n4  4  d\n5  5  e\n\n# Örnek veri çerçevesi 2: data2 oluşturma\ndata2 &lt;- data.frame(\n  x1 = 0,            # Sabit bir değer\n  x3 = 5:9           # Sayılar 5'ten 9'a kadar\n)\ndata2\n\n  x1 x3\n1  0  5\n2  0  6\n3  0  7\n4  0  8\n5  0  9\n\n# Örnek veri çerçevesi 3: data3 oluşturma\ndata3 &lt;- data.frame(\n  x3 = 5:9,          # Sayılar 5'ten 9'a kadar\n  x4 = letters[5:9]  # Harfler 'e' ile 'i' arasında\n)\ndata3\n\n  x3 x4\n1  5  e\n2  6  f\n3  7  g\n4  8  h\n5  9  i\n\n\n\nBu kodda, üç farklı veri çerçevesi oluşturuluyor:\n\n\ndata1: x1 ve x2 sütunlarından oluşur. x1 sayılar, x2 ise harfler içerir.\n\ndata2: x1 ve x3 sütunlarından oluşur. x1 sabit bir değer, x3 ise bir sayı aralığıdır.\n\ndata3: x3 ve x4 sütunlarından oluşur. x3 sayılar, x4 ise harfler içerir.\n\n\n\nSatır Bazında Birleştirme İşlemi\n\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# Satır bazında birleştirme\nresult &lt;- bind_rows(data1, data2)\n\n# Sonucu görüntüleme\nresult\n\n   x1   x2 x3\n1   1    a NA\n2   2    b NA\n3   3    c NA\n4   4    d NA\n5   5    e NA\n6   0 &lt;NA&gt;  5\n7   0 &lt;NA&gt;  6\n8   0 &lt;NA&gt;  7\n9   0 &lt;NA&gt;  8\n10  0 &lt;NA&gt;  9\n\n\n\n\n\nbind_rows(), dplyr paketinden bir fonksiyon olup birden fazla veri çerçevesini satır bazında birleştirir.\nSütunlar eşleştiği sürece, eksik sütunlara NA atanarak işlemi tamamlar.\nBu örnekte, data1 ve data2 satır bazında birleştirilir ve sonuçta her iki veri çerçevesinin birleşimi elde edilir.\nEğer data1 ve data2’de eksik sütunlar varsa, bind_rows() eksik sütunları NA ile doldurur.\n\n\n\n2.3.2.2 Sütun ekleme: bind_cols()\n\nbind_cols(), birden fazla veri çerçevesini sütun bazında birleştirmek için kullanılan bir fonksiyondur. Bu fonksiyon, satır sayılarını dikkate alarak veri çerçevelerini yan yana birleştirir. Eğer veri çerçevelerinde farklı sayıda satır varsa, eksik satırlar için NA eklenir. cbind() ile farkı, bind_cols()’un daha esnek olmasıdır; sütunları birleştirirken veri çerçevelerinin isimlendirilmiş sütun yapılarını korur ve modern veri işleme ihtiyaçlarına daha uygun şekilde çalışır. Buna karşılık, cbind(), sütun isimlerine bakmaz ve uyumsuz satır sayılarına sahip veri çerçevelerinde hata verir. Bu nedenle, bind_cols(), sütun bazında birleştirme işlemlerinde kullanım kolaylığı ve hata toleransı açısından daha avantajlıdır.\n\nSütun Bazında Birleştirme İşlemi\n\n\n# Gerekli paketin yüklenmesi\nlibrary(dplyr)\n\n# Sütun bazında birleştirme\nresult &lt;- bind_cols(data1, data3)\n\n# Sonucu görüntüleme\nresult\n\n  x1 x2 x3 x4\n1  1  a  5  e\n2  2  b  6  f\n3  3  c  7  g\n4  4  d  8  h\n5  5  e  9  i\n\n\n\n\n\nbind_cols(), dplyr paketinden bir fonksiyon olup birden fazla veri çerçevesini sütun bazında birleştirir.\nSatır sayılarını dikkate alır; eğer veri çerçevelerindeki satır sayıları eşleşmezse, eksik satırlar için NA ekler.\nBu örnekte, data1 ve data3 sütun bazında birleştirilir ve iki veri çerçevesi yan yana eklenerek yeni bir veri çerçevesi oluşturulur.",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Veri Manipülasyonu</span>"
    ]
  },
  {
    "objectID": "veri_manipulasyonu.html#metin-string-manipülasyonu",
    "href": "veri_manipulasyonu.html#metin-string-manipülasyonu",
    "title": "\n2  Veri Manipülasyonu\n",
    "section": "\n2.4 Metin (String) Manipülasyonu",
    "text": "2.4 Metin (String) Manipülasyonu\n\n2.4.1 Temel Metin Manipülasyonu\n\n2.4.1.1 Metin Birleştirme\nstr_c(), stringr paketinde bulunan ve metinleri birleştirmek için kullanılan güçlü bir fonksiyondur. Bu fonksiyon, birden fazla metin girdisini yan yana getirerek tek bir metin oluşturur. str_c() fonksiyonu, temel birleştirme işlemlerinin yanı sıra, sep argümanı ile birleştirme sırasında kullanılacak ayırıcı karakteri belirleme, vektörleri birleştirme ve farklı veri tiplerini metin olarak birleştirme gibi birçok esnek özellik sunar.\n\nTemel Metin Birleştirme İşlemi (str_c())\n\n\n\n# Gerekli paketlerin yüklenmesi\n# install.packages(\"stringr\")\nlibrary(stringr)\n\n# Örnek metinler\ntext1 &lt;- \"Merhaba\"\ntext2 &lt;- \"Dunya\"\ntext3 &lt;- \"!\"\n\n# Temel metin birleştirme\nresult_basic &lt;- str_c(text1, text2, text3)\n\n# Sonucu görüntüleme\nresult_basic\n\n[1] \"MerhabaDunya!\"\n\n\n\n1. str_c() Fonksiyonu:\n\n\nAçıklama: str_c() fonksiyonu, stringr paketine ait bir fonksiyondur ve bir veya daha fazla metni birleştirmek için kullanılır. Bu fonksiyon, verilen metinleri ardışık olarak birleştirir.\n\nArgümanlar: text1, text2, text3: Birleştirilecek metinlerdir.\n\nSonuç: Verilen metinler, aralarındaki boşluk veya herhangi bir ek karakter olmadan doğrudan birleştirilir.\n\n2. Birleştirme İşlemi:\n\n\nAçıklama: str_c(text1, text2, text3) kodu, text1, text2, ve text3 metinlerini birleştirir. Sonuç olarak \"MerhabaDunya!\" metni elde edilir.\n\n\n\nsep Argümanı ile Metin Birleştirme\n\n\n# Gerekli paketlerin yüklenmesi\nlibrary(stringr)\n\n# Örnek metinler\ntext1 &lt;- \"Merhaba\"\ntext2 &lt;- \"Dunya\"\ntext3 &lt;- \"!\"\n\n# sep argümanı ile metin birleştirme\nresult_sep &lt;- str_c(text1, text2, text3, sep = \" \") \n# 'sep = \" \"', metinler arasına boşluk ekler.\n\n# Sonucu görüntüleme\nresult_sep\n\n[1] \"Merhaba Dunya !\"\n\n\n\nstr_c() Fonksiyonu ile Veri Çerçevesinde Metin Birleştirme\n\n\n# Örnek veri çerçevesi oluşturma\ndata &lt;- data.frame(\n  ad = c(\"Ali\", \"Ayse\", \"Veli\"),\n  soyad = c(\"Yilmaz\", \"Demir\", \"Kara\"),\n  yas = c(25, 30, 28))\n\n# Gerekli kütüphanenin yüklenmesi\nlibrary(stringr)\n\n# 'ad_soyad_yas' adında yeni bir sütun oluşturuluyor ve bu sütun,\n# 'ad', 'soyad' ve 'yas' sütunlarının birleşiminden oluşuyor\ndata$ad_soyad_yas &lt;- str_c(   # str_c() fonksiyonu bu birleştirme işlemini yapıyor.\n  data$ad,           # 'ad' sütunundaki değerler \n  data$soyad,        # 'soyad' sütunundaki değerler \n  \" (\",              # Metin içerisinde sabit bir karakter dizisi, parantez açma\n  data$yas,          # 'yas' sütunundaki değerler (örneğin: 25)\n  \" yas)\",           \n  # Metin içerisinde sabit bir karakter dizisi, parantez kapama ve 'yas' kelimesi\n  sep = \" \"          # Birleştirilen metin parçaları arasına boşluk ekleniyor.\n)\n\n# Sonuç veri çerçevesini görüntüleme\ndata\n\n    ad  soyad yas           ad_soyad_yas\n1  Ali Yilmaz  25 Ali Yilmaz  ( 25  yas)\n2 Ayse  Demir  30 Ayse Demir  ( 30  yas)\n3 Veli   Kara  28  Veli Kara  ( 28  yas)\n\n\n\n\n\n\n\n\nstr_c() vs. paste()\n\n\n\nstr_c() (stringr paketi) ve paste() (base R) Karşılaştırması\n\n# stringr::str_c() örneği\nlibrary(stringr)\nmetin1 &lt;- c(\"a\", \"b\", \"c\")\nmetin2 &lt;- c(1, 2, 3)\nsonuc_str_c &lt;- str_c(metin1, metin2, sep = \"-\")\nsonuc_str_c  # \"a-1\" \"b-2\" \"c-3\"\n\n[1] \"a-1\" \"b-2\" \"c-3\"\n\n# base::paste() örneği\nmetin1 &lt;- c(\"a\", \"b\", \"c\")\nmetin2 &lt;- c(1, 2, 3)\nsonuc_paste &lt;- paste(metin1, metin2, sep = \"-\")\nsonuc_paste  # \"a-1\" \"b-2\" \"c-3\"\n\n[1] \"a-1\" \"b-2\" \"c-3\"\n\n# NA değerler ile paste ve str_c kullanımı\nmetin3 &lt;- c(\"a\", \"b\", NA)\nmetin4 &lt;- c(1, 2, 3)\n\nstr_c(metin3, metin4, sep = \"-\")  # \"a-1\" \"b-2\" NA\n\n[1] \"a-1\" \"b-2\" NA   \n\npaste(metin3, metin4, sep = \"-\") # \"a-1\" \"b-2\" \"NA-3\"\n\n[1] \"a-1\"  \"b-2\"  \"NA-3\"\n\n# collapse argümanı ile paste kullanımı\npaste(metin1, collapse = \", \") # \"a, b, c\"\n\n[1] \"a, b, c\"\n\n# str_c()'de collapse argümanı yoktur.\n\n\n\n\n2.4.1.2 Metin Uzunluğu\nstr_length() fonksiyonu, stringr paketine ait bir fonksiyondur ve verilen metnin karakter sayısını döndürür. Bu fonksiyon, boşluklar da dahil olmak üzere tüm karakterleri sayar. Örnek metinler oluşturularak farklı uzunluklardaki ve özelliklerdeki (boşluklu, boşluksuz) metinlerin uzunlukları str_length() fonksiyonu ile hesaplanır. Ardından cat() fonksiyonu kullanılarak sonuçlar ekrana yazdırılır. str_length() fonksiyonunun bir diğer kullanım şekli de, bir veri çerçevesindeki metinlerin uzunluklarını hesaplayıp, yeni bir sütuna kaydetmektir. Bu işlem sırasında stringAsFactors = FALSE argümanı kullanılarak metinlerin faktör değil, string olarak kalması sağlanır.\n\nstr_length() Fonksiyonu ile Metin Uzunluğu Hesaplama\n\n\n# Gerekli kütüphanenin yüklenmesi\nlibrary(stringr)\n\n# str_length() fonksiyonu, stringr paketine ait bir fonksiyondur ve verilen metnin\n# karakter sayısını döndürür. Bu fonksiyon, boşluklar da dahil olmak üzere tüm\n# karakterleri sayar.\n\n# Örnek metinler\ntext1 &lt;- \"Hello World\"\ntext2 &lt;- \"  Space  \"\ntext3 &lt;- \"MerhabaDunya!\"\n\n# Metinlerin uzunlukları str_length() fonksiyonu ile hesaplanır.\nlength1 &lt;- str_length(text1)\nlength2 &lt;- str_length(text2)\nlength3 &lt;- str_length(text3)\n\n# cat() fonksiyonu ile sonuçlar ekrana yazdırılır.\ncat(\"Text 1:'\",text1, \"\\n\", \"'- Length:\", length1, \"\\n\")\n\nText 1:' Hello World \n '- Length: 11 \n\ncat(\"Text 2: '\", text2, \"' - Length: \", length2)\n\nText 2: '   Space   ' - Length:  9\n\ncat(\"Text 3: '\", text3, \"' - Length: \", length3, \"\\n\")\n\nText 3: ' MerhabaDunya! ' - Length:  13 \n\n\n\n\n\n\n\n\ncat() fonksyonu ve işlevleri\n\n\n\ncat() Fonksiyonu\n\n\nTemel İşlevi: cat() fonksiyonu, R’da metinleri ve diğer değerleri (sayılar, mantıksal değerler vb.) ekrana yazdırmak için kullanılan bir temel fonksiyondur.\n\nÇıktı Biçimi: cat(), print() fonksiyonuna göre daha basit bir çıktı üretir. Genellikle, çıktıya ek bilgiler (satır numaraları, değişken isimleri vb.) eklemez, sadece verilen metni ve değerleri yan yana yazdırır.\n\nBirleştirme: cat() fonksiyonu, birden fazla metin veya değeri birleştirmek için kullanılabilir. Bu, farklı türdeki bilgileri aynı satırda görüntülemek için oldukça kullanışlıdır.\n\nAyırıcı Karakter: cat() fonksiyonu varsayılan olarak metinleri bir boşlukla ayırır, ancak bu davranış sep argümanı ile değiştirilemez. sep argümanı print() fonksiyonunda vardır.\n\nYeni Satır Karakteri: cat() fonksiyonunda yeni bir satıra geçmek için \\n kaçış dizisi (escape sequence) kullanılır. Bu sayede çıktı daha düzenli hale getirilebilir.\n\nGenel Kullanım: cat(), özellikle döngüler içinde veya koşullu çıktıların oluşturulmasında kullanışlıdır. Gelişmiş çıktılar veya değişkenlere ait bilgilerin görüntülenmesi için print() veya message() fonksiyonları daha uygun olabilir.\n\nÖzet:\ncat() fonksiyonu, R’da metin ve değerleri basit bir şekilde ekrana yazdırmak için kullanılan bir araçtır. Özellikle birden fazla metni birleştirmek ve formatlı çıktılar oluşturmak için kullanışlıdır. print() fonksiyonuna göre daha yalın bir çıktı verir ve sep argümanı kullanılamaz, ancak \\n kaçış dizisi ile yeni satır eklenebilir.\n\n\n\nstr_length() Fonksiyonu ile Veri Çerçevesinde Yeni Sütun Eklemek\n\n\n# Gerekli paketlerin yüklenmesi\nlibrary(stringr)\n\n# str_length() fonksiyonunun bir diğer kullanım şekli de, bir veri çerçevesindeki\n# metinlerin uzunluklarını hesaplayıp, yeni bir sütuna kaydetmektir.\n\n# Örnek bir veri çerçevesi oluşturulur\ndata_fruits &lt;- data.frame(\n  text = c(\"apple\", \"banana\", \"cherry\", \"date\"),\n  stringsAsFactors = FALSE # Metinlerin faktör değil string olarak kalmasını sağlar\n)\n\n# str_length() fonksiyonu kullanılarak metinlerin uzunlukları hesaplanır ve \n# yeni bir sütuna eklenir\ndata_fruits$text_length &lt;- str_length(data_fruits$text)\n\n# Sonucu görüntüleme\ndata_fruits\n\n    text text_length\n1  apple           5\n2 banana           6\n3 cherry           6\n4   date           4\n\n\n\n\n\n\n\n\n$ İşareti ile Yeni Sütun/Değişken Eklemek\n\n\n\nR dilinde, veri çerçevelerine yeni sütun eklemek için $ işareti sıkça kullanılır. Bu, veri çerçevesine doğrudan yeni bir sütun eklemeye olanak tanır.\nVeri Çerçevesinde Yeni Değişken Ekleme:\nVeri çerçevesine yeni bir sütun (veya değişken) eklemek için şu adımları izlersiniz:\n\n\n$ işareti kullanarak yeni sütunun adını belirtirsiniz.\nSağdaki kısımda, eklemek istediğiniz değişkenin hesaplamasını veya değerini belirtirsiniz.\n\n\n# data$new_column &lt;- 5  # Tüm satırlar için 5 değeri eklenir.\n\n\nBu işlem, data veri çerçevesine new_column adında yeni bir sütun ekler ve tüm satırlara 5 değeri atar.\n\nVeri Çerçevesine Hesaplanan Değer Ekleme:\nBir sütun, başka sütunlardaki değerlerin hesaplanması ile de eklenebilir. Örneğin, metin uzunlukları, sayılar, vb. hesaplanarak yeni bir sütun oluşturulabilir.\n\n# data$new_column &lt;- data$var1 + data$var2  # var1 ve var2 sütunlarının toplamı\n\n\nBu örnekte, data$new_column sütunu, data$var1 ve data$var2 sütunlarının toplamını içerir.\n\nÖzet:\n\n\n$ işareti, veri çerçevesine yeni bir sütun eklerken kullanılır. Yeni sütunun adı belirlenir ve sağdaki kısımda bu sütuna atanacak değerler veya hesaplamalar belirtilir.\n\nYeni sütun eklemek için: data$new_column &lt;- value şeklinde kullanılır. Bu, veri çerçevesine new_column adında bir sütun ekler ve value’yu bu sütuna atar.\nBu yöntem, veri çerçevesinde hızlı bir şekilde yeni değişkenler oluşturmanızı sağlar ve veri manipülasyonu için oldukça kullanışlıdır.\n\n\n\n\n2.4.1.3 Metin Dönüştürme\n\nBüyük/Küçük Harf Dönüşümleri: str_to_lower() ve str_to_upper()\n\nBu fonksiyonlar, karakter dizilerindeki harfleri tamamen küçük veya büyük harfe dönüştürmek için kullanılır. Bu işlem, metin normalizasyonu ve karşılaştırma gibi işlemlerde oldukça faydalıdır.\n\nstr_to_lower(): Metni tamamen küçük harfe dönüştürür.\nstr_to_upper(): Metni tamamen büyük harfe dönüştürür.\n\n\n# Gerekli paketlerin yüklenmesi\nlibrary(stringr)\n\n# Küçük harfe dönüştürme\ntext &lt;- \"Merhaba Dunya!\"\nlower_text &lt;- str_to_lower(text)\nprint(lower_text) # Çıktı: \"merhaba dunya!\"\n\n[1] \"merhaba dunya!\"\n\n# Büyük harfe dönüştürme\nupper_text &lt;- str_to_upper(text)\nprint(upper_text) # Çıktı: \"MERHABA DÜNYA!\"\n\n[1] \"MERHABA DUNYA!\"\n\n\n\nAlt Dizeleri Çıkarma: str_sub()\n\nBu fonksiyon, bir karakter dizisinden belirli bir başlangıç ve bitiş pozisyonuna göre alt dizeler çıkarmak veya mevcut bir alt diziyi değiştirmek için kullanılır.\n\n# Gerekli kütüphanenin yüklenmesi\nlibrary(stringr)\n\n# Örnek bir telefon numarası tanımlanır\nphone_number &lt;- \"+90 123 456 7890\" \n\n# str_sub() fonksiyonu kullanılarak telefon numarasının alan kodu alınır\narea_code &lt;- str_sub(phone_number, 1, 3) # 1.'den 3. karaktere kadar olan kısmı alır.\n\n# Sonucu ekrana yazdırma\nprint(area_code) # Çıktı: \"+90\"\n\n[1] \"+90\"\n\n\n\nBaş ve Sondaki Boşlukları Temizleme: str_trim()\n\nBu fonksiyon, bir metnin başındaki ve sonundaki boşlukları temizlemek için kullanılır. side argümanıyla, sadece baştan, sondan veya her iki taraftan boşlukları temizleme seçeneği sunar.\n\nstr_trim(string): Baş ve sondaki boşlukları temizler.\nstr_trim(string, side = \"left\"): Sadece baştaki boşlukları temizler.\nstr_trim(string, side = \"right\"): Sadece sondaki boşlukları temizler.\n\n\n# Gerekli kütüphanenin yüklenmesi\nlibrary(stringr)\n\n# Kullanıcı girdisi (boşluklu)\nuser_input &lt;- \"   Kullanici Adi   \"  \n\n# str_trim() fonksiyonu kullanılarak baştaki ve sondaki boşluklar temizlenir\ncleaned_input &lt;- str_trim(user_input)  # Boşlukları temizler\n\n# Sonucu ekrana yazdırma\nprint(cleaned_input) # Çıktı: \"Kullanici Adi\"\n\n[1] \"Kullanici Adi\"\n\n\n\n2.4.2 Desen Eşleştirme ve Değiştirme\nstr_detect() fonksiyonu, bir karakter vektöründe belirli bir desenin varlığını kontrol etmek için kullanılır. Bu fonksiyon, bir mantıksal vektör (TRUE/FALSE) döndürür.\n\nlibrary(stringr)\n\n#str_detect(string, #Aranacak metin ya da metin vektörü.\n           #pattern) #Aranacak desen (düz metin ya da regex).\n\nstr_detect() Fonksiyonu ile Desen Arama\n\n# Gerekli kütüphanenin yüklenmesi\nlibrary(stringr)\n\n# Örnek metinler\ntext_vector &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\", \"apricot\")\n\n# Desen arama örnekleri\n\n# \"a\" içeren kelimeler\nhas_a &lt;- str_detect(text_vector, \"a\")\nhas_a\n\n[1]  TRUE  TRUE FALSE  TRUE  TRUE\n\n# \"ap\" ile başlayan kelimeler\nhas_ap &lt;- str_detect(text_vector, \"^ap\")\nhas_ap\n\n[1]  TRUE FALSE FALSE FALSE  TRUE\n\n# \"e\" ile biten kelimeler\nhas_e &lt;- str_detect(text_vector, \"e$\")\nhas_e\n\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\n# Rakam içeren kelimeler (bu örnekte yok)\nhas_num &lt;- str_detect(text_vector, \"\\\\d\")\nhas_num\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\n\nYukarıdaki kodda, text_vector adında bir metin vektörü tanımlanır. Ardından, str_detect() fonksiyonu kullanılarak bu vektördeki metinlerde belirli desenler aranır. Örneğin, “a” harfini içeren kelimeler, “ap” ile başlayan kelimeler, “e” ile biten kelimeler ve rakam içeren kelimeler kontrol edilir. Sonuçlar, TRUE veya FALSE değerlerinden oluşan mantıksal bir vektör olarak döndürülür.\n\n\n2.4.2.1 Desen Değiştirme: str_replace() ve str_replace_all()\n\nstr_replace() ve str_replace_all() fonksiyonları, metin içindeki belirli desenleri değiştirmek için kullanılır. str_replace() fonksiyonu, desenin ilk eşleşmesini değiştirirken, str_replace_all() fonksiyonu desenin tüm eşleşmelerini değiştirir.\n\n# Gerekli kütüphanenin yüklenmesi\nlibrary(stringr)\n\n# Örnek metin\ntext2 &lt;- \"red apple, green apple, red banana\"\n\n# İlk eşleşmeyi değiştirme\nreplaced_text1 &lt;- str_replace(text2, \"apple\", \"orange\")\nreplaced_text1\n\n[1] \"red orange, green apple, red banana\"\n\n# Tüm eşleşmeleri değiştirme\nreplaced_text2 &lt;- str_replace_all(text2, \"apple\", \"orange\")\nreplaced_text2\n\n[1] \"red orange, green orange, red banana\"\n\n\n\n2.4.3 Regex Temelleri (Düzenli İfadeler - Regular Expressions)\nTemel Regex Karakterleri\n\n\n\n\n\n\n\nKarakter\nAnlamı\nÖrnek\n\n\n\n.\nHerhangi bir karakter\n\na.b → “acb”, “a2b”\n\n\n*\nÖnceki karakter 0 veya daha fazla kez\n\nab* → “a”, “ab”, “abb”\n\n\n+\nÖnceki karakter 1 veya daha fazla kez\n\nab+ → “ab”, “abb”\n\n\n?\nÖnceki karakter 0 veya 1 kez\n\nab? → “a”, “ab”\n\n\n[]\nKarakter kümesi\n\n[aeiou] → “a”, “e”, “o”\n\n\n^\nBaşlangıç\n\n^abc → “abcdef”\n\n\n$\nBitiş\n\nxyz$ → “123xyz”\n\n\n\\d\nRakam (digit)\n\n\\d+ → “123”, “45”\n\n\n\\w\nKelime karakteri\n\n\\w+ → “abc”, “123”, “word1”\n\n\n\\s\nBoşluk karakteri\n\n\\s+ → ” “,” ”\n\n\n\nRegex ile str_detect(), str_replace(), ve str_replace_all() Kullanımı\n\n# Örnek metinler\nmetin &lt;- c(\"abc123\", \"def456\", \"ghi\")\n# - \"abc123\": Hem harf hem rakam içerir.\n# - \"def456\": Hem harf hem rakam içerir.\n# - \"ghi\": Harf içerir.\n\n# Desen arama işlemi\nsonuc &lt;- str_detect(metin, \"\\\\d+\")\n# - \"\\\\d+\": Bir veya daha fazla rakamı arayan düzenli ifade (regex).\n# - Dönen sonuç: Her bir öğe için TRUE/FALSE (mantıksal vektör).\n\nprint(sonuc)\n\n[1]  TRUE  TRUE FALSE\n\n# Açıklama: Her bir öğe en az bir rakam içerdiği için tüm sonuçlar TRUE döner.\n\nÖrnek Kullanım:\n\n# Gerekli paketlerin yüklenmesi\n# install.packages(\"kableExtra\")\nlibrary(kableExtra)\nlibrary(tidyverse)\nlibrary(stringr)\n\n# Örnek tibble oluşturma\ndata_regex &lt;- tibble(\n  ID = 1:5,\n  Customer_Info = c(\n    \" John Doe, john.doe@example.com \",\n    \" Jane Smith, JANE.SMITH@example.com \",\n    \" Bob Brown, BOB.BROWN@example.com \",\n    \" Alice Johnson, alice.johnson@example.com \",\n    \" Carol White, carol.white@example.com \"\n  )\n)\n\n# Veri manipülasyonu\nprocessed_data_regex &lt;- data_regex %&gt;%\n  # 1. str_trim(): Baş ve sondaki boşlukları temizleme\n  mutate(Customer_Info = str_trim(Customer_Info)) %&gt;%\n  \n  # 2. str_detect(): E-posta adreslerinin doğru formatta olup olmadığını kontrol etme\n  mutate(Valid_Email = str_detect(Customer_Info, \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\")) %&gt;%\n  \n  # 3. str_sub(): İsim ve e-posta adreslerini ayıklama\n  mutate(\n    Name = str_sub(Customer_Info, 1, str_locate(Customer_Info, \",\")[, 1] - 1), \n    # Virgülden önceki kısmı isim olarak al\n    Email = str_sub(Customer_Info, str_locate(Customer_Info, \",\")[, 1] + 2)    \n    # Virgülden sonraki kısmı e-posta olarak al\n  ) %&gt;%\n    \n    # 4. str_to_lower() ve str_to_upper(): İsimleri büyük harf, e-posta adreslerini\n    # küçük harf yapma\n  mutate(\n    Name = str_to_upper(Name),\n    Email = str_to_lower(Email)\n  ) %&gt;%\n  \n  # 5. str_length(): İsmin uzunluğunu hesaplama\n  mutate(Name_Length = str_length(Name)) %&gt;%\n  \n  # 6. str_replace(): E-postalardaki \"example.com\" kısmını \"domain.com\" ile değiştirme\n  mutate(Email = str_replace(Email, \"example.com\", \"domain.com\")) %&gt;%\n  \n  # 7. str_c(): İsim ve yeni e-posta adresini birleştirerek tam müşteri bilgisi oluşturma\n  mutate(Updated_Info = str_c(Name, \" &lt;\", Email, \"&gt;\"))\n\n# Sonuçları göster\nkableExtra::kable(processed_data_regex)\n\n\n\nID\nCustomer_Info\nValid_Email\nName\nEmail\nName_Length\nUpdated_Info\n\n\n\n1\nJohn Doe, john.doe@example.com\nTRUE\nJOHN DOE\njohn.doe@domain.com\n8\nJOHN DOE john.doe@domain.com\n\n\n\n2\nJane Smith, JANE.SMITH@example.com\nTRUE\nJANE SMITH\njane.smith@domain.com\n10\nJANE SMITH jane.smith@domain.com\n\n\n\n3\nBob Brown, BOB.BROWN@example.com\nTRUE\nBOB BROWN\nbob.brown@domain.com\n9\nBOB BROWN bob.brown@domain.com\n\n\n\n4\nAlice Johnson, alice.johnson@example.com\nTRUE\nALICE JOHNSON\nalice.johnson@domain.com\n13\nALICE JOHNSON alice.johnson@domain.com\n\n\n\n5\nCarol White, carol.white@example.com\nTRUE\nCAROL WHITE\ncarol.white@domain.com\n11\nCAROL WHITE carol.white@domain.com\n\n\n\n\n\n\n\n2.4.4 Janitor Paketi ile Veri Temizleme\n\n# Gerekli paketlerin yüklenmesi\nlibrary(janitor)\n\n# Örnek veri çerçevesi\ndata_janitor &lt;- data.frame(\n  \"First Name\" = c(\"Ali\", \"Ayse\", \"Mehmet\"),\n  \"LastNAME\" = c(\"Kara\", \"Demir\", \"Yilmaz\"),\n  \"DateOFBirth\" = c(\"1990-01-01\", \"1985-05-12\", \"2000-07-22\"),\n  stringsAsFactors = FALSE\n)\n\n# Orijinal veri çerçevesi\nprint(data_janitor)\n\n  First.Name LastNAME DateOFBirth\n1        Ali     Kara  1990-01-01\n2       Ayse    Demir  1985-05-12\n3     Mehmet   Yilmaz  2000-07-22\n\n# clean_names() fonksiyonu ile sütun isimlerini temizleme\ncleaned_data_janitor &lt;- clean_names(data_janitor)\n\n# Temizlenmiş veri çerçevesi\nprint(cleaned_data_janitor)\n\n  first_name last_name date_of_birth\n1        Ali      Kara    1990-01-01\n2       Ayse     Demir    1985-05-12\n3     Mehmet    Yilmaz    2000-07-22",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Veri Manipülasyonu</span>"
    ]
  },
  {
    "objectID": "veri_manipulasyonu.html#fonksiyonlarla-veri-manipülasyonu",
    "href": "veri_manipulasyonu.html#fonksiyonlarla-veri-manipülasyonu",
    "title": "\n2  Veri Manipülasyonu\n",
    "section": "\n2.5 Fonksiyonlarla Veri Manipülasyonu",
    "text": "2.5 Fonksiyonlarla Veri Manipülasyonu\n\n2.5.1 Fonksiyon Uygulamaları:\n\n\napply(), sapply(), lapply() gibi fonksiyonlar ile veri üzerinde işlemler gerçekleştirme.\n\n2.5.2 Gruplama İşlemleri:\n\n\ntapply() fonksiyonu ile gruplar üzerinde hesaplamalar yapma.\n\n2.5.3 Kesim ve Sınıflandırma:\n\n\ncut() fonksiyonu ile sayısal veriyi sınıflara ayırma.\n\nReferanslar\nhttps://www.tidyverse.org/\nhttps://dplyr.tidyverse.org/\nhttps://tibble.tidyverse.org/\nhttps://stringr.tidyverse.org/\nhttps://r4ds.hadley.nz/\nhttps://mine-cetinkaya-rundel.github.io/r4ds-solutions/\nhttps://www.geeksforgeeks.org/merge-function-in-r/\nhttps://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\nhttps://statisticsglobe.com/r-bind_rows-bind_cols-functions-dplyr-package\nhttps://r-coder.com/r-data-manipulation/\nhttps://r-primers.andrewheiss.com/\nhttps://app.datacamp.com/learn/courses/data-manipulation-with-dplyr",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Veri Manipülasyonu</span>"
    ]
  },
  {
    "objectID": "veri_temizleme.html",
    "href": "veri_temizleme.html",
    "title": "\n3  Veri Temizleme\n",
    "section": "",
    "text": "3.1 Eksik Verilerle Çalışma\nBüyük veri, hacim, hız, çeşitlilik, doğruluk ve değer gibi özellikleriyle öne çıkar. Bu karmaşık veri dünyasında anlamlı bilgiler çıkarma ve analiz süreçlerini yönetme görevi veri bilimine düşmektedir. Ancak, büyük veri analizi sürecinde en sık karşılaşılan zorluklardan biri eksik verilerdir. Eksik veriler, genellikle yanıt eksiklikleri veya veri kaybı gibi nedenlerle ortaya çıkar ve bu durum, analiz sonuçlarının doğruluğunu ve güvenilirliğini tehlikeye atar. Eksik veri problemi, istatistiksel gücün azalması, parametre tahminlerinde yanlılık, örneklemlerin temsil gücünün zayıflaması ve analiz süreçlerinin karmaşıklaşması gibi sorunlara yol açabilir. Bu nedenle, eksik verilerle doğru bir şekilde başa çıkmak, sağlam ve güvenilir analiz sonuçları elde etmek için kritik öneme sahiptir.\nhttps://www.rpubs.com/justjooz/miss_data\nEksik veri yönetimi, veri analizi sürecinin temel yapı taşlarından biri olarak değerlendirilmelidir. Bu bağlamda, eksik verilerin tespiti ve görselleştirilmesi için naniar paketi kullanılabilir; özellikle vis_miss() fonksiyonu, eksik veri desenlerini analiz etmek için etkili bir araçtır. Eksik verileri doldurma yöntemleri arasında, özellikle çok değişkenli veri setleri için uygun olan mice paketi dikkat çeker. Bu paket, birden fazla doldurma yöntemi sunarak, veri setinin istatistiksel gücünü ve temsiliyetini artırır. Ayrıca, eksik veri profillerini detaylı bir şekilde analiz etmek ve raporlamak için dlookr paketi gibi araçlardan yararlanmak mümkündür. Eksik veri yönetiminde kullanılan bu yaklaşımlar, veri analistlerinin daha doğru öngörüler yapmasını sağlayarak, stratejik karar alma süreçlerine destek olur. Böylece, eksik veri probleminin üstesinden gelmek için yöntem seçimi ve uygulaması, veri analizinde güvenilirlik ve doğruluk açısından vazgeçilmez bir süreçtir.\nVeri Seti airquality\n# Gerekli kütüphanelerin yüklenmesi\nlibrary(dplyr)\n# datasets paketini yükleme (otomatik olarak yüklü olmalı)\nlibrary(datasets)\n\n# airquality veri setinin görüntülenmesi\nhead(airquality, 10)\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Veri Temizleme</span>"
    ]
  },
  {
    "objectID": "veri_temizleme.html#eksik-verilerle-çalışma",
    "href": "veri_temizleme.html#eksik-verilerle-çalışma",
    "title": "\n3  Veri Temizleme\n",
    "section": "",
    "text": "Airquality Veri Seti\nairquality veri seti, 1973 yılında New York’ta ölçülen hava kalitesi değerlerini içeren bir veri setidir. Bu veri seti, hava kalitesini etkileyen çeşitli değişkenleri içerir ve çevresel analizler için kullanılır. Veri seti, aşağıdaki değişkenlerden oluşur:\n\n\nOzone: Ozon seviyelerini ifade eder (ppb - parts per billion).\n\nSolar.R: Solar radyasyon değerlerini içerir (langley).\n\nWind: Rüzgar hızını içerir (mph - miles per hour).\n\nTemp: Günlük maksimum sıcaklık ölçümlerini içerir (Fahrenheit).\n\nMonth: Ölçümün yapıldığı ayı temsil eder (1-12 arasında).\n\nDay: Ölçümün yapıldığı gün bilgisini içerir (1-31 arasında).\n\n\n\n3.1.1 Ad-hoc Yöntemler - Liste Bazlı Silme (Listwise Deletion)\nEksik verilerle başa çıkmak için veri bilimciler tarafından en sık kullanılan yöntemlerden biri, eksik değerlere sahip durumları tamamen çıkarmak ve yalnızca kalan veri setini analiz etmektir. Bu yönteme liste bazlı silme veya tam durum analizi (complete-case analysis) denir. R programında na.omit() fonksiyonu, veri setinde bir veya daha fazla eksik değeri olan tüm durumları kaldırır.\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nVeri setinde bazı NA değerlerini şimdiden gözlemleyebiliyoruz.\nSonraki adımda, NA değerleri içeren durumları veri setinden kaldırıyoruz.\n\nna.omit() ile Eksik Verileri Çıkarma\n\n\n# Eksik verileri çıkarma\nairquality_omit &lt;- na.omit(airquality)\n\n# İlk birkaç satırı görüntüleme\nhead(airquality_omit)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n7    23     299  8.6   65     5   7\n8    19      99 13.8   59     5   8\n\n\n\nİlk çıktıda airquality, eksik değerler (NA) hala veri setinde bulunurken, airquality_omit veri setinde eksik değerler içeren satırlar tamamen çıkarılmıştır. Bu, satır sayısının azalmasına yol açar.\n\n\n\n\n\n\n\nListe bazlı silme\n\n\n\nListe bazlı silme (Listwise deletion) yöntemi, eksik veriler içeren satırları tamamen kaldırdığı için genellikle birkaç nedenden dolayı tercih edilmez:\n\n\nVeri Kaybı: Eksik değerlere sahip satırların tamamen silinmesi, veri setinin boyutunu küçültür ve bu da analiz için kullanılabilir bilgi miktarını azaltır. Bu durum, özellikle eksik verilerin oranı yüksekse, analiz sonuçlarını ciddi şekilde etkileyebilir.\n\nÖrnekleme Yanlılığı: Eksik veriler rastgele (MCAR - Missing Completely at Random) değilse, bu yöntemin kullanımı örneklemde yanlılığa neden olabilir. Sonuç olarak, elde edilen analiz sonuçları tüm veri setini veya popülasyonu doğru bir şekilde temsil etmeyebilir.\n\nİstatistiksel Güç Kaybı: Veri setinin boyutunun küçülmesi, istatistiksel gücü azaltır. Bu da yapılan analizlerin daha az anlamlı sonuçlar üretmesine yol açabilir.\n\nKarmaşık Eksiklik Yapıları: Eksik veriler farklı desenler izleyebilir ve listwise deletion, bu desenleri dikkate almadan tüm eksik satırları kaldırır. Bu, özellikle eksik verilerin analizin anahtar değişkenlerinde olduğu durumlarda önemli bilgilerin kaybolmasına neden olabilir.\n\nBu nedenlerle, eksik verilerle başa çıkmak için çoklu doldurma (multiple imputation) veya eksik değerlerin modelleme yöntemleriyle ele alınması gibi daha gelişmiş yöntemler genellikle listwise deletion’a tercih edilir.\n\n\n\n3.1.2 Eksik Verilerin Grafik Olarak Tespiti\n\nEksik Verileri Görselleştirme (naniar Paketinin Kullanımı)\n\n\n# naniar kütüphanesini yükleme (eğer yüklü değilse)\n# install.packages(\"naniar\")\n\n# naniar kütüphanesini yükleme\nlibrary(naniar)\n\n# Eksik verileri görselleştirme\nvis_miss(airquality)\n\n\n\n\n\n\n\n\nGrafik, vis_miss() fonksiyonu ile oluşturulmuş ve airquality veri setindeki eksik verilerin genel yapısını göstermektedir.\n\n\nSiyah alanlar eksik verileri (Missing) temsil ederken, gri alanlar mevcut verileri (Present) temsil eder.\nOzon değişkeninde %24, Solar.R değişkeninde %5 oranında eksik veri bulunmaktadır. Diğer değişkenler (Wind, Temp, Month, Day) ise eksiksizdir.\n\n\n\n3.1.2.1 Eksik Değerlerin Eşzamanlı Görülmesi\n\n# naniar kütüphanesini yükleme\nlibrary(naniar)\n\n# Eksik verilerin UpSet grafiği ile gösterimi\ngg_miss_upset(airquality)\n\n\n\n\n\n\n\n\ngg_miss_upset(airquality) fonksiyonu, naniar paketine ait bir fonksiyondur ve eksik değerlerin birlikteliğini (co-occurrence) görselleştirmek için UpSetR paketini kullanarak bir grafik oluşturur. Bu grafik, hangi değişkenlerin birlikte eksik olduğunu ve bu kombinasyonların ne sıklıkta görüldüğünü gösterir.\n\n\nÇubuk grafikler (dikey): Her bir çubuk, belirli bir eksik değer kombinasyonunu temsil eder. Çubuğun yüksekliği, bu kombinasyonun veri setinde kaç kez tekrarlandığını gösterir.\n\nNoktalar ve çizgiler (yatay): Her bir değişken için bir nokta bulunur. Eğer bir çubukta o değişkenle ilgili nokta doluysa (yani çizgiyle bağlıysa), o kombinasyonda o değişkende eksik değer olduğu anlamına gelir. Örneğin, sadece “Ozone” değişkenine bağlı bir çubuk, sadece “Ozone” değerinin eksik olduğu satırları temsil eder. Hem “Ozone” hem de “Solar.R” değişkenlerine bağlı bir çubuk ise, her iki değişkende de aynı anda eksik değer olan satırları temsil eder.\n\n\n\n3.1.2.2 Faktör Düzeyine Göre Veri Eksikliğini Görselleştirme\n\n# naniar kütüphanesini yükleme\nlibrary(naniar)\n\n# Eksik verileri faktör düzeyine göre görselleştirme\ngg_miss_fct(x = airquality, fct = Month)\n\n\n\n\n\n\n\n\ngg_miss_fct(x = airquality, fct = Month) fonksiyonu, naniar paketine ait bir fonksiyondur ve airquality veri setindeki eksik verilerin Month değişkenine göre dağılımını görselleştirir. Month burada bir faktör (kategorik değişken) olarak kabul edilir ve her bir ay (5, 6, 7, 8, 9) için ayrı bir çubuk gösterilir.\nGrafikte şunlar görülebilir:\n\n\nX ekseni (Month): Ayları temsil eder (5 = Mayıs, 6 = Haziran, …, 9 = Eylül).\n\nY ekseni (Missing Percentage): Eksik veri yüzdesini temsil eder.\n\nÇubuklar: Her bir ay için bir çubuk bulunur. Çubuğun yüksekliği, o ayda ne kadar eksik veri olduğunu (tüm değişkenler için toplamda) gösterir.\n\nBu grafik, eksik verilerin aylara göre nasıl değiştiğini anlamak için çok faydalıdır. Örneğin, belirli aylarda daha fazla eksik veri olup olmadığını veya eksik verilerin aylara göre bir örüntü izleyip izlemediğini görebilirsiniz. Bu bilgi, veri toplama sürecindeki olası sorunları veya mevsimsel etkileri anlamanıza yardımcı olabilir. Örneğin, eğer belirli bir ayda ölçüm cihazlarında bir arıza olduysa, o ayda daha fazla eksik veri görülebilir.\n\n\nEksik Verileri Faktör Düzeyine Göre Nokta Grafiği ile Görselleştirme\n\n\n# Gerekli paketlerin yüklenmesi\nlibrary(ggplot2)\nlibrary(naniar)\n\n# Eksik veri noktalarını görselleştirme\nggplot(airquality, aes(x = Ozone, y = Solar.R)) +\n  geom_miss_point()  # Eksik verileri noktalar olarak görselleştirir\n\n\n\n\n\n\n\n\n3.1.2.3 dlookr Paketi ile Eksik Veriler\n\n# dlookr kütüphanesini yükleme (gerekli fonksiyon için)\n# install.packages(\"dlookr\")\nlibrary(dlookr)\n\n# Eksik verilerin Pareto grafiği ile gösterimi\nplot_na_pareto(airquality, col = \"blue\")\n\n\n\n\n\n\n\n\nplot_na_pareto(airquality) fonksiyonu, dlookr paketine ait bir fonksiyondur ve airquality veri setindeki eksik değerleri bir Pareto grafiği ile görselleştirir.\n\n\nX ekseni: Değişkenleri temsil eder. Değişkenler, eksik değer sayılarına göre en çoktan en aza doğru sıralanmıştır.\n\nSol Y ekseni: Eksik değer sayısını temsil eder. Her bir değişken için bir çubuk bulunur ve çubuğun yüksekliği o değişkendeki eksik değer sayısını gösterir.\n\nSağ Y ekseni: Kümülatif eksiklik yüzdesini temsil eder. Çizgi grafiği, değişkenler eklendikçe toplam eksiklik oranının nasıl arttığını gösterir.\n\nPareto grafiği, hangi değişkenlerde en çok eksik değer olduğunu ve bu değişkenlerin toplam eksikliğe ne kadar katkıda bulunduğunu hızlıca anlamak için kullanışlıdır. Genellikle, birkaç değişkenin toplam eksikliğin büyük bir kısmını oluşturduğu görülür (“80/20 kuralı” olarak da bilinir). Bu grafik, eksik verilerle başa çıkarken önceliklerin belirlenmesine yardımcı olabilir. Örneğin, en çok eksik değere sahip değişkenlere odaklanmak, genel eksiklik sorununu çözmek için daha etkili bir yaklaşım olabilir. airquality örneğinde Ozone değişkeninin diğerlerine göre çok daha fazla eksik veriye sahip olduğu kolayca görülebilir.\n\nEksik Verilerin Hiyerarşik Kümeleme Grafiği ile Gösterimi dlookr\n\n# dlookr kütüphanesini yükleme\nlibrary(dlookr)\n\n# Eksik verilerin hiyerarşik kümeleme grafiği ile gösterimi\nplot_na_hclust(airquality, main = \"Distribution of missing value\")\n\n\n\n\n\n\n\n\n\n\nplot_na_hclust(airquality, main = \"Distribution of missing value\") fonksiyonu, dlookr paketine ait bir fonksiyondur ve airquality veri setindeki eksik değer örüntülerini hiyerarşik kümeleme (hierarchical clustering) kullanarak görselleştirir.\n\nmain = \"Distribution of missing value\" argümanı, grafiğe bir başlık ekler.\n\nBu grafik, eksik değerlerin veri setinde rastgele mi dağıldığını yoksa belirli örüntüler izleyip izlemediğini anlamak için çok faydalıdır. Örneğin, belirli satır gruplarının benzer eksik değer örüntülerine sahip olduğunu görmek, veri toplama sürecinde veya verilerin doğasında bir sorun olduğunu gösterebilir. Bu bilgi, eksik verilerle nasıl başa çıkılacağına (örneğin, hangi doldurma yönteminin kullanılacağına) karar verirken önemli bir rol oynayabilir.\n\nEksik Verilerin Kesişim Grafiği ile Gösterimi dlookr\n\n# dlookr kütüphanesini yükleme\nlibrary(dlookr)\n\n# Eksik verilerin kesişim grafiği ile gösterimi\nplot_na_intersect(airquality)\n\n\n\n\n\n\n\n\nplot_na_intersect(airquality) fonksiyonu, dlookr paketine ait bir fonksiyondur ve airquality veri setindeki eksik değerlerin kesişimlerini (yani hangi değişkenlerin aynı satırlarda birlikte eksik olduğunu) görselleştirir.\n\n\n3.1.3 Eksik Değerlerin Toplam Sayıları ve Oranları\nn_miss fonksiyonu, verilerdeki tüm NA (yani eksik) değerlerinin toplam sayısını döndürür.\n\n3.1.3.1 NA olan değerlerin sayısı için:\n\n# naniar kütüphanesini yükleme\nlibrary(naniar)\n\n# Eksik değer sayısını hesaplama\nn_miss(airquality)\n\n[1] 44\n\n\n\nn_miss(airquality) fonksiyonu, naniar paketine ait bir fonksiyondur ve airquality veri setindeki toplam eksik değer (NA) sayısını hesaplar. Çıktı olarak [1] 44 değeri döner. Bu, airquality veri setinde toplam 44 adet eksik değer olduğunu gösterir.\n\n\n3.1.3.2 NA olmayan (complete) değerlerin sayısı için:\n\n# naniar kütüphanesini yükleme\nlibrary(naniar)\n\n# Tamamlanmış değer sayısını hesaplama\nn_complete(airquality)\n\n[1] 874\n\n\n\nn_complete(airquality) fonksiyonu, naniar paketine ait bir fonksiyondur ve airquality veri setindeki tamamlanmış (yani eksik olmayan) toplam değer sayısını hesaplar. Çıktı olarak [1] 874 değeri döner. Bu, airquality veri setinde toplam 874 adet tamamlanmış değer olduğunu gösterir.\n\n\n3.1.3.3 NA olan değerlerin oranı için:\n\n# naniar kütüphanesini yükleme\nlibrary(naniar)\n\n# Eksik değer oranını hesaplama\nprop_miss(airquality)\n\n[1] 0.04793028\n\n\n\nprop_miss(airquality) fonksiyonunun çıktısı olan [1] 0.04792626, airquality veri setindeki verilerin yaklaşık %4.79’unun eksik olduğunu gösterir. Bu oran, eksik değer sayısının toplam veri noktası sayısına bölünmesiyle bulunur.\n\n\n3.1.3.4 NA olmayan (complete) değerlerin oranı için:\n\n# naniar kütüphanesini yükleme\nlibrary(naniar)\n\n# Tamamlanmış değer oranını hesaplama\nprop_complete(airquality)\n\n[1] 0.9520697\n\n\n\nprop_complete(airquality) fonksiyonu, naniar paketine ait bir fonksiyondur ve airquality veri setindeki tamamlanmış (yani eksik olmayan) değerlerin oranını hesaplar. Çıktı olarak [1] 0.9520737 değeri döner. Bu, airquality veri setindeki değerlerin yaklaşık %95.2’sinin tamamlanmış olduğunu gösterir.\n\n\n3.1.3.5 Eksik veriler için pareto tablosu dlookr\n\n\n# dlookr kütüphanesini yükleme (gerekli fonksiyon için)\n# install.packages(\"dlookr\")\nlibrary(dlookr)\n\n# Eksik verilerin Pareto grafiği ile gösterimi\nplot_na_pareto(airquality, \n               only_na = TRUE, \n               # sadece eksik değer içeren değişkenlerin gösterilmesini sağlar.\n               plot = FALSE) \n\n# A tibble: 2 × 5\n  variable frequencies  ratio grade cumulative\n  &lt;fct&gt;          &lt;int&gt;  &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;\n1 Ozone             37 0.242  Bad         84.1\n2 Solar.R            7 0.0458 Good       100  \n\n               # grafik yerine sadece tablo çıktısının gösterilmesini sağlar.\n\n\n3.1.4 Web Raporu Oluşturma\n\n# dlookr kütüphanesini yükleme \nlibrary(dlookr)  \n\n# Web raporu oluşturma \n# diagnose_web_report(airquality, subtitle = \"airquality\")\n\n\ndiagnose_web_report(airquality, subtitle = \"airquality\") fonksiyonu, dlookr paketine ait bir fonksiyondur ve airquality veri seti için kapsamlı bir veri teşhis raporu oluşturur. Bu rapor bir HTML dosyası olarak kaydedilir ve bir web tarayıcısında görüntülenebilir.\n\n\n3.1.5 DLOOKR Paketi ile Eksik Değerleri Doldurma\ndlookr paketi ve imputate_na()\ndlookr paketi, veri teşhisi (data diagnosis) ve veri keşfi (data exploration) için tasarlanmış bir R paketidir. Bu paket, veri kalitesini değerlendirmek, veri setini özetlemek, değişkenler arasındaki ilişkileri incelemek ve eksik verilerle başa çıkmak için çeşitli kullanışlı fonksiyonlar içerir. imputate_na() fonksiyonu da bu paketin eksik veri yönetimi araçlarından biridir.\nimputate_na() fonksiyonunun temel amacı, bir veri setindeki eksik değerleri (NA) çeşitli yöntemlerle doldurmaktır. Bu fonksiyon, hem sayısal (numeric) hem de kategorik (categorical) değişkenlerdeki eksik değerleri ele alabilir ve farklı doldurma yöntemleri sunar.\n\n\nSayısal Değişkenler için Doldurma Yöntemleri:\n\n\n\"mean\": Eksik değerleri değişkenin ortalamasıyla doldurur.\n\n\"median\": Eksik değerleri değişkenin medyanıyla (ortanca) doldurur.\n\n\"mode\": Eksik değerleri değişkenin moduyla (en sık tekrar eden değer) doldurur.\n\n\"knn\": K-en yakın komşu algoritmasını kullanarak eksik değerleri doldurur. Bu yöntem, eksik değerin bulunduğu satıra en yakın olan K tane gözlemi bulur ve bu gözlemlerin değerlerini kullanarak eksik değeri tahmin eder. Bu yöntem için bir referans değişken belirtmek gereklidir.\n\n\"rpart\": Özyinelemeli Bölümleme ve Regresyon Ağaçları (Recursive Partitioning and Regression Trees) yöntemini kullanarak eksik değerleri doldurur. Bu yöntem, bir karar ağacı modeli oluşturarak eksik değerleri tahmin eder. Bu yöntem için bir referans değişken belirtmek gereklidir.\n\n\"mice\": Zincirleme Denklemlerle Çoklu Atama (Multivariate Imputation by Chained Equations) yöntemini kullanarak eksik değerleri doldurur. Bu yöntem, her eksik değişken için bir model oluşturur ve diğer değişkenleri kullanarak eksik değerleri tahmin eder. Bu yöntem için bir referans değişken belirtmek ve bir rastgele sayı başlangıç değeri (random seed) ayarlamak gereklidir.\n\n\n\nKategorik Değişkenler için Doldurma Yöntemleri:\n\n\n\"mode\": Eksik değerleri değişkenin moduyla (en sık tekrar eden kategori) doldurur.\n\n\"rpart\": Özyinelemeli Bölümleme ve Regresyon Ağaçları yöntemini kullanarak eksik değerleri doldurur. Bu yöntem için bir referans değişken belirtmek gereklidir.\n\n\"mice\": Zincirleme Denklemlerle Çoklu Atama yöntemini kullanarak eksik değerleri doldurur. Bu yöntem için bir referans değişken belirtmek ve bir rastgele sayı başlangıç değeri (random seed) ayarlamak gereklidir.\n\n\n\nimputate_na() fonksiyonu, veri ön işleme adımlarında eksik verileri ele almak için kullanışlı bir araçtır. Doldurma yöntemini seçerken, verinizin yapısını ve analizin amacını göz önünde bulundurmanız önemlidir. Örneğin, ortalama ile doldurma, aykırı değerlerden etkilenebilirken, medyan ile doldurma bu etkiyi azaltır. knn, rpart ve mice gibi daha gelişmiş yöntemler ise, değişkenler arasındaki ilişkileri dikkate alarak daha doğru tahminler yapabilir.\n\n3.1.5.1 Eksik değer içeren sütunu görüntüleme\n\ndata(\"airquality\")\n\n# airquality veri setinin Ozone sütununu görüntüleme\nairquality$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6\n [19]  30  11   1  11   4  32  NA  NA  NA  23  45 115  37  NA  NA  NA  NA  NA\n [37]  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA\n [55]  NA  NA  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA\n [73]  10  27  NA   7  48  35  61  79  63  16  NA  NA  80 108  20  52  82  50\n [91]  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22\n[109]  59  23  31  44  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73\n[127]  91  47  32  20  23  21  24  44  21  28   9  13  46  18  13  24  16  13\n[145]  23  36   7  14  30  NA  14  18  20\n\n\n\nairquality$Ozone kodu, airquality adlı veri çerçevesinin Ozone adlı sütununu seçer ve bu sütundaki tüm değerleri bir vektör olarak döndürür.\nÇıktıda görüldüğü gibi, Ozone sütunu sayısal değerler ve NA (Not Available - Mevcut Değil) değerleri içermektedir. NA değerleri, o gün için ozon ölçümünün yapılamadığını veya kaydedilmediğini gösterir.\n\n\n\n\n\n\n\nVektör Çıktılarında Köşeli Parantez\n\n\n\nÇıktının başında ve sonunda [1], [28], [55] gibi ifadeler bulunur. Bunlar, çıktının hangi indeksinden itibaren yeni bir satıra geçildiğini gösterir. Örneğin, [28] ifadesi, o satırda 28. elemandan itibaren değerlerin listelendiğini belirtir. Bu, çıktının daha okunabilir olmasını sağlar, özellikle de çok uzun vektörler görüntülendiğinde.\n\n\n\n3.1.5.2 Eksik değerleri ortalama (mean) ile doldurma\n\n# dlookr kütüphanesini yükleme \nlibrary(dlookr)\n\n# Ozone değişkenini Temp i referans alarak ortalama ile doldurma\naq_imp_ozone_mean &lt;- imputate_na(airquality, Ozone, Temp, method = \"mean\")\n\n# SADECE Ozone sütununu görüntüleme\naq_imp_ozone_mean\n\n  [1]  41.00000  36.00000  12.00000  18.00000  42.12931  28.00000  23.00000\n  [8]  19.00000   8.00000  42.12931   7.00000  16.00000  11.00000  14.00000\n [15]  18.00000  14.00000  34.00000   6.00000  30.00000  11.00000   1.00000\n [22]  11.00000   4.00000  32.00000  42.12931  42.12931  42.12931  23.00000\n [29]  45.00000 115.00000  37.00000  42.12931  42.12931  42.12931  42.12931\n [36]  42.12931  42.12931  29.00000  42.12931  71.00000  39.00000  42.12931\n [43]  42.12931  23.00000  42.12931  42.12931  21.00000  37.00000  20.00000\n [50]  12.00000  13.00000  42.12931  42.12931  42.12931  42.12931  42.12931\n [57]  42.12931  42.12931  42.12931  42.12931  42.12931 135.00000  49.00000\n [64]  32.00000  42.12931  64.00000  40.00000  77.00000  97.00000  97.00000\n [71]  85.00000  42.12931  10.00000  27.00000  42.12931   7.00000  48.00000\n [78]  35.00000  61.00000  79.00000  63.00000  16.00000  42.12931  42.12931\n [85]  80.00000 108.00000  20.00000  52.00000  82.00000  50.00000  64.00000\n [92]  59.00000  39.00000   9.00000  16.00000  78.00000  35.00000  66.00000\n [99] 122.00000  89.00000 110.00000  42.12931  42.12931  44.00000  28.00000\n[106]  65.00000  42.12931  22.00000  59.00000  23.00000  31.00000  44.00000\n[113]  21.00000   9.00000  42.12931  45.00000 168.00000  73.00000  42.12931\n[120]  76.00000 118.00000  84.00000  85.00000  96.00000  78.00000  73.00000\n[127]  91.00000  47.00000  32.00000  20.00000  23.00000  21.00000  24.00000\n[134]  44.00000  21.00000  28.00000   9.00000  13.00000  46.00000  18.00000\n[141]  13.00000  24.00000  16.00000  13.00000  23.00000  36.00000   7.00000\n[148]  14.00000  30.00000  42.12931  14.00000  18.00000  20.00000\nattr(,\"var_type\")\n[1] \"numerical\"\nattr(,\"method\")\n[1] \"mean\"\nattr(,\"na_pos\")\n [1]   5  10  25  26  27  32  33  34  35  36  37  39  42  43  45  46  52  53  54\n[20]  55  56  57  58  59  60  61  65  72  75  83  84 102 103 107 115 119 150\nattr(,\"type\")\n[1] \"missing values\"\nattr(,\"message\")\n[1] \"complete imputation\"\nattr(,\"success\")\n[1] TRUE\nattr(,\"class\")\n[1] \"imputation\" \"numeric\"   \n\n\n\n\n\ndata: İşlem yapılacak veri seti. Burada airquality veri seti kullanılmıştır.\n\ntarget: Eksik değerlerin doldurulacağı sütun. Burada Ozone sütunu belirtilmiştir.\n\nref: Referans alınacak sütun. Burada Temp sütunu kullanılmıştır.\n\nmethod: Doldurma yöntemi. \"mean\" ile ortalama kullanılarak doldurma yapılır.\n\nÇalışma Prensibi: Eksik değerler Temp sütununun ortalamasına göre doldurulmuş ve yeni bir veri seti (aq_imp_ozone) oluşturulmuştur.\n\n\n3.1.5.3 Ortalama (mean) ile doldurma öncesi ve sonrası yoğunluk dağılımları\n\nplot(aq_imp_ozone_mean)\n\n\n\n\n\n\n\n\nEksik değerlerin ortalama ile doldurulması, veri setinin genel dağılımında hafif değişikliklere yol açmıştır. Eksik değerlerin doldurulması, yoğunluk eğrisini daha düzgün hale getirmiştir, ancak bu işlem, verilerin doğal dağılımını biraz değiştirebilir. Özellikle veri çok eksikse, ortalama ile doldurma yöntemi dağılımın şeklini etkileyebilir. Eğer veri setinin doğal varyasyonunu korumak çok önemliyse, alternatif doldurma yöntemleri (örneğin, knn veya regresyon tabanlı yöntemler) düşünülebilir.\n\n\n3.1.5.4 Medyan (median) ile doldurma öncesi ve sonrası yoğunluk dağılımları\n\nlibrary(dlookr)\n\n# Ozone ve Temp değişkenlerini medyan ile doldurma\naq_imp_ozone_median &lt;- imputate_na(airquality, Ozone, Temp, method = \"median\")\n\n# plot() fonksiyonu ile çizim (indekse karşı değer grafiği)\nplot(aq_imp_ozone_median)\n\n\n\n\n\n\n\n\nGrafikte, x ekseni vektördeki elemanların sırasını (indeks), y ekseni ise medyan ile doldurulmuş Ozone değerlerini gösterir. Ortalama ile doldurmaya benzer şekilde, grafikte noktaların rastgele yukarı aşağı hareket ettiğini görürsünüz. Ancak, medyan ile doldurmada, ortalama ile doldurmaya kıyasla grafikte daha az yatay çizgi veya düz bölge görürsünüz. Bunun nedeni, medyanın ortalamadan farklı değerlere sahip olabilmesi ve aynı değerin daha az tekrar etmesidir.\n\n\n\n\n\n\n\nOrtalama vs. Medyan ile Eksik Değer Doldurma\n\n\n\nOrtalama ile doldurma, dağılımın ortasında bir yığılmaya neden olurken, medyan ile doldurma bu yığılmayı daha az belirgin hale getirir. Çünkü medyan, aykırı değerlerden ortalamaya göre daha az etkilenir. Bu nedenle, verilerinizde aykırı değerler varsa, medyan ile doldurma ortalama ile doldurmaya göre daha iyi bir seçenek olabilir.\n\n\n\n3.1.5.5 knn ile doldurma öncesi ve sonrası yoğunluk dağılımları\n\nlibrary(dlookr)\n\n# Ozone değişkenini knn ile doldurma (Temp'i referans değişken olarak kullanır)\naq_imp_ozone_knn &lt;- imputate_na(airquality, Ozone, Temp, method = \"knn\")\n\n# plot() fonksiyonu ile çizim (indekse karşı değer grafiği)\nplot(aq_imp_ozone_knn)\n\n\n\n\n\n\n\n\nYukarıdaki kod, Ozone değişkenindeki eksik değerleri knn (k-Nearest Neighbors - k-En Yakın Komşu) yöntemiyle dolduruyor ve ardından bu doldurulmuş değerleri plot() fonksiyonu ile çiziliyor.\n\n\nReferans Değişkenin Önemi: knn ile doldurma yaparken, seçilen referans değişkenin (burada Temp) eksik değerlere sahip olmaması veya çok az eksik değere sahip olması önemlidir. Aksi takdirde, modelin doğruluğu düşebilir.\n\nBenzer Gözlemler: knn, eksik değere sahip olan gözleme en benzer k tane gözlemi bulur ve bu gözlemlerin değerlerini kullanarak eksik değeri tahmin eder. Bu nedenle, verideki yerel örüntüleri yakalamada etkilidir.\n\nk Değeri: k parametresi (komşu sayısı) önemlidir. Çok küçük bir k değeri, aşırı uyuma (overfitting) neden olabilirken, çok büyük bir k değeri, yerel örüntüleri kaçırmaya neden olabilir. imputate_na() fonksiyonunda k değeri varsayılan olarak 5’tir, ancak gerekirse değiştirilebilir.\n\nDağılımın Değişimi: knn ile doldurma, ortalama veya medyan ile doldurmaya göre dağılımı daha az etkiler. Çünkü bu yöntem, eksik değerleri tek bir sabit değerle doldurmak yerine, benzer gözlemlerin değerlerine göre farklı değerlerle doldurur.\n\n\n\n3.1.5.6 rpart ile doldurma öncesi ve sonrası yoğunluk dağılımları\n\nlibrary(dlookr)\n\n# Ozone değişkenini rpart ile doldurma (Temp'i referans değişken olarak kullanır)\naq_imp_ozone_rpart &lt;- imputate_na(airquality, Ozone, Temp, method = \"rpart\")\n\n# plot() fonksiyonu ile çizim (indekse karşı değer grafiği)\nplot(aq_imp_ozone_rpart)\n\n\n\n\n\n\n\n\nYukarıdaki kod ile Ozone değişkenindeki eksik değerleri rpart (Recursive Partitioning and Regression Trees - Özyinelemeli Bölümleme ve Regresyon Ağaçları) yöntemiyle dolduruyor ve ardından bu doldurulmuş değerleri plot() fonksiyonu ile çiziyor.\n\n\nreferans Değişkenin Önemi: rpart ile doldurma yaparken, seçilen referans değişkenin (burada Temp) eksik değerlere sahip olmaması veya çok az eksik değere sahip olması önemlidir. Aksi takdirde, modelin doğruluğu düşebilir.\n\nDoğrusal Olmayan İlişkiler: rpart, değişkenler arasındaki doğrusal olmayan ilişkileri de yakalayabildiği için, ortalama veya medyan ile doldurmaya göre daha doğru sonuçlar verebilir. Ancak, aşırı uyum (overfitting) riskini de beraberinde getirebilir.\n\nDağılımın Değişimi: rpart ile doldurma, ortalama veya medyan ile doldurmaya göre dağılımı daha az etkiler. Çünkü bu yöntem, eksik değerleri tek bir sabit değerle doldurmak yerine, referans değişkene göre farklı değerlerle doldurur.\n\n\n\n3.1.5.7 mice ile doldurma öncesi ve sonrası yoğunluk dağılımları\n\nlibrary(dlookr)\nlibrary(mice)\n\n# Ozone değişkenini mice ile doldurma (Temp'i ve diğer değişkenleri kullanır)\naq_imp_ozone_mice &lt;- imputate_na(airquality, Ozone, Temp, \n                                 method = \"mice\", \n                                 seed = 111, \n                                 print =FALSE)\n\n# plot() fonksiyonu ile çizim (indekse karşı değer grafiği)\nplot(aq_imp_ozone_mice)\n\n\n\n\n\n\n\n\nYukarıdaki kod ile Ozone değişkenindeki eksik değerleri mice (Multivariate Imputation by Chained Equations - Zincirleme Denklemlerle Çoklu Atama) yöntemiyle dolduruyor ve ardından bu doldurulmuş değerleri plot() fonksiyonu ile çiziyor.\n\n\nÇoklu Atama: mice, eksik değerler için birden fazla olası değer ürettiği için, eksik verilerin belirsizliğini daha iyi yansıtır. Bu, daha doğru ve güvenilir sonuçlar elde etmenizi sağlar.\n\nDeğişkenler Arası İlişkiler: mice, değişkenler arasındaki ilişkileri dikkate aldığı için, diğer yöntemlere göre daha iyi tahminler yapabilir.\n\nDağılımın Korunması: mice, verinin orijinal dağılımını daha iyi korur. Ortalama veya medyan ile doldurma, dağılımda bozulmalara neden olabilirken, mice bu etkiyi en aza indirir.\n\n\n\n3.1.5.8 Doldurulmuş veriyi orijinal veriye entegre etme - Aşama 1\n\n# Gerekli kütüphanelerin yüklenmesi\nlibrary(dlookr)\nlibrary(tidyverse)\nlibrary(mice)\n\n# Orijinal veri setini kopyalama\nairquality_imp &lt;- airquality\n\n# Doldurulmuş Ozone verisini orijinal veri setine atama\nairquality_imp$Ozone &lt;- aq_imp_ozone_mice\n\n# Doldurulmuş veri setini görüntüleme\nhead(airquality_imp, 10)\n\n   Ozone Solar.R Wind Temp Month Day\n1   41.0     190  7.4   67     5   1\n2   36.0     118  8.0   72     5   2\n3   12.0     149 12.6   74     5   3\n4   18.0     313 11.5   62     5   4\n5   21.0      NA 14.3   56     5   5\n6   28.0      NA 14.9   66     5   6\n7   23.0     299  8.6   65     5   7\n8   19.0      99 13.8   59     5   8\n9    8.0      19 20.1   61     5   9\n10  40.2     194  8.6   69     5  10\n\n\n\n3.1.5.9 Doldurulmuş veriyi orijinal veriye entegre etme - Aşama 2\n\n# Gerekli kütüphanelerin yüklenmesi\nlibrary(dlookr)\nlibrary(mice)\nlibrary(tidyverse)\n\n# Ozone değişkenini ortalama ile doldurma\naq_imp_solar_mice &lt;- imputate_na(airquality_imp, Solar.R, Temp, \n                                 method = \"mice\", \n                                 seed = 111,\n                                 print = FALSE)\n# \"print =\" argümanı eğer TRUE olarak ayarlanırsa, mice işlemin geçmişini konsolda\n# yazdıracaktır. Sessiz bir hesaplama için print=FALSE kullanın.\n\n# Doldurulmuş Ozone verisini orijinal veri setine atama\nairquality_imp$Solar.R &lt;- aq_imp_solar_mice\n\n# Doldurulmuş veri setini görüntüleme\nhead(airquality_imp, 10)\n\n   Ozone Solar.R Wind Temp Month Day\n1   41.0   190.0  7.4   67     5   1\n2   36.0   118.0  8.0   72     5   2\n3   12.0   149.0 12.6   74     5   3\n4   18.0   313.0 11.5   62     5   4\n5   21.0   266.2 14.3   56     5   5\n6   28.0   218.0 14.9   66     5   6\n7   23.0   299.0  8.6   65     5   7\n8   19.0    99.0 13.8   59     5   8\n9    8.0    19.0 20.1   61     5   9\n10  40.2   194.0  8.6   69     5  10\n\n\nOrijinal veri seti ile karşılaştırma\n\nhead(airquality, 10)\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10\n\n\n\n3.1.6 MICE Paketi ile Eksik Değerleri Doldurma\nMICE paketi, eksik veri problemini çözmek için kullanılan bir araçtır ve eksik verileri çoklu imputasyon yöntemini kullanarak işleme alır. Süreç, eksik veri içeren bir veri setiyle başlar. Bu veri genellikle bir data frame formatındadır ve eksik verilerin, diğer değişkenlerle olan ilişkilerine dayanarak doldurulması hedeflenir.\nMICE paketi, eksik veri problemini çözmek için şu adımları takip eder:\n\n\nEksik verileri birden fazla doldurur (mice()).\nİlk adımda, mice() fonksiyonu kullanılarak eksik veriler birden fazla iterasyonla doldurulur. Her iterasyonda eksik olan değişkenler, diğer değişkenlerle olan ilişkileri kullanılarak tahmin edilir. Bu işlem sonucunda, doldurulmuş veri setlerini içeren bir “mids” nesnesi oluşturulur.\n\n\nDoldurulan veri setleri üzerinde analizler yapar (with()).\nDaha sonra, with() fonksiyonu aracılığıyla doldurulan veri setleri üzerinde istatistiksel analizler gerçekleştirilir. Örneğin, her doldurulmuş veri seti için regresyon analizi gibi istatistiksel işlemler yapılabilir ve bu analizlerin sonuçları “mira” nesnesi olarak saklanır.\n\n\nAnaliz sonuçlarını havuzlar ve birleştirir (pool()).\nSon aşamada, pool() fonksiyonu kullanılarak her bir doldurulmuş veri seti üzerinde yapılan analizlerin sonuçları birleştirilir. Bu birleştirme işlemi, eksik veri kaynaklı belirsizliği hesaba katarak daha güvenilir ve tutarlı sonuçlar elde etmeyi sağlar. Bu süreç sonucunda, analizlerin nihai sonuçları “mipo” nesnesi olarak elde edilir.\n\n\nMICE paketi, eksik veri problemini istatistiksel olarak en iyi şekilde ele alarak analizlerin güvenilirliğini artırmayı hedefler ve eksik veriden kaynaklanan yanlılığı azaltır.\n\nFigür: https://www.jstatsoft.org/article/view/v045i03\nVeri Seti nhanes\n\n# Gerekli kütüphanelerin yüklenmesi\n# install.packages(\"mice\")\n# install.packages(\"tidyverse\")\n# install.packages(\"NHANES\")\n\nlibrary(mice)\nlibrary(tidyverse)\nlibrary(NHANES)\n\nnhanes3 &lt;- NHANES %&gt;% \n   select(Weight, Height, TotChol, PhysActive)\n\n# NHANES veri setinin görüntülenmesi (örnek olarak ilk 10 satır)\nhead(nhanes3, 10)\n\n# A tibble: 10 × 4\n   Weight Height TotChol PhysActive\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;     \n 1   87.4   165.    3.49 No        \n 2   87.4   165.    3.49 No        \n 3   87.4   165.    3.49 No        \n 4   17     105.   NA    &lt;NA&gt;      \n 5   86.7   168.    6.7  No        \n 6   29.8   133.    4.86 &lt;NA&gt;      \n 7   35.2   131.    4.09 &lt;NA&gt;      \n 8   75.7   167.    5.82 Yes       \n 9   75.7   167.    5.82 Yes       \n10   75.7   167.    5.82 Yes       \n\n\n\nnhanes Veri Seti\nNHANES (Ulusal Sağlık ve Beslenme İnceleme Anketi), ABD’de yetişkinlerin ve çocukların sağlık ve beslenme durumunu ölçen bir CDC araştırmasıdır. Anketler ve fiziksel muayeneler içerir. 76 farklı değişkeni bulunmaktadır. Araştırmamızda özellikle aşağıda yer alan\nÇalışmamızda aşağıda yer alan değişkenlere odaklanılacaktır:\n\n\nWeight (Kilo): Obezite ve aşırı kiloyu değerlendirmek için ölçülür.\n\nHeight (Boy): VKİ (Vücut Kitle İndeksi) hesaplamak için kullanılır.\n\nTotChol (Toplam Kolesterol): Kalp hastalığı riskini gösterir.\n\nPhysActive (Fiziksel Aktivite): Genel sağlık için önemlidir.\n\nBu veriler, halk sağlığı sorunlarını anlamak ve sağlık politikalarını değerlendirmek için kullanılır.\n\nnhanes Veri Setinde Eksik Değerler Özet Tablosu\n\nlibrary(naniar)\n\n# miss_var_summary() fonksiyonunu uygulama\nmiss_var_summary(nhanes3)\n\n# A tibble: 4 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 PhysActive   1674    16.7 \n2 TotChol      1526    15.3 \n3 Height        353     3.53\n4 Weight         78     0.78\n\n\n\n\n\nPhysActive 167 16.7: PhysActive değişkeninde 167 eksik veri vardır ve bu, toplam verinin %16.7’sine karşılık gelir.\n\nTotChol 152 15.3: TotChol değişkeninde 152 eksik veri vardır ve bu, toplam verinin %15.3’üne karşılık gelir.\n\nHeight 35 3.53: Height değişkeninde 35 eksik veri vardır ve bu, toplam verinin %3.53’üne karşılık gelir.\n\nWeight 7 0.78: Weight değişkeninde 7 eksik veri vardır ve bu, toplam verinin %0.78’ine karşılık gelir.\n\n\n\n3.1.6.1 MICE (Çoklu İmputasyon) ile Eksik Veri Doldurma\n\nlibrary(mice)\n\n# nhanes veri setinde eksik değerleri doldurma (20 imputasyon seti oluşturma)\nnhanes_multiimp &lt;- mice(nhanes3, m = 20, print = FALSE)\n\n\nBu kod, mice paketi kullanılarak nhanes veri setindeki eksik değerlerin doldurulması için çoklu imputasyon işlemi gerçekleştirir. Burada, m = 20 parametresiyle eksik değerlerin 20 farklı tahmini yapılır ve her biri bir imputasyon veri seti olarak oluşturulur.\n\n\nnhanes: İçerisinde eksik değerler bulunan örnek bir veri seti.\n\nm = 20: Çoklu imputasyon işlemiyle 20 farklı doldurulmuş veri seti oluşturulacağını belirtir.\n\nYukarıdaki kod, mice paketini kullanarak nhanes veri setindeki eksik değerleri çoklu atama yöntemiyle doldurur. Bu yöntem, eksik verilerin belirsizliğini hesaba katarak daha doğru ve güvenilir analizler yapmanıza olanak tanır. Kodun doğru çalışması için data(nhanes) satırının eklenmesi önemlidir. Ayrıca, seed eklenmesi, sonuçların tekrarlanabilirliğini sağlar. Kod, nhanes_multiimp adında bir mids nesnesi oluşturur.\n\n\n\n\n\n\n\nÇoklu Atama ve mice’ın Avantajları\n\n\n\n\n\nÇoklu Atama: mice, eksik değerler için tek bir değer yerine birden fazla olası değer ürettiği için, eksik verilerin belirsizliğini daha iyi yansıtır. Bu, standart tek atama yöntemlerine (ortalama, medyan vb.) göre daha doğru ve güvenilir sonuçlar elde etmenizi sağlar. Tek bir değer atamak yerine, olası değerlerin bir dağılımını kullanarak, eksik veriden kaynaklanan belirsizliği modelinize dahil edersiniz.\n\nDeğişkenler Arası İlişkiler: mice, atama işlemi sırasında değişkenler arasındaki ilişkileri dikkate alır. Bu, eksik verilerin daha gerçekçi ve tutarlı bir şekilde tahmin edilmesini sağlar. Örneğin, yaş ve VKİ arasındaki ilişkiyi göz önünde bulundurarak, eksik VKİ değerlerini daha doğru bir şekilde tahmin edebilir.\n\nDağılımın Korunması: mice, verinin orijinal dağılımını daha iyi korur. Ortalama veya medyan ile doldurma gibi basit yöntemler, veri dağılımında bozulmalara neden olabilirken, mice bu etkiyi en aza indirir. Bu, analizlerinizin daha güvenilir ve anlamlı olmasını sağlar.\n\n\n\n\n3.1.6.2 MICE ile Veri Setleri Üzerinde Lineer Regresyon Modeli Kurma\n\nlibrary(mice)\n\n# Her bir atanmış veri setine lineer regresyon modeli uygula\nlm_multiimp &lt;- with(nhanes_multiimp, lm(Weight ~ Height + TotChol + PhysActive))\n\n\nBu kod, daha önce oluşturulan nhanes_multiimp adlı mids (multiple imputation data set) nesnesini kullanarak, her bir tamamlanmış veri setine bir lineer regresyon modeli uygular.\n\nwith(nhanes_multiimp, ...): Bu fonksiyon, nhanes_multiimp nesnesindeki her bir tamamlanmış veri seti üzerinde belirtilen ifadeyi uygular. Yani, 20 farklı tamamlanmış veri setin varsa, bu ifade 20 kez çalıştırılır ve her biri için ayrı bir lineer regresyon modeli oluşturulur.\nlm(Weight ~ Height + TotChol + PhysActive): Bu, lineer regresyon modelini tanımlar. Weight (Kilo) bağımlı değişken, Height (Boy), TotChol (Toplam Kolesterol) ve PhysActive (Fiziksel Aktivite) ise bağımsız değişkenlerdir. Yani, kilonun boy, toplam kolesterol ve fiziksel aktivite ile nasıl ilişkili olduğunu inceliyoruz.\n\n\n\n\n\n\n\n\nFaktör Dönüşümü\n\n\n\nPhysActive’in Faktöre Dönüştürülmesi: Eğer PhysActive değişkeni sayısal olarak kodlanmış bir kategorik değişken ise (örneğin, 1=Aktif, 2=Pasif ya da yes, no gibi), lineer regresyon modelinde doğru şekilde yorumlanabilmesi için bu değişkeni factor() fonksiyonu ile faktöre dönüştürmek çok önemlidir. Kodu bu duruma göre güncelledim. Eğer PhysActive zaten bir faktör ise bu satıra gerek yoktur.\n\n\n\n3.1.6.3 MICE ile Regresyon Sonuçlarını Havuzlama\n\nlibrary(mice)\n\n# Çoklu imputasyon veri setleri üzerindeki regresyon sonuçlarını havuzlama\nlm_pooled &lt;- pool(lm_multiimp)\n\n# Havuzlanmış sonuçları özetleme\nsummary(lm_pooled)\n\n           term    estimate  std.error  statistic         df      p.value\n1   (Intercept) -93.2761794 1.88249998 -49.549100   85.53417 9.001161e-65\n2        Height   0.9997873 0.01088186  91.876490   92.25780 1.680034e-92\n3       TotChol   1.5587611 0.17765159   8.774259 2354.76828 3.236133e-18\n4 PhysActiveYes  -5.9132808 0.38186334 -15.485332 1660.16326 1.265237e-50\n\n\n\nBu kod, lm_multiimp nesnesindeki çoklu imputasyon veri setleri üzerinde oluşturulan lineer regresyon modellerinin sonuçlarını birleştirir (pool). Havuzlama işlemi, eksik veriler nedeniyle ortaya çıkan belirsizliği hesaba katar ve tüm imputasyon veri setlerinden elde edilen sonuçları birleştirerek daha doğru ve güvenilir tahminler sunar.\nBu model, bağımlı değişken olan Weight (Ağırlık) üzerinde Height (Boy Uzunluğu), TotChol (Toplam Kolesterol) ve PhysActive (Fiziksel Aktivite Durumu) değişkenlerinin etkilerini anlamlı bir şekilde açıklamaktadır. Tüm değişkenlerin p-değerleri oldukça küçüktür ve bu değişkenlerin modelde anlamlı bir etkisi olduğunu göstermektedir. Modeldeki katsayılar, bağımlı değişken üzerinde her bir bağımsız değişkenin etkisini istatistiksel olarak güçlü bir şekilde temsil etmektedir.\n\n\n3.1.6.4 MICE ile Tamamlanmış Veri Seti\n\nlibrary(mice)\n\n# İlk doldurulmuş veri setini elde etme\nnhanes3_completed &lt;- complete(nhanes_multiimp)\n\n# Doldurulmuş veri setini görüntüleme\nhead(nhanes3_completed, 10)\n\n   Weight Height TotChol PhysActive\n1    87.4  164.7    3.49         No\n2    87.4  164.7    3.49         No\n3    87.4  164.7    3.49         No\n4    17.0  105.4    3.80         No\n5    86.7  168.4    6.70         No\n6    29.8  133.1    4.86        Yes\n7    35.2  130.6    4.09        Yes\n8    75.7  166.7    5.82        Yes\n9    75.7  166.7    5.82        Yes\n10   75.7  166.7    5.82        Yes\n\n\n\nhead(nhanes3, 10)\n\n# A tibble: 10 × 4\n   Weight Height TotChol PhysActive\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;     \n 1   87.4   165.    3.49 No        \n 2   87.4   165.    3.49 No        \n 3   87.4   165.    3.49 No        \n 4   17     105.   NA    &lt;NA&gt;      \n 5   86.7   168.    6.7  No        \n 6   29.8   133.    4.86 &lt;NA&gt;      \n 7   35.2   131.    4.09 &lt;NA&gt;      \n 8   75.7   167.    5.82 Yes       \n 9   75.7   167.    5.82 Yes       \n10   75.7   167.    5.82 Yes       \n\n\n\n3.1.6.5 Ham Veri ile Son Veriyi Karşılaştırma\n\nsummary(nhanes3_completed)\n\n     Weight           Height         TotChol       PhysActive\n Min.   :  2.80   Min.   : 83.6   Min.   : 1.530   No :4710  \n 1st Qu.: 56.10   1st Qu.:155.7   1st Qu.: 4.030   Yes:5290  \n Median : 72.70   Median :165.5   Median : 4.680             \n Mean   : 70.99   Mean   :159.9   Mean   : 4.813             \n 3rd Qu.: 88.90   3rd Qu.:174.3   3rd Qu.: 5.480             \n Max.   :230.70   Max.   :200.4   Max.   :13.650             \n\n\n\nsummary(nhanes3)\n\n     Weight           Height         TotChol       PhysActive \n Min.   :  2.80   Min.   : 83.6   Min.   : 1.530   No  :3677  \n 1st Qu.: 56.10   1st Qu.:156.8   1st Qu.: 4.110   Yes :4649  \n Median : 72.70   Median :166.0   Median : 4.780   NA's:1674  \n Mean   : 70.98   Mean   :161.9   Mean   : 4.879              \n 3rd Qu.: 88.90   3rd Qu.:174.5   3rd Qu.: 5.530              \n Max.   :230.70   Max.   :200.4   Max.   :13.650              \n NA's   :78       NA's   :353     NA's   :1526",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Veri Temizleme</span>"
    ]
  },
  {
    "objectID": "veri_temizleme.html#aykırı-değerler-ile-çalışma",
    "href": "veri_temizleme.html#aykırı-değerler-ile-çalışma",
    "title": "\n3  Veri Temizleme\n",
    "section": "\n3.2 Aykırı Değerler ile Çalışma",
    "text": "3.2 Aykırı Değerler ile Çalışma\n\n3.2.1 Aykırı Değerleri Tanımlama ve İlk İnceleme\nAykırı değerler, veri analizinde istatistiksel çıkarımları bozabilecek uç değerlerdir. Bu değerleri tanımlamak ve analiz etmek için keşifçi veri analizi (EDA) kullanılmalıdır. Aşağıda, bir veri seti üzerinden aykırı değerleri tespit etmek için uygulanabilecek yöntemler sunulmuştur.\nÖrnek Veri Seti\nÖrnek olarak bir müşteri gelir veri seti oluşturalım:\n\nset.seed(123)\nveri &lt;- data.frame(\n  musteri_id = 1:150,  # 150 müşteri için ID oluşturulur\n  gelir = c(rnorm(140, mean = 5000, sd = 1000),  \n            # 140 müşteri için ortalama 5000 TL gelir\n            rnorm(10, mean = 15000, sd = 2000)) \n            # 10 müşteri için ortalama 15000 TL gelir (Aykırı değerler)\n)\n\nBu veri setinde 140 müşterinin geliri ortalama 5000 TL civarındayken, 10 müşterinin geliri 15000 TL ve üzerinde olarak ayarlanmıştır. Böylece, yüksek gelire sahip müşteriler aykırı değerler olarak tespit edilmelidir.\n\n3.2.1.1 Boxplot ile Aykırı Değerleri Görselleştirme\nBoxplot, aykırı değerleri hızlı bir şekilde görselleştirmek için kullanılır.\n\n# ggplot2 kütüphanesini yükleme\nlibrary(ggplot2)\n\n# Boxplot oluşturma\nggplot(veri, aes(y = gelir)) +  # 'veri' veri setinden 'gelir' değişkenini \n                                #y eksenine yerleştir\n  geom_boxplot(fill = \"lightblue\") +  # Boxplot çiz ve kutuyu açık mavi renk \n  ggtitle(\"Musteri Gelirleri icin Boxplot\") +  # Grafiğe başlık ekle\n  ylab(\"Gelir\")  # Y ekseninin adını \"Gelir\" olarak belirle\n\n\n\n\n\n\n\nBoxplot’ta kutu dışındaki noktalar aykırı değerleri temsil eder. Üst ve alt sınırların dışında kalan noktalar potansiyel aykırı değerlerdir.\n\n3.2.1.2 Histogram ile Dağılım Analizi\nHistogram, veri setinin genel dağılımını incelemek için kullanılır. Aykırı değerler, histogramda ana dağılımın dışında kalan uç noktalarda yoğunlaşacaktır.\n\n# Histogram oluşturma\nggplot(veri, aes(x = gelir)) +  \n  # 'veri' veri setinden 'gelir' değişkenini x eksenine yerleştir\n  geom_histogram(binwidth = 500, fill = \"lightblue\", color = \"black\") +  \n  # Bin genişliği 500 olan histogram oluştur\n  ggtitle(\"Gelir Dagilimi\") +  # Grafiğe başlık ekle\n  xlab(\"Gelir\") +  # X ekseninin adını \"Gelir\" olarak belirle\n  ylab(\"Frekans\")  # Y ekseninin adını \"Frekans\" olarak belirle\n\n\n\n\n\n\n\nHistogram, verinin çarpıklığını (skewness) gösterir. Eğer sağa veya sola çarpık bir dağılım varsa, bu genellikle aykırı değerlerin etkisiyle ortaya çıkar.\n\n3.2.1.3 Özet İstatistiklerle İlk Değerlendirme\nAykırı değerleri tespit etmek için özet istatistiklerden de faydalanabiliriz:\n\n# Gelir değişkeninin özet istatistiklerini hesaplama\nsummary(veri$gelir)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2691    4441    5062    5611    5811   19200 \n\n\n\nBu çıktı, minimum, maksimum, medyan ve çeyrek değerleri içerir. Eğer maksimum değer, üst çeyrek (Q3) değerinden çok daha büyükse, aykırı değerlerin varlığına işaret edebilir.\nSonuç Yerine\n\n\nBoxplot, aykırı değerleri görselleştirmenin en hızlı yollarından biridir.\n\nHistogram, veri setinin genel dağılımını göstererek aşırı uç değerleri tespit etmeye yardımcı olur.\n\nÖzet istatistikler, minimum ve maksimum değerleri analiz ederek aykırı değerleri sayısal olarak belirlemeye yardımcı olur.\n\n\nBu temel yöntemlerle aykırı değerleri belirledikten sonra, IQR, Z-Skoru veya Mahalanobis mesafesi gibi istatistiksel yöntemlerle daha detaylı bir analiz yapılabilir.\n\n3.2.2 Tek Değişkenli Aykırı Değer Analizi\nTek değişkenli aykırı değer analizi, yalnızca tek bir değişkenin dağılımına bakarak uç değerlerin belirlenmesini sağlar. Aşağıda, farklı yöntemlerle aykırı değerleri tespit etmek için uygulanabilecek teknikler açıklanmaktadır.\nÖrnek Veri Seti Daha önce kullanılan müşteri gelir veri setini kullanmaya devam ediyoruz:\nBu veri setinde yüksek gelirli müşteriler aykırı değer olarak incelenecektir.\n\n3.2.3 Aykırı Değer Tespiti ve Veri Setinden Çıkarılması (Tek Değişkenli)\n\n3.2.3.1 Z-Skoru Yöntemi\nZ-Skoru yöntemi, veri analistleri ve istatistikçiler tarafından özellikle normal dağılıma sahip veri setlerinde aykırı değerleri belirlemek için yaygın olarak kullanılır. Bir gözlemin ortalamadan kaç standart sapma uzaklıkta olduğunu hesaplayarak, ±3 standart sapma dışındaki değerleri aykırı kabul eder. Finans, kalite kontrol ve biyomedikal araştırmalar gibi alanlarda sıkça uygulanır. Ancak, normal dağılım varsayımına dayandığı için çarpık dağılımlarda yanıltıcı sonuçlar verebilir ve uç değerler standart sapmayı artırarak yöntemin duyarlılığını düşürebilir. Buna rağmen, özellikle simetrik dağılımlı veri setlerinde hızlı ve etkili bir aykırı değer tespit yöntemi olarak kullanılmaktadır.\nZ-skoru yöntemi, bir gözlemin ortalamadan kaç standart sapma uzaklıkta olduğunu belirler. Eğer bir gözlem ±3 standart sapmadan daha uzaktaysa, bu gözlem aykırı değer olarak kabul edilir.\nZ-Skoru Yönteminin Temeli Z-Skoru yöntemi, bir veri noktasının ortalamadan kaç standart sapma uzaklıkta olduğunu hesaplar.\nStandart normal dağılımda (%99.7 kuralı - 3 sigma kuralı):\n\nVerilerin %68’i ortalama ±1 standart sapma içinde bulunur.\nVerilerin %95’i ortalama ±2 standart sapma içinde bulunur.\nVerilerin %99.7’si ortalama ±3 standart sapma içinde bulunur.\nBu yüzden, Z-Skoru ±3’ü aşan gözlemler aykırı kabul edilir.\n\n\n# Gelir değişkeni için Z-Skoru yöntemi kullanılarak aykırı değerlerin tespiti\n\n# 1. 'scale()' fonksiyonu ile gelir değişkeninin standartlaştırılması\n# (Z-Skoru hesaplama)\nz_scores &lt;- scale(veri$gelir)\n\n# 2. Z-Skoru mutlak değeri 3'ten büyük olan gözlemlerin aykırı değer olarak \n# belirlenmesi\noutliers_z &lt;- veri$gelir[abs(z_scores) &gt; 3]\n\n# 3. Aykırı değerlerin çıktısını görüntüleme\noutliers_z\n\n[1] 16403.57 14475.61 13938.19 16375.83 19200.22\n\n\n\nNeden Sadece 5 Aykırı Değer Belirlendi?\nZ-Skoru yöntemi verinin normal dağıldığını varsayar.\nEğer veri seti normal dağılıma yakınsa, standart sapma istatistiği doğru şekilde çalışır ve aykırı değerleri doğru tespit edebilir. Ancak veri seti çarpık (skewed) bir dağılıma sahipse, standart sapma aşırı büyük veya küçük olabilir ve Z-Skoru bazı uç noktaları tespit edemeyebilir. Standart sapmanın büyüklüğü aykırı değer tespitini etkileyebilir.\nVeri setinde büyük uç değerler varsa, standart sapma artar (çünkü standart sapma, aşırı uç değerlerden etkilenir). Standart sapma büyüdüğünde, ortalama +3σ eşiği de yükselir, dolayısıyla bazı yüksek değerler 3 standart sapma sınırını aşamayabilir. Bu durumda, normalde aykırı olması beklenen bazı yüksek gelirler, büyük bir standart sapma nedeniyle Z-Skoru yöntemi tarafından aykırı olarak kabul edilmeyebilir. Z-Skoru yöntemi simetrik dağılımlar için uygundur.\nEğer veri sağa çarpık (right-skewed) dağılım gösteriyorsa (yani büyük değerler normalden daha fazla bulunuyorsa), Z-Skoru yöntemi düşük hassasiyet gösterebilir. Çünkü Z-Skoru yöntemi, ortalamaya ve standart sapmaya bağımlıdır ve bu tür çarpık dağılımlarda ortalama üst tarafa kayar. Sonuç: Üst sınır daha yukarı kaydığı için bazı yüksek değerler aykırı olarak algılanmaz. Veri setinde doğal olarak geniş bir dağılım olabilir.\nÖrneğin, müşteri gelirleri genellikle çarpık bir dağılıma sahiptir ve bazı müşterilerin diğerlerine göre çok daha yüksek geliri olabilir. Eğer dağılım doğal olarak genişse, Z-Skoru yöntemi bu genişliği normal kabul ederek sadece en uçtaki birkaç değeri aykırı olarak seçebilir.\nSonuç\n\nZ-Skoru yöntemi 5 adet aykırı değer belirledi, çünkü veri seti muhtemelen normal dağılım göstermiyor ve sağa çarpık bir yapıya sahip.\nStandart sapma büyük olduğu için, bazı yüksek gelirler Z-Skoru için “ortalama civarında” kalmış olabilir.\nEğer veri normal dağılsaydı, standart sapma daha küçük olurdu ve belki daha fazla gözlem aykırı olarak tespit edilirdi.\nBu yüzden Z-Skoru yöntemi, normal dağılım varsayımına daha iyi uyan veri setlerinde daha doğru çalışır.\n\n\n\n3.2.3.2 IQR (Çeyrekler Açıklığı) Yöntemi\nIQR (Çeyrekler Açıklığı) yöntemi, veri analistleri ve istatistikçiler tarafından özellikle çarpık dağılımlara sahip veri setlerinde aykırı değerleri tespit etmek için sıkça kullanılır. Finans, sağlık ve makine öğrenmesi gibi alanlarda, uç değerleri belirlemek ve analizleri güvenilir hale getirmek amacıyla tercih edilir. Normal dağılım varsayımı gerektirmediği için Z-Skoru yöntemine kıyasla daha esnek bir yöntemdir. Ancak, belirlenen eşik değerlerin her veri seti için en uygun olmayabileceği unutulmamalıdır. Buna rağmen, veri temizleme ve istatistiksel analiz süreçlerinde yaygın olarak kullanılan sağlam (robust) bir yöntemdir.\nIQR yöntemi, alt çeyrek (Q1) ve üst çeyrek (Q3) değerleri arasındaki farkı kullanarak aykırı değerleri belirler.\nBu yöntem, veri setinin merkezi dağılımına odaklanır ve uç değerleri belirlemek için alt (Q1) ve üst (Q3) çeyrekler aralığını kullanır.\n\nQ1 (Alt Çeyrek - %25 Dilim): Verinin %25’inin altında kaldığı değer.\nQ3 (Üst Çeyrek - %75 Dilim): Verinin %75’inin altında kaldığı değer.\n\nIQR (Interquartile Range - Çeyrekler Açıklığı): \\(IQR = Q3 - Q1\\)\n\nBu aralık, veri setinin orta %50’lik kısmını temsil eder.\nAykırı değerler, genellikle bu aralığın 1.5 katı kadar alt ve üst limitlerin dışına çıkan değerler olarak tanımlanır.\n\n\n\nAykırı değer sınırları şu şekilde belirlenir:\n\\(\\text{Alt Sınır} = Q1 − 1.5 × IQR\\) $ = Q3 + 1.5 × IQR $\nBu sınırların dışına çıkan tüm gözlemler aykırı kabul edilir.\n\n# IQR (Çeyrekler Açıklığı) yöntemi ile aykırı değer tespiti\n\n# 1. Gelir değişkeninin alt ve üst çeyreklerini hesapla\nQ1 &lt;- quantile(veri$gelir, 0.25)  # 1. çeyrek (Q1 - %25'lik dilim)\nQ3 &lt;- quantile(veri$gelir, 0.75)  # 3. çeyrek (Q3 - %75'lik dilim)\n\n# 2. IQR hesapla (Q3 - Q1 farkı)\nIQR &lt;- Q3 - Q1  # Çeyrekler arası açıklık (Interquartile Range - IQR)\n\n# 3. Aykırı değer eşiklerini belirleme\nlower_bound &lt;- Q1 - 1.5 * IQR  # Alt sınır\nupper_bound &lt;- Q3 + 1.5 * IQR  # Üst sınır\n\n# 4. Aykırı değerleri tespit etme\noutliers_iqr &lt;- veri$gelir[veri$gelir &lt; lower_bound | veri$gelir &gt; upper_bound]\n\n# 5. Aykırı değerleri görüntüleme\noutliers_iqr\n\n [1] 16403.57 14475.61 11855.71 11970.66 11796.93 13938.19 12076.49 16375.83\n [9] 19200.22 12425.94\n\n\n\nNeden Daha Fazla Aykırı Değer Bulundu?\nZ-Skoru yöntemi sadece 5 değer tespit ederken, IQR yöntemi 10 değer tespit etti. Bunun sebebi nedir?\nIQR yöntemi veri dağılımının şeklinden bağımsızdır.\n\nZ-Skoru, normal dağılıma bağımlıdır ve standart sapmayı kullanarak eşik belirler.\nIQR yöntemi ise sadece çeyrekler açıklığını dikkate alır.\nBu nedenle, veri çarpık dağılıma sahipse veya standart sapması büyükse, IQR yöntemi daha fazla aykırı değer bulabilir.\n\nIQR yöntemi ortalamadan etkilenmez, dağılımın orta kısmına odaklanır.\n\nZ-Skoru, verinin ortalamasına ve standart sapmasına bağlı olarak çalıştığı için bazı uç noktaları normal kabul edebilir.\nIQR yöntemi, verinin %50’lik merkezi kısmını esas aldığı için aşırı uçları daha kolay tespit eder.\n\nVeri setinde sağa çarpık bir dağılım olabilir.\n\nEğer veri sağa çarpık (right-skewed) ise, ortalama daha yukarı kayar ve Z-Skoru yöntemi bazı uç değerleri kaçırabilir.\nIQR yöntemi, böyle bir durumda yüksek gelire sahip daha fazla müşteriyi aykırı olarak değerlendirebilir.\n\nSonuç yerine\n\nIQR yöntemi, dağılımın orta %50’lik kısmını temel alarak uç değerleri belirler ve standart sapmadan etkilenmez.\nZ-Skoru yöntemi normal dağılım varsayar ve standart sapmaya bağlıdır, bu yüzden bazı uç değerleri normal kabul edebilir.\nBu nedenle, IQR yöntemi Z-Skoru yöntemine göre daha fazla aykırı değer tespit etmiştir.\nVeri seti normal dağılmış olsaydı, Z-Skoru yöntemi de benzer sayıda aykırı değer bulabilirdi.\nAncak mevcut durumda veri sağa çarpık (right-skewed) olduğu için IQR yöntemi daha hassas bir tespit yapmıştır.\n\n\nIQR yöntemi, verinin merkezi dağılımını baz aldığı için özellikle sağa çarpık (asymmetric) veya geniş varyansa sahip veri setlerinde daha fazla aykırı değer tespit edebilir. Bu durumda, IQR yöntemi veriyi daha iyi temsil eden bir metot olarak değerlendirilmelidir.\n\n3.2.3.3 MAD (Ortanca Mutlak Sapma) Yöntemi\nMAD (Ortanca Mutlak Sapma) yöntemi, aykırı değerlere karşı dayanıklı (robust) bir istatistiksel yöntemdir ve özellikle çarpık dağılımlarda veya uç değerlerin yoğun olduğu veri setlerinde güvenilir sonuçlar sağlar. Medyan merkezli bir ölçüm olduğu için, ortalama ve standart sapmadan etkilenen Z-Skoru yöntemine kıyasla uç değerlerden daha az etkilenir. Finans, biyomedikal ve makine öğrenmesi gibi alanlarda, aşırı uç noktaların yanlış sınıflandırılmasını önlemek amacıyla sıkça kullanılır. Ancak, hesaplaması Z-Skoru ve IQR yöntemlerine göre daha karmaşıktır ve veri setinin dağılımına bağlı olarak eşik değerlerin dikkatle belirlenmesi gerekir. Dayanıklı yapısı nedeniyle, normal dağılım varsayımı gerektirmeyen veri setlerinde güvenilir bir aykırı değer tespit yöntemi olarak tercih edilir.\nOrtanca Mutlak Sapma (MAD) yöntemi, medyanı referans alarak aykırı değerleri belirler ve aşırı uç gözlemlere karşı dayanıklıdır.\nAykırı değer sınırları: - Alt sınır = Medyan - 3 * MAD - Üst sınır = Medyan + 3 * MAD\n\n# MAD (Ortanca Mutlak Sapma) yöntemi ile aykırı değer tespiti\n\n# 1. Gelir değişkeninin MAD değerini hesapla\nmad_value &lt;- mad(veri$gelir)  # Ortanca mutlak sapma (MAD) hesaplanır\n\n# 2. Aykırı değerleri belirleme kriteri:\n# Bir gözlem, medyandan 3 * MAD kadar uzaksa aykırı kabul edilir\noutliers_mad &lt;- veri$gelir[abs(veri$gelir - median(veri$gelir)) &gt; 3 * mad_value]\n\n# 3. Aykırı değerleri görüntüleme\noutliers_mad\n\n [1] 16403.57 14475.61 11855.71 11970.66 11796.93 13938.19 12076.49 16375.83\n [9] 19200.22 12425.94\n\n\n\nIQR ve MAD Sonuçlarının Yorumu\n\nİki yöntem de aynı 10 aykırı değeri tespit etti.\n\n\nBu, veri setinin yapısının çarpık olmasına rağmen her iki yöntemin de güvenilir sonuçlar üretebileceğini gösteriyor.\nEğer veri çok çarpık olsaydı, MAD yöntemi daha farklı bir sonuç verebilirdi.\n\n\nIQR yöntemi çeyrekler açıklığını (Q1 ve Q3) kullanırken, MAD yöntemi doğrudan medyan ve sapma ölçüsüne dayanır.\n\n\nEğer uç değerler çok fazlaysa, IQR yöntemi geniş bir dağılımda bazı noktaları kaçırabilir.\nMAD yöntemi, medyan etrafındaki mutlak sapmaları kullanarak daha geniş çerçevede uç değerleri değerlendirebilir.\n\n\nÇok uç değerler mevcut olsaydı, MAD yöntemi daha az hassas olabilirdi.\n\n\nÇünkü MAD, medyana dayalı bir ölçeklendirme kullanır ve uç değerlerin yayılmasını daha küçük bir aralıkta tutabilir.\nIQR yöntemi ise uç değerler belirli bir aralığın dışına çıktığında daha geniş bir kapsama sahip olabilir.\n\nHangi Durumda Hangi Yöntem Kullanılmalı?\n\nVeri normal veya simetrik dağılıma sahipse: IQR yöntemi iyi bir seçim olabilir.\nVeri sağa/sola çarpık dağılıma sahipse veya çok uç değerler varsa: MAD yöntemi daha güvenilir olabilir.\nEğer veri setinde hem çarpıklık hem de geniş varyasyon mevcutsa: Her iki yöntem birlikte kullanılabilir.\n\nBu durumda, IQR’nin belirlediği sınırları kontrol etmek için MAD yöntemi ek bir test olarak uygulanabilir.\n\n\n\nSonuç: IQR ve MAD Neden Aynı Sonuçları Verdi?\nBu veri setinde her iki yöntemin de aynı 10 gözlemi aykırı olarak belirlemesi, veri setinin yapısından kaynaklanmaktadır.\n\nEğer veri daha çarpık olsaydı, MAD yöntemi bazı ek uç noktaları aykırı olarak belirleyebilir veya bazıları için daha dayanıklı kalabilirdi.\nEğer veri daha homojen dağılıma sahip olsaydı, IQR yöntemi daha dar bir sınır belirleyebilir ve bazı uç noktaları dışarıda bırakabilirdi.\n\nSonuç olarak, veri setindeki çarpıklık düzeyi ve uç noktaların dağılımı bu iki yöntemin aynı sonucu vermesine neden olmuştur.\nEğer veri daha dengesiz bir dağılıma sahip olsaydı, bu iki yöntem arasında belirgin farklılıklar görülebilirdi.\n\nMAD yöntemi, özellikle veri çarpık dağılım gösterdiğinde veya aşırı uç değerler fazla olduğunda daha güvenilir sonuçlar verir.\n\n3.2.3.4 Yöntemlerin Karşılaştırılması\n\n\n\n\n\n\n\nDurum\nÖnerilen Yöntem\nR Uygulaması\n\n\n\nVeri normal dağılmış mı?\nZ-Skoru Yöntemi\nabs(scale(veri$deger)) &gt; 3\n\n\nVeri normal dağılmamış mı?\nIQR Yöntemi\nQ1 - 1.5*IQR, Q3 + 1.5*IQR\n\n\nVeri aşırı uç değerlere dayanıklı bir ölçüme mi ihtiyaç duyuyor?\nMAD Yöntemi\nmedian(abs(veri$deger - median(veri$deger)))\n\n\n\nHer yöntemin avantaj ve dezavantajları bulunur, bu yüzden veri setinin yapısına en uygun olan yöntem seçilmelidir.\n\n3.2.3.5 Genel Karşılaştırma Tablosu\n\n\n\n\n\n\n\n\nYöntem\nNe Zaman Kullanılır?\nAvantajları\nDezavantajları\n\n\n\nZ-Skoru\nVeri normal dağılıyorsa\nKolay uygulanabilir, farklı ölçeklerde çalışır\nNormal dağılım varsayımı gerektirir, uç değerlerden etkilenir\n\n\nIQR (Çeyrekler Açıklığı)\nVeri normal dağılmıyorsa\nAykırı değerlere dayanıklı, uygulanması kolay\nBazı önemli uç değerleri silebilir\n\n\nMAD (Ortanca Mutlak Sapma)\nAykırı değerlere dayanıklı analiz gerekiyorsa\nUç değerlere duyarlı değil, çarpık dağılımlar için uygun\nHesaplaması biraz daha karmaşıktır\n\n\n\n\n\n\n\n\n\nSonuç: Hangi Yöntemi Seçmeli?\n\n\n\nAykırı değer analizi yaparken hangi yöntemin seçileceği, veri setinin yapısına ve dağılım özelliklerine bağlıdır. Z-Skoru yöntemi, normal dağılıma sahip veri setlerinde hızlı bir analiz için uygundur. Ancak, gerçek dünya verilerinde normal dağılım her zaman sağlanamayacağı için, IQR ve MAD gibi uç değerlere dayanıklı (robust) yöntemler daha sık tercih edilmektedir.\nIQR yöntemi, çarpık dağılım gösteren verilerde güvenilir bir seçimdir ve veri bilimciler tarafından en yaygın kullanılan yöntemlerden biridir. Bunun nedeni, hesaplamasının kolay olması, aşırı uç değerlerden etkilenmemesi ve dağılım varsayımı gerektirmemesidir. Özellikle finans, ekonomi, sağlık ve makine öğrenmesi gibi alanlarda, verilerin çoğu çarpık dağılım gösterdiğinden IQR yöntemi daha güvenilir bir seçim olarak öne çıkar.\nMAD yöntemi ise, aykırı değerlere karşı en dayanıklı yöntem olup, aşırı uç verilerin fazla olduğu ve çarpıklığın aşırı yüksek olduğu durumlarda tercih edilir. Ancak, hesaplaması IQR’ye göre daha karmaşıktır ve belirli senaryolarda daha az hassas olabilir.\nÖneri: Eğer veri setinin dağılımı bilinmiyorsa, önce histogram veya boxplot ile dağılım incelenmeli ve en uygun yöntem seçilmelidir. Genellikle IQR yöntemi, pratikliği ve güvenilirliği nedeniyle en sık tercih edilen aykırı değer tespit yöntemidir.\nUygulama Alanı: Mod, özellikle kategorik verilerde en sık görülen grubu veya sınıfı anlamak için yararlıdır. Sayısal verilerde de merkezi eğilimi gösterir, ancak tüm veri setini tam olarak temsil etmeyebilir.\nEksiklikler: Mod her zaman var olmayabilir (tekrarlanan bir değer yoksa). Çok modlu veri setlerinde tek bir merkezi eğilim ölçüsü sağlamak zordur.\n\n\n\n3.2.4 Çok Değişkenli Aykırı Değer Analizi\nÇok değişkenli aykırı değer analizi, bir gözlemin yalnızca tek bir değişken bazında değil, diğer değişkenlerle olan ilişkisine göre de aykırı olup olmadığını belirlemeyi amaçlar.\nNe zaman kullanılır?\n\nEğer iki veya daha fazla değişken arasındaki ilişkilere göre aykırı değerleri tespit etmek istiyorsak.\nEğer bir gözlem tek değişken bazında normal görünüyorsa ama diğer değişkenlerle birlikte değerlendirildiğinde aykırıysa.\n\nÖrnek Veri Seti\nAşağıda, ihracat hacmini etkileyen değişkenleri içeren bir veri seti (data_multiple) oluşturulmuştur.\n\nset.seed(123)\nlibrary(tidyverse)\n\n\ndata_multiple &lt;- data.frame(\n  firma_id = 1:150,\n  ihracat = c(pmax(rnorm(120, mean = 500000, sd = 100000), 0), \n              pmax(rnorm(10, mean = 1000000, sd = 200000), 0), \n              pmax(rnorm(20, mean = 100000, sd = 50000), 0)),  \n  devlet_yardimi = as_factor(sample(c(0, 1), 150, replace = TRUE, \n                                    prob = c(0.7, 0.3))),      \nciro = c(pmax(rnorm(130, mean = 2000000, sd = 500000), 0), \n         pmax(rnorm(10, mean = 5000000, sd = 2000000), 0), \n         pmax(rnorm(10, mean = 800000, sd = 100000), 0)),  \nihracat_ulkesi = c(sample(10:50, 120, replace = TRUE), \n                   sample(30:80, 20, replace = TRUE), \n                   sample(1:10, 10, replace = TRUE)),  \ncalisan_sayisi = c(round(pmax(rnorm(140, mean = 150, sd = 50), 5)), \n                   round(pmax(rnorm(5, mean = 400, sd = 150), 5)), \n                   round(pmax(rnorm(5, mean = 20, sd = 5), 5))),  \n  faaliyet_yili = c(sample(1:50, 130, replace = TRUE), \n                    sample(50:100, 20, replace = TRUE))\n)\n\nhead(data_multiple)\n\n  firma_id  ihracat devlet_yardimi    ciro ihracat_ulkesi calisan_sayisi\n1        1 443952.4              1 2029875             21            222\n2        2 476982.3              0 1647702             20            202\n3        3 655870.8              1 1641391             16            172\n4        4 507050.8              1 2442325             12            186\n5        5 512928.8              0 1492204             32            196\n6        6 671506.5              0 2977647             30             17\n  faaliyet_yili\n1            43\n2            45\n3            25\n4            36\n5            25\n6            50\n\n\nBu data_multiple setinde ihracat (bağımlı değişken) ve bağımsız değişkenler olarak devlet yardımı, ciro, ihracat yapılan ülke sayısı, çalışan sayısı ve faaliyet yılı bulunmaktadır.\n\nsummary(data_multiple)\n\n    firma_id         ihracat        devlet_yardimi      ciro         \n Min.   :  1.00   Min.   :      0   0:106          Min.   :   68204  \n 1st Qu.: 38.25   1st Qu.: 407386   1: 44          1st Qu.: 1639746  \n Median : 75.50   Median : 482660                  Median : 2011415  \n Mean   : 75.50   Mean   : 478313                  Mean   : 2190169  \n 3rd Qu.:112.75   3rd Qu.: 563527                  3rd Qu.: 2439966  \n Max.   :150.00   Max.   :1368772                  Max.   :10142916  \n ihracat_ulkesi  calisan_sayisi  faaliyet_yili  \n Min.   : 1.00   Min.   :  5.0   Min.   : 1.00  \n 1st Qu.:18.00   1st Qu.:120.2   1st Qu.:14.00  \n Median :30.00   Median :154.0   Median :28.50  \n Mean   :30.52   Mean   :156.6   Mean   :31.82  \n 3rd Qu.:40.00   3rd Qu.:187.8   3rd Qu.:43.00  \n Max.   :78.00   Max.   :526.0   Max.   :98.00  \n\n\n\n\n\nİhracat (Bağımlı Değişken): Büyük bir dağılım var, bazı firmaların ihracat değeri oldukça düşükken bazıları aşırı yüksek olabilir (potansiyel aykırı değerler var).\n\nDevlet Yardımı (Kategori): Çoğu firma devlet yardımı almıyor, ancak yaklaşık %30’u destek alıyor.\n\nCiro (Bağımsız Değişken): Ciro ile ihracat arasında büyük farklar olabilir. Bazı firmaların cirosu aşırı yüksek (potansiyel aykırı değerler var).\n\nİhracat Yapılan Ülke Sayısı: Çoğu firma 30-40 ülkeye ihracat yapıyor, ancak 80 ülkeye ihracat yapan firmalar uç değer olabilir.\n\nÇalışan Sayısı: Çalışan sayısında büyük bir değişkenlik var, bazı firmalar küçük ölçekli iken bazıları oldukça büyük.\n\nFaaliyet Yılı: Firma yaşlarında ciddi bir farklılık var. Uzun süredir faaliyet gösteren firmalar ile yeni kurulan firmalar arasında ciddi fark olabilir.\n\n\n\n3.2.5 Aykırı Değer Tespiti ve Veri Setinden Çıkarılması (Çok Değişkenli)\nAykırı değerleri tespit etmek için Mahalanobis Mesafesi, İzolasyon Ormanı ve DBSCAN yöntemleri kullanılabilir.\n\n3.2.5.1 Mahalanobis Mesafesi\nMahalanobis mesafesi, çok değişkenli aykırı değerleri belirlemek için kullanılan istatistiksel bir yöntemdir. Bu yöntem, bir gözlemin tüm değişkenler açısından veri setinin merkezinden (ortalama vektöründen) ne kadar uzak olduğunu ölçer. Öklidyen mesafeye benzer ancak verinin kovaryans yapısını dikkate alarak ölçek bağımsız ve yön duyarlı bir mesafe ölçüsü sağlar.\nBu mesafe, bir gözlemin merkezden ne kadar farklı olduğunu ölçmek için kullanılır. Değeri ne kadar büyükse, gözlem diğerlerinden o kadar farklıdır ve aykırı olma olasılığı artar.\n\n# Gerekli paketi yükleyelim (eğer yüklü değilse)\nif (!requireNamespace(\"MASS\", quietly = TRUE)) {\n  install.packages(\"MASS\")\n}\n\n# Gerekli kütüphaneyi çağıralım\nlibrary(MASS)  # Mahalanobis mesafesi hesaplaması için gerekli paket\n\n# Gerekli değişkenleri seçelim\ndata_multiple_mahal &lt;- data_multiple[, c(\"ihracat\", \"ciro\", \"ihracat_ulkesi\",\n                                         \"calisan_sayisi\", \"faaliyet_yili\")]\n\n# Mahalanobis mesafesini hesaplayalım\nmean_vector &lt;- colMeans(data_multiple_mahal)\ncov_matrix &lt;- cov(data_multiple_mahal)\nmahal_dist &lt;- mahalanobis(data_multiple_mahal, mean_vector, cov_matrix)\n\n# Kritik eşik belirleme (χ² testi, df = değişken sayısı, %95 güven aralığı)\nthresh &lt;- qchisq(0.95, df = ncol(data_multiple_mahal))\n\n# Aykırı değerleri belirleyelim (eşik değerini aşan gözlemler aykırıdır)\ndata_multiple_mahal$aykiri_mahal &lt;- mahal_dist &gt; thresh\n\n# Aykırı gözlemleri içeren veri setini oluşturalım\ndata_multiple_mahal_outlier &lt;- \n   data_multiple_mahal[data_multiple_mahal$aykiri_mahal == TRUE, ]\n\n# Aykırı gözlemleri ana veri seti ile eşleştirerek devlet yardımı değişkenini ekleyelim\ndata_multiple_mahal_result &lt;- data_multiple[data_multiple$firma_id \n                                            %in% rownames(data_multiple_mahal_outlier), ]\n\n# Yeni veri setini tibble formatında görüntüleyelim\nas_tibble(data_multiple_mahal_result)\n\n# A tibble: 21 × 7\n   firma_id  ihracat devlet_yardimi      ciro ihracat_ulkesi calisan_sayisi\n      &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt;          &lt;int&gt;          &lt;dbl&gt;\n 1      121 1023529. 0               1123381.             37            198\n 2      123  901889. 0               1714075.             68            111\n 3      125 1368772. 1               1910047.             73            128\n 4      132  122575. 0               5822860.             76            145\n 5      133  102062. 0               4933928.             64             91\n 6      134   78875. 1                 68204.             37            175\n 7      135       0  0              10142916.             68             98\n 8      136  156567. 0               4589401.             78            139\n 9      137   26968. 0               6302387.             51            169\n10      139  195455. 0               7049346.             43            179\n# ℹ 11 more rows\n# ℹ 1 more variable: faaliyet_yili &lt;int&gt;\n\n\n\n\nMahalanobis Mesafesi İçin Değişken Seçimi\n\ndata_multiple_mahal &lt;- data_multiple[, c(\"ihracat\", \"ciro\", \"ihracat_ulkesi\", \"calisan_sayisi\", \"faaliyet_yili\")]\n\nBu değişkenler, çok değişkenli aykırı değer tespiti için neden seçildi?\n\nMahalanobis mesafesi, birden fazla değişken arasındaki ilişkileri dikkate alarak aykırı gözlemleri belirler. Bu nedenle, doğrudan sayısal değer içeren ve işletmenin büyüklüğü veya performansını gösteren değişkenler seçilmiştir.\n\n\n\n\n\n\n\nDeğişken\nAçıklama\nNeden Seçildi?\n\n\n\nihracat\nFirmanın toplam ihracat miktarı\nİşletmenin finansal büyüklüğünü gösterir\n\n\nciro\nFirmanın toplam geliri (cirosu)\nGenellikle ihracat ile ilişkilidir, finansal durumu gösterir\n\n\nihracat_ulkesi\nFirmanın ihracat yaptığı ülke sayısı\nİşletmenin uluslararası faaliyet kapsamını gösterir\n\n\ncalisan_sayisi\nFirmanın çalışan sayısı\nFirmanın ölçeği ve büyüklüğü hakkında bilgi verir\n\n\nfaaliyet_yili\nFirmanın kaç yıldır faaliyette olduğu\nİşletmenin deneyim seviyesi ve sürdürülebilirliği hakkında bilgi verir\n\n\n\n\nNeden devlet_yardimi Seçilmedi?\n\nDevlet yardımı (0 veya 1) gibi kategorik değişkenler, Mahalanobis mesafesi hesaplamalarında anlamlı değildir.\nMahalanobis mesafesi kovaryans matrisine dayandığı için sürekli (sayısal) değişkenler gereklidir.\nEğer kategorik değişkenler kullanılmak istenirse, önce one-hot encoding (dummy değişkenler) gibi dönüşümler yapılmalıdır.\n\n\n\n\nMahalanobis Mesafesi: Veri Noktalarının Merkezden Uzaklığını Ölçme\n\n\n\nOrtalama Vektörü Hesaplanıyor\nmean_vector &lt;- colMeans(data_multiple_mahal)\n\nHer değişkenin (sütunun) ortalama değeri hesaplanıyor.\nVeri setinin merkez noktası belirleniyor.\nBu ortalama vektör, gözlemlerin merkeze ne kadar uzak olduğunu hesaplamak için kullanılacak.\n\n\n\nKovaryans Matrisi Hesaplanıyor\ncov_matrix &lt;- cov(data_multiple_mahal)\n\nVeri setindeki değişkenlerin birbirleriyle ilişkileri ölçülüyor.\nDeğişkenler arasındaki kovaryans hesaplanarak ölçek farkları dengeleniyor.\nÖklidyen mesafeden farklı olarak, Mahalanobis mesafesi değişkenlerin kovaryans yapısını dikkate alıyor.\n\n\n\nMahalanobis Mesafesi Hesaplanıyor\nmahal_dist &lt;- mahalanobis(data_multiple_mahal, mean_vector, cov_matrix)\n\nHer gözlemin (satırın) veri setinin merkezine (ortalama vektörüne) olan Mahalanobis mesafesi hesaplanıyor.\nBu mesafe ne kadar büyükse, gözlem veri setinin genel yapısından o kadar farklıdır ve aykırı olma ihtimali yüksektir.\n\n\n\n\nMahalanobis Mesafesi İçin Ki-Kare Eşik Değeri Hesaplama\n\nBu kod bloğunda, Mahalanobis mesafesi için kritik eşik belirleniyor. İşlemler adım adım şu şekilde gerçekleşiyor:\n\n\nKi-kare Dağılımı Kullanılarak Eşik Değeri Hesaplanıyor\nthresh &lt;- qchisq(0.95, df = ncol(data_multiple_mahal))\n\n\nqchisq(0.95, df) fonksiyonu, ki-kare dağılımının %95’lik yüzdelik dilimini hesaplar.\nEğer bir gözlemin Mahalanobis mesafesi bu eşik değerinden büyükse, aykırı kabul edilir.\n%95 güven aralığı, gözlemlerin %5’inin aykırı kabul edileceğini gösterir.\n\n\n\nSerbestlik Derecesi (df) Belirleniyor\ndf = ncol(data_multiple_mahal)\n\ndf (derece serbestlik) = Veri setindeki değişken (sütun) sayısıdır.\nHer eklenen değişken, gözlemlerin çok değişkenli dağılımdaki konumunu etkiler.\nBu yüzden df, değişken sayısına eşit alınır.\n\n\n\n\nSonuçların Değerlendirilmesi:\nŞu anki veri setimizde 18 tane aykırı değere sahip firma tespit edildi ve devlet yardımı alan firmalardan 6 tanesi de aykırı olarak tespit edildi.\nBu durum analiz sonuçlarını etkileyebilir çünkü:\n\nEğer bu firmalar aşırı yüksek ihracat yapan firmalar ise, devlet yardımı alan firmaların ihracatlarının genelde arttığına dair yanlış bir çıkarım yapabiliriz.\nTam tersi, eğer bu firmalar aşırı düşük ihracat yapan firmalar ise, devlet yardımı alan firmaların başarısız olduğu gibi hatalı bir sonuç çıkabilir.\nBu yüzden, devlet yardımı alan firmalar içindeki aykırı gözlemleri temizleyerek analiz yapmak daha doğru olacaktır.\n\nAykırı Olmayan Gözlemler (Mahalonobis)\n\n# Aykırı olmayan gözlemleri filtreleyerek temiz bir veri seti oluşturma\nmahal_data_clean &lt;- data_multiple[!data_multiple$firma_id %in% rownames(data_multiple_mahal_result), ]\n\n# Yeni veri setini tibble formatında görüntüleyelim\nas_tibble(mahal_data_clean)\n\n# A tibble: 129 × 7\n   firma_id ihracat devlet_yardimi     ciro ihracat_ulkesi calisan_sayisi\n      &lt;int&gt;   &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;          &lt;int&gt;          &lt;dbl&gt;\n 1        1 443952. 1              2029875.             21            222\n 2        2 476982. 0              1647702.             20            202\n 3        3 655871. 1              1641391.             16            172\n 4        4 507051. 1              2442325.             12            186\n 5        5 512929. 0              1492204.             32            196\n 6        6 671506. 0              2977647.             30             17\n 7        7 546092. 0              1954840.             30            206\n 8        8 373494. 0              2107269.             46            126\n 9        9 431315. 0              1630736.             11            162\n10       10 455434. 0              1712806.             18            135\n# ℹ 119 more rows\n# ℹ 1 more variable: faaliyet_yili &lt;int&gt;\n\n\nSonuç: Mahalanobis Mesafesi Kullanmalı mıyım?\n\n\n\n\n\n\nAvantajları\nDezavantajları\n\n\n\nÇok değişkenli aykırı değerleri belirler.\nVeri normal dağılmıyorsa yanlış sonuçlar verebilir.\n\n\nÖlçek bağımsızdır, farklı ölçeklerdeki değişkenleri dengeler.\nÇok değişkenli normal dağılım varsayımı yapar.\n\n\nDeğişkenler arasındaki ilişkileri dikkate alır.\nKovaryans matrisi tekillik sorunu yaşayabilir.\n\n\nHızlı hesaplanabilir ve yorumlanabilir.\nBüyük veri setlerinde hesaplama maliyeti artabilir.\n\n\n\nÖneri:\nEğer veri normal dağılıma yakınsa ve doğrusal ilişkiler varsa, Mahalanobis mesafesi güvenilir bir yöntemdir. Ancak, veri çarpık dağılım gösteriyorsa veya çok karmaşıksa, İzolasyon Ormanı (Isolation Forest) veya DBSCAN gibi yöntemler daha uygun olabilir.\nUygunluk Değerlendirmesi:\nMahalanobis Mesafesi, eğer veri normal dağılıma yakınsa ve değişkenler arasındaki ilişki doğrusal ise oldukça etkili olabilir. Ancak, bizim veri setimizde değişkenlerin dağılımı normal olmayabilir ve ciro, ihracat gibi değişkenler oldukça değişkenlik gösterebilir. Bu durumda Mahalanobis Mesafesi yanıltıcı sonuçlar verebilir.\n\n3.2.5.2 İzolasyon Ormanı (Isolation Forest)\nIsolation Forest, aykırı değerleri tespit etmek için karar ağaçlarına dayalı bir makine öğrenmesi yöntemidir. Bu yöntem, rastgele alt kümeler alarak veri noktalarını izole eder ve bir gözlemin ne kadar hızlı izole edilebildiğini ölçer. Aykırı gözlemler, diğer gözlemlerden daha az bölme (izolasyon) gerektirdiği için, daha düşük derinlikte daha erken izole edilir.\nBu yöntem, verinin dağılımına bağımlı değildir ve büyük veri setlerinde ölçeklenebilir bir yapı sunar. Aykırılık skoru 0 ile 1 arasında değişir ve 1’e yakın değerler gözlemin aykırı olma olasılığını artırır. Özellikle karmaşık veri setlerinde, doğrusal olmayan ilişkileri tespit etmek için kullanışlıdır.\n\n# Gerekli değişkenleri seçelim\ndata_multiple_iso &lt;- data_multiple[, c(\"ihracat\", \"ciro\", \"ihracat_ulkesi\",\n                                       \"calisan_sayisi\", \"faaliyet_yili\")]\n\n# Gerekli paketi yükleyelim (eğer yüklü değilse)\nif (!requireNamespace(\"isotree\", quietly = TRUE)) {\n  install.packages(\"isotree\")\n   }\n\n# Gerekli kütüphaneyi çağıralım\nlibrary(isotree)  # Isolation Forest modeli için gerekli paket\n\n# Isolation Forest modelini oluşturalım\niso_model &lt;- isotree::isolation.forest(data_multiple_iso, ntrees = 100)\n\n# Aykırılık skorlarını tahmin edelim\noutlier_scores &lt;- predict(iso_model, data_multiple_iso)\n\n# Aykırı gözlemleri belirleyelim (skoru 0.6'dan büyük olanlar aykırıdır)\ndata_multiple_iso$aykiri_iso &lt;- outlier_scores &gt; 0.6\n\n# Aykırı gözlemleri içeren veri setini oluşturalım\ndata_multiple_iso_outlier &lt;- \n   data_multiple_iso[data_multiple_iso$aykiri_iso == TRUE, ]\n\n# Aykırı gözlemleri ana veri seti ile eşleştirerek devlet yardımı \n# değişkenini ekleyelim\ndata_multiple_iso_result &lt;- data_multiple[data_multiple$firma_id %in% \n                                             rownames(data_multiple_iso_outlier), ]\n\n# Yeni veri setini tibble formatında görüntüleyelim\nas_tibble(data_multiple_iso_result)\n\n# A tibble: 5 × 7\n  firma_id  ihracat devlet_yardimi      ciro ihracat_ulkesi calisan_sayisi\n     &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt;          &lt;int&gt;          &lt;dbl&gt;\n1      125 1368772. 1               1910047.             73            128\n2      135       0  0              10142916.             68             98\n3      139  195455. 0               7049346.             43            179\n4      143   21393. 1                705459.              7            526\n5      145   19923. 1                753896.              9            476\n# ℹ 1 more variable: faaliyet_yili &lt;int&gt;\n\n\n\n\n\nAykırı Değer Tespiti İçin Gerekli Değişkenlerin Seçilmesi\ndata_multiple_iso &lt;- data_multiple[, c(\"ihracat\", \"ciro\", \"ihracat_ulkesi\", \"calisan_sayisi\", \"faaliyet_yili\")]\n\nAnalizde kullanılacak sayısal değişkenler seçiliyor.\nKategorik değişkenler dahil edilmeden sadece sayısal değişkenlerle işlem yapılıyor.\n\n\n\nIsolation Forest Modelinin Kurulması ve Aykırı Değerlerin Belirlenmesi\nlibrary(isotree) iso_model &lt;- isotree::isolation.forest(data_multiple_iso, ntrees = 100)\n\nIsolation Forest modeli oluşturuluyor.\n100 ağaç (ntrees = 100) kullanılarak model eğitiliyor.\n\n\n\noutlier_scores &lt;- predict(iso_model, data_multiple_iso) data_multiple_iso$aykiri_iso &lt;- outlier_scores &gt; 0.6  # Skoru 0.6'dan büyük olanlar aykırı\n\nModel, her gözlem için bir “aykırılık skoru” hesaplıyor.\nSkoru 0.6’dan büyük olan gözlemler aykırı olarak kabul ediliyor.\n\n\n\n\n\n\n\n\n\n\nAğaç sayısı neye göre belirlenir\n\n\n\nIsolation Forest modelinde ntrees = 100 genellikle hız ve doğruluk arasında iyi bir denge sağladığı için tercih edilir. Her bir ağaç, veri setinden rastgele örnekler alarak gözlemleri izole etmeye çalışır ve bu süreç ne kadar az bölme ile gerçekleşirse, gözlem o kadar aykırı kabul edilir. Daha fazla ağaç kullanmak, modelin daha tutarlı ve stabil sonuçlar vermesini sağlar çünkü her ağaç farklı alt kümelerle eğitildiğinden, tek bir ağacın etkisi azalır ve genel eğilim daha güvenilir hale gelir. Ancak, ağaç sayısının fazla olması işlem süresini artırırken, çok az olması modelin kararsız olmasına yol açabilir. Küçük veri setlerinde ntrees = 50 genellikle yeterli olurken, orta ölçekli veri setlerinde ntrees = 100 - 200, büyük veri setlerinde ise ntrees = 200+ kullanmak daha iyi sonuç verir. Eğer hız ön planda ise daha az ağaç kullanılabilir, ancak daha hassas aykırı değer tespiti yapılmak isteniyorsa ağaç sayısının artırılması önerilir. Bu nedenle, 100 ağaç genellikle güvenilir ve dengeli bir seçim olarak kabul edilir.\n\n\n\n\n\n\n\n\n0,6 Ne Anlama Geliyor ve Neden Seçildi\n\n\n\nIsolation Forest modelinde predict() fonksiyonu, her gözlem için bir “aykırılık skoru” (outlier_scores) üretir. Bu skor, bir gözlemin model tarafından ne kadar hızlı izole edilebildiğini gösterir. Skor değeri 0 ile 1 arasında değişir ve şu şekilde yorumlanır:\n\n0’a yakın skorlar → Normal gözlemler (izole edilmesi zor, yani veri kümesine iyi uyuyor).\n1’e yakın skorlar → Aykırı gözlemler (çok hızlı izole edilebiliyor, yani genel dağılımdan çok farklı).\n\nBu şu anlama gelir: Eğer bir gözlemin Isolation Forest tarafından hesaplanan skoru 0.6’dan büyükse, bu gözlem aykırı kabul edilir.\nNeden 0.6 Seçildi? Daha Fazla veya Daha Az Olamaz mıydı?\nGenellikle 0.6, normal gözlemler ile aykırı gözlemleri ayırt etmek için dengeli bir eşik değeridir. Ancak, analiz yapılan veri setine bağlı olarak bu değer değiştirilebilir. Eğer veri setinde çok fazla aykırı değer olduğu düşünülüyorsa, eşik değeri artırılabilir (0.7 veya 0.8 yapılabilir). Eğer daha fazla gözlem aykırı kabul edilmek isteniyorsa, eşik değeri düşürülebilir (0.5 veya 0.55 yapılabilir).\n\n\n\n\n\nAykırı Olan Gözlemlerin Seçilmesi\ndata_multiple_iso_outlier &lt;- data_multiple_iso[data_multiple_iso$aykiri_iso == TRUE, ]\n\nAykırı olarak belirlenen gözlemler ayrı bir veri setine kaydediliyor.\n\n\n\nAykırı Gözlemleri Ana Veri Seti ile Eşleştirerek Devlet Yardımı Bilgisinin Eklenmesi\ndata_multiple_iso_result &lt;- data_multiple[data_multiple$firma_id %in% rownames(data_multiple_iso_outlier), ] as_tibble(data_multiple_iso_result)\n\nAykırı gözlemlerin firma_id’si kullanılarak ana veri setindeki devlet yardımı değişkeniyle eşleştirilmesi yapılıyor.\nBöylece devlet yardımı alan firmalar arasında aykırı olanlar analiz edilebilir.\n\n\n\n\nSonuçların Değerlendirilmesi:\nŞu anki veri setimizde 9 tane aykırı değere sahip firma tespit edildi ve bu firmalardan 3 tanesi devlet yardımı almış firmalar arasındadır.\nBu durum analiz sonuçlarını etkileyebilir çünkü:\n\nEğer bu firmalar aşırı yüksek ihracat yapan firmalar ise, devlet yardımı alan firmaların ihracatlarının genelde arttığına dair yanlış bir çıkarım yapabiliriz.\nTam tersi, eğer bu firmalar aşırı düşük ihracat yapan firmalar ise, devlet yardımı alan firmaların başarısız olduğu gibi hatalı bir sonuç çıkabilir.\nBazı firmalar hiç ihracat yapmadığı halde model tarafından aykırı olarak belirlenmiştir, bu da sonuçları yanıltabilir.\n\nBu yüzden, devlet yardımı alan firmalar içindeki aykırı gözlemleri temizleyerek analiz yapmak daha doğru olacaktır. Böylece, aykırı gözlemlerden kaynaklanan sapmalar engellenerek, devlet desteğinin ihracat üzerindeki etkisi daha sağlıklı bir şekilde ölçülebilir.\nAykırı Olmayan Gözlemler (Isolation Forest)\n\n# Aykırı olmayan gözlemleri filtreleyerek temiz bir veri seti oluşturma\niso_data_clean &lt;- data_multiple[!data_multiple$firma_id \n                                %in% rownames(data_multiple_iso_outlier), ]\n\n# Yeni veri setini tibble formatında görüntüleyelim\nas_tibble(iso_data_clean)\n\n# A tibble: 145 × 7\n   firma_id ihracat devlet_yardimi     ciro ihracat_ulkesi calisan_sayisi\n      &lt;int&gt;   &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;          &lt;int&gt;          &lt;dbl&gt;\n 1        1 443952. 1              2029875.             21            222\n 2        2 476982. 0              1647702.             20            202\n 3        3 655871. 1              1641391.             16            172\n 4        4 507051. 1              2442325.             12            186\n 5        5 512929. 0              1492204.             32            196\n 6        6 671506. 0              2977647.             30             17\n 7        7 546092. 0              1954840.             30            206\n 8        8 373494. 0              2107269.             46            126\n 9        9 431315. 0              1630736.             11            162\n10       10 455434. 0              1712806.             18            135\n# ℹ 135 more rows\n# ℹ 1 more variable: faaliyet_yili &lt;int&gt;\n\n\nSonuç: Isolation Forest Kullanmalı mıyım?\n\n\n\n\n\n\nAvantajları\nDezavantajları\n\n\n\nAykırı değerleri belirlemek için dağılım varsayımına ihtiyaç duymaz, veri normal dağılmak zorunda değildir.\nHiperparametre ayarlaması gerektirir, özellikle ağaç sayısı (ntrees) ve eşik değeri (threshold) analiz sonuçlarını etkileyebilir.\n\n\nKarmaşık ve doğrusal olmayan ilişkileri tespit edebilir.\nAykırı değerlerin neden aykırı olduğunu doğrudan açıklamaz, yalnızca skorlara dayalı karar verir.\n\n\nBüyük veri setlerinde ölçeklenebilir, veri hacmi arttığında bile verimli çalışır.\nÇok küçük veri setlerinde güvenilirliği azalabilir, az sayıda gözlem olduğunda daha fazla hata yapabilir.\n\n\nHızlı çalışır ve hesaplama maliyeti düşüktür, yüksek boyutlu veri setlerinde de uygulanabilir.\nNihai sonuçlar modelin eğitimine bağlıdır, farklı çalıştırmalarda küçük farklılıklar olabilir.\n\n\n\nÖneri:\nEğer veri normal dağılmıyorsa, değişkenler arasındaki ilişki karmaşıksa ve büyük veri setleriyle çalışıyorsanız, Isolation Forest güçlü bir alternatiftir. Özellikle yoğunluk bazlı veya doğrusal olmayan aykırı değerleri yakalamak için Mahalanobis mesafesinden daha etkili olabilir.\nUygunluk Değerlendirmesi:\nIsolation Forest bizim veri setimiz için oldukça uygundur. Firmalar arasındaki büyüklük farklılıklarını ve doğrusal olmayan ilişkileri yakalayabilir. Ayrıca verinin normal dağılım gösterip göstermediği önemli olmadığı için, değişkenler arasındaki ilişkilere dayanarak iyi bir aykırı değer analizi yapabilir. Ancak, eşik değeri (threshold) dikkatli seçilmeli ve sonuçlar diğer yöntemlerle karşılaştırılmalıdır.\n\n3.2.5.3 DBSCAN (Yoğunluk Bazlı Kümeleme)\nDBSCAN, yoğunluk tabanlı bir kümeleme algoritmasıdır ve yoğunluk düşük alanlarda kalan gözlemleri aykırı olarak kabul eder. Bu yöntem, veri noktalarının yoğunluklarını ölçerek kümeler oluşturur ve belirlenen bir eps mesafesi içinde en az minPts kadar komşuya sahip olmayan noktaları aykırı değer olarak işaretler.\nDBSCAN, önceden belirlenmiş küme sayısına ihtiyaç duymadan çalışır ve karmaşık şekilli kümeleri tespit edebilir. Ancak, veri setine uygun eps ve minPts değerlerinin dikkatli seçilmesi gereklidir. Küme numarası 0 olan gözlemler aykırı olarak kabul edilir ve bu gözlemler veri setinin genel yoğunluğundan farklı alanlarda bulunur.\n\n# Gerekli değişkenleri seçelim\ndata_multiple_dbscan &lt;- data_multiple[, c(\"ihracat\", \"ciro\", \"ihracat_ulkesi\",\n                                          \"calisan_sayisi\", \"faaliyet_yili\")]\n\n# Gerekli paketi yükleyelim (eğer yüklü değilse)\nif (!requireNamespace(\"dbscan\", quietly = TRUE)) {\n  install.packages(\"dbscan\")\n}\n\n# Gerekli kütüphaneyi çağıralım\nlibrary(dbscan)  # DBSCAN kümeleme yöntemi için gerekli paket\n\n# Veriyi ölçekleyelim\nscaled_data &lt;- scale(data_multiple_dbscan)\n\n# DBSCAN modelini oluşturalım\ndb_model &lt;- dbscan::dbscan(scaled_data, eps = 1, minPts = 5)\n\n# Aykırı gözlemleri belirleyelim (küme numarası 0 olanlar aykırıdır)\ndata_multiple_dbscan$aykiri_dbscan &lt;- db_model$cluster == 0\n\n# Aykırı gözlemleri içeren veri setini oluşturalım\ndata_multiple_dbscan_outlier &lt;- \n   data_multiple_dbscan[data_multiple_dbscan$aykiri_dbscan == TRUE, ]\n\n# Aykırı gözlemleri ana veri seti ile eşleştirerek \n# devlet yardımı değişkenini ekleyelim\ndata_multiple_dbscan_result &lt;- \n   data_multiple[data_multiple$firma_id %in% rownames(data_multiple_dbscan_outlier), ]\n\n# Yeni veri setini tibble formatında görüntüleyelim\nas_tibble(data_multiple_dbscan_result)\n\n# A tibble: 26 × 7\n   firma_id  ihracat devlet_yardimi     ciro ihracat_ulkesi calisan_sayisi\n      &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;          &lt;int&gt;          &lt;dbl&gt;\n 1        6  671506. 0              2977647.             30             17\n 2       72  269083. 0              2933426.             22            145\n 3      121 1023529. 0              1123381.             37            198\n 4      122  810505. 0              2049664.             38             77\n 5      123  901889. 0              1714075.             68            111\n 6      124  948782. 0              1512995.             56            166\n 7      125 1368772. 1              1910047.             73            128\n 8      126  869610. 0              2507472.             64            219\n 9      127 1047077. 0              1003626.             30            184\n10      128 1015592. 0              1786360.             65            154\n# ℹ 16 more rows\n# ℹ 1 more variable: faaliyet_yili &lt;int&gt;\n\n\n\n\n\nAykırı Değer Tespiti İçin Gerekli Değişkenler Seçiliyor\ndata_multiple_dbscan &lt;- data_multiple[, c(\"ihracat\", \"ciro\", \"ihracat_ulkesi\", \"calisan_sayisi\", \"faaliyet_yili\")]\n\nAnalizde kullanılacak sayısal değişkenler seçiliyor.\nKategorik değişkenler dahil edilmeden sadece sayısal değişkenler kullanılıyor.\n\n\n\nVeri Ölçekleniyor\nscaled_data &lt;- scale(data_multiple_dbscan)\n\nDBSCAN yöntemi, değişkenlerin farklı ölçeklere sahip olması durumunda doğru çalışmayabilir.\nBu yüzden, tüm değişkenler scale() fonksiyonu ile standartlaştırılıyor (ortalama = 0, standart sapma = 1 olacak şekilde dönüştürülüyor).\n\n\n\nDBSCAN Modeli Oluşturuluyor ve Aykırı Gözlemler Belirleniyor\ndb_model &lt;- dbscan::dbscan(scaled_data, eps = 1, minPts = 5)\n\nDBSCAN algoritması çalıştırılıyor.\n\neps = 1, bir gözlemin kümeye dahil olması için gereken maksimum mesafeyi belirler.\n\nminPts = 5, bir küme oluşturmak için en az kaç noktanın birbirine yakın olması gerektiğini tanımlar.\n\n\n\n\n\n\n\n\n\n\nParametreler Neye Göre Seçildi\n\n\n\nDBSCAN algoritmasında eps = 1, bir noktanın kümeye dahil olabilmesi için gereken maksimum mesafeyi belirler. Küçük eps değeri fazla aykırı değer tespitine, büyük eps ise kümelerin birleşmesine neden olabilir. minPts = 5, bir küme oluşturmak için gereken minimum nokta sayısını tanımlar. Çok küçük minPts yanlış kümeler oluşturabilir, çok büyük minPts ise küçük kümeleri göz ardı edebilir. Bu değerler genellikle veri setine göre optimize edilmelidir.\n\n\n\n\n\nDBSCAN, küme numarası 0 olan gözlemleri aykırı olarak kabul eder.\ndata_multiple_dbscan$aykiri_dbscan &lt;- db_model$cluster == 0\n\n\nDBSCAN çıktısından aykırı olan gözlemler (cluster == 0) belirleniyor.\nBu kod, DBSCAN algoritmasının çıktılarını kullanarak aykırı değerleri belirliyor. DBSCAN kümeleme yönteminde küme numarası 0 olan gözlemler aykırı olarak kabul edilir. db_model$cluster değişkeni, her gözlem için hangi kümeye ait olduğunu gösterir.\nKodda, db_model$cluster == 0 ifadesi küme numarası 0 olan gözlemleri TRUE olarak işaretleyerek aykırı değer olarak belirler ve bu bilgiler data_multiple_dbscan$aykiri_dbscan değişkenine kaydedilir. Böylece, aykırı olan ve kümelere dahil edilemeyen gözlemler veri setinde işaretlenmiş olur.\n\n\n\n\nAykırı Gözlemler İçin Yeni Bir Veri Seti Oluşturuluyor\ndata_multiple_dbscan_outlier &lt;- data_multiple_dbscan[data_multiple_dbscan$aykiri_dbscan == TRUE, ]\n\nSadece aykırı olan gözlemlerden oluşan bir veri seti oluşturuluyor.\n\n\n\nAykırı Gözlemleri Ana Veri Seti ile Eşleştirerek Devlet Yardımı Bilgisi Ekleniyor\ndata_multiple_dbscan_result &lt;- data_multiple[data_multiple$firma_id %in% rownames(data_multiple_dbscan_outlier), ]\n\nAykırı firmaların firma_id’leri kullanılarak, ana veri setindeki devlet yardımı bilgisiyle eşleştirme yapılıyor.\n\n\n\nSonuçlar Tibble Formatında Görüntüleniyor\nas_tibble(data_multiple_dbscan_result)\n\nAykırı değerlerin tibble formatında okunaklı bir şekilde görüntülenmesi sağlanıyor.\n\n\n\n\nSonuçların Değerlendirilmesi: DBSCAN Yöntemi ve Aykırı Değerler\nŞu anki veri setimizde DBSCAN yöntemiyle 28 tane aykırı değere sahip firma tespit edilmiştir ve bu firmalardan 5 tanesi devlet yardımı almış firmalar arasındadır.\nDBSCAN yöntemi, yoğunluk tabanlı bir kümeleme algoritması olduğu için, veri setinde düşük yoğunlukta kalan firmaları aykırı olarak belirlemiştir. Bu durum özellikle küçük ölçekli firmalar, sınırlı sayıda ülkeye ihracat yapanlar ve ciro açısından diğerlerinden belirgin şekilde farklı olan firmalar için etkili olabilir. Ancak, DBSCAN veri yoğunluğu düşük bölgelerde kalan firmaları doğrudan aykırı kabul ettiği için, iş modelinden kaynaklı olarak farklı özellikler gösteren bazı firmaları da yanlışlıkla aykırı olarak işaretleyebilir.\nBu durum analiz sonuçlarını etkileyebilir çünkü:\n\nEğer tespit edilen aykırı firmalar aşırı yüksek ihracat yapan firmalar ise, devlet yardımı alan firmaların ihracatlarının genelde arttığına dair yanıltıcı bir çıkarım yapılabilir.\nEğer aykırı firmalar düşük ihracat yapan veya ihracatı olmayan firmalar ise, devlet yardımı alan firmaların başarısız olduğu gibi hatalı bir sonuca varabiliriz.\nBazı firmalar model tarafından sadece yoğunluk farkından dolayı aykırı belirlenmiş olabilir, bu da analiz sonuçlarının yanlış yorumlanmasına neden olabilir.\nÖzellikle büyük cirolara sahip ancak düşük ihracat yapan firmalar model tarafından aykırı olarak işaretlenmiş olabilir, bu da firmanın iş modeline göre farklı değerlendirilmesi gerektiğini gösterir.\n\nDBSCAN yöntemi küme sayısını önceden belirlemeye gerek duymadığı için esnek bir model sunarken, eps ve minPts parametrelerinin uygun şekilde ayarlanması büyük önem taşır. Eğer eps değeri çok küçük seçilmişse çok fazla firma aykırı olarak belirlenebilir, minPts değeri çok büyükse bazı aykırı firmalar göz ardı edilebilir.\nAykırı Olmayan Gözlemler (DBSCAN)\n\n# Aykırı olmayan gözlemleri filtreleyerek temiz bir veri seti oluşturma\ndbscan_data_clean &lt;- data_multiple[!data_multiple$firma_id \n                                   %in% rownames(data_multiple_dbscan_outlier), ]\n\n# Yeni veri setini tibble formatında görüntüleyelim\nas_tibble(dbscan_data_clean)\n\n# A tibble: 124 × 7\n   firma_id ihracat devlet_yardimi     ciro ihracat_ulkesi calisan_sayisi\n      &lt;int&gt;   &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;          &lt;int&gt;          &lt;dbl&gt;\n 1        1 443952. 1              2029875.             21            222\n 2        2 476982. 0              1647702.             20            202\n 3        3 655871. 1              1641391.             16            172\n 4        4 507051. 1              2442325.             12            186\n 5        5 512929. 0              1492204.             32            196\n 6        7 546092. 0              1954840.             30            206\n 7        8 373494. 0              2107269.             46            126\n 8        9 431315. 0              1630736.             11            162\n 9       10 455434. 0              1712806.             18            135\n10       11 622408. 0              1341492.             31            194\n# ℹ 114 more rows\n# ℹ 1 more variable: faaliyet_yili &lt;int&gt;\n\n\nSonuç: DBSCAN Kullanmalı mıyım?\n\n\n\n\n\n\nAvantajları\nDezavantajları\n\n\n\nÖnceden belirlenmiş küme sayısına ihtiyaç duymaz, veri yapısına göre esnek çalışır.\nParametre seçimi (eps ve minPts) sonuçları büyük ölçüde etkiler, yanlış seçilirse ya çok fazla ya da çok az aykırı değer belirlenebilir.\n\n\nYoğunluk bazlı olduğu için farklı şekillerdeki kümeleri tespit edebilir ve doğrusal olmayan ilişkileri yakalayabilir.\nDüşük yoğunluklu veri setlerinde veya eşit dağılıma sahip verilerde hatalı sonuçlar verebilir, çünkü aykırı değerleri yalnızca yoğunluk farklarına dayalı olarak belirler.\n\n\nAykırı değerleri, düşük yoğunluklu bölgelerde kalan noktalar üzerinden belirler, böylece aşırı uç noktaları daha doğal bir şekilde tespit edebilir.\nAykırı değerlerin neden aykırı olduğu konusunda doğrudan bir yorum yapmaz, yalnızca yoğunluk bazlı aykırılık tespiti yapar.\n\n\nBüyük veri setlerinde iyi çalışır ve kümeler içindeki aykırı değerleri belirlemede başarılıdır.\nYüksek boyutlu veri setlerinde performansı düşebilir, çünkü mesafe hesaplamaları daha karmaşık hale gelir.\n\n\n\nÖneri\nEğer veri yoğunluk farklılıkları gösteriyorsa ve kümeler arasında doğal ayrımlar varsa, DBSCAN etkili bir aykırı değer tespit yöntemi olabilir. Ancak, eps ve minPts değerlerinin dikkatlice belirlenmesi gerekir, aksi takdirde model gereğinden fazla veya yetersiz aykırı değer belirleyebilir. Yoğunluk bazlı aykırılık analizi yapmak istiyorsanız, Mahalanobis mesafesi veya Isolation Forest yerine DBSCAN tercih edilebilir.\nVeri Setimiz İçin Uygunluk Değerlendirmesi\nDBSCAN, yoğunluk bazlı çalıştığı için firmalar arasında belirgin yoğunluk farkları varsa iyi bir sonuç verebilir. Ancak, firmaların büyüklükleri ve iş modelleri doğal olarak farklı olduğu için, DBSCAN’in bazı firmaları yanlışlıkla aykırı olarak belirleme riski vardır.\nSonuç olarak, DBSCAN yöntemi veri setimiz için doğrudan uygun olmayabilir. Eğer firmalar arasında net bir yoğunluk farkı yoksa, Isolation Forest veya Mahalanobis Mesafesi gibi yöntemler daha güvenilir sonuç verebilir.\n\n3.2.5.4 DBSCAN, Isolation Forest ve Mahalanobis Mesafesi Karşılaştırması\n\n\n\n\n\n\n\n\nKriter\nDBSCAN\nIsolation Forest\nMahalanobis Mesafesi\n\n\n\nDağılım Varsayımı\nYok, yoğunluk tabanlı çalışır.\nYok, veri yapısından bağımsız çalışır.\nNormal dağılım varsayar, çarpık veri setlerinde sorun olabilir.\n\n\nDoğrusal Olmayan İlişkileri Yakalama\nYüksek, farklı küme yapılarını tanıyabilir.\nYüksek, karmaşık ilişkileri yakalayabilir.\nDüşük, doğrusal ilişkilere dayalıdır.\n\n\nBüyük Veri Setlerinde Performans\nOrta, yüksek boyutlu veri setlerinde yavaşlayabilir.\nYüksek, büyük veri setleri için ölçeklenebilir.\nOrta, yüksek boyutlarda kovaryans matrisinin hesaplanması zorlaşabilir.\n\n\nAykırı Değer Tanımlama Yöntemi\nDüşük yoğunlukta kalan noktaları aykırı kabul eder.\nİzolasyon skoru ile veri noktalarını sıralar.\nVerinin merkezine olan uzaklığı ölçerek belirler.\n\n\nAçıklanabilirlik\nDüşük, neden aykırı olduğunu doğrudan açıklayamaz.\nOrta, skor üretir ama sebebi doğrudan açıklayamaz.\nYüksek, matematiksel olarak yorumlanabilir.\n\n\nParametre Hassasiyeti\nYüksek, eps ve minPts yanlış seçilirse yanıltıcı olabilir.\nOrta, ntrees ve threshold ayarlanmalıdır.\nOrta, eğer kovaryans matrisinde tekillik varsa yanlış sonuç verebilir.\n\n\n\nDevlet yardımlarının ihracata etkisini analiz etmek için en doğru yöntemin seçilmesi gerekir, çünkü yanlış belirlenen aykırı değerler analiz sonuçlarını saptırabilir.\n\nDBSCAN yöntemi, firmalar arasında yoğunluk farkına dayalı olarak aykırı değerleri belirlediği için, devlet yardımı alan ve almayan firmaların doğal yoğunluk farklarını yanlış bir şekilde aykırı değer olarak işaretleyebilir. Bu yüzden DBSCAN bizim analizimiz için uygun değildir.\nMahalanobis Mesafesi yöntemi, çok değişkenli normal dağılım varsayımı ile çalışır, ancak bizim veri setimizde değişkenlerin normal dağılmaması bu yöntemin güvenilirliğini düşürebilir. Ayrıca, iş dünyasında büyüklük farkları doğrusal olmayabileceği için Mahalanobis Mesafesi bazı büyük ölçekli firmaları hatalı aykırı olarak belirleyebilir.\nIsolation Forest, veri dağılımına bağımlı olmadan, karmaşık ilişkileri tespit edebilir ve büyük veri setlerinde ölçeklenebilir bir yöntemdir. Firmaların farklı büyüklüklerde olmasını dikkate alarak aykırı değerleri belirler. Bu nedenle, devlet yardımlarının ihracata etkisini değerlendirirken en uygun yöntem olarak öne çıkmaktadır.\n\n3.2.6 Aykırı Değerlerle Ne Yapılmalı?\nAykırı değerler tespit edildikten sonra şu kararlar alınmalıdır:\n\n3.2.6.1 Aykırı Değerlerin Dönüştürülmesi\nNe zaman kullanılır?\n\nEğer aykırı değerleri tamamen silmek istemiyorsak ama etkilerini azaltmak istiyorsak.\nEğer veri sağa çarpık ise (pozitif çarpıklık varsa).\n\nUygulanacak Yöntemler:\n\n\n\n\n\n\n\nDurum\nÖnerilen Yöntem\nR Uygulaması\n\n\n\nVeri sağa çarpık mı?\nLog dönüşümü\nlog(veri$deger)\n\n\nVeri orta derecede çarpık mı?\nKarekök dönüşümü\nsqrt(veri$deger)\n\n\nAykırı değerler belli bir aralıkta tutulmalı mı?\nWinsorization\nWinsorize(veri$deger, probs=c(0.05, 0.95))\n\n\n\n3.2.7 Aykırı Değerlerin İmpute Edilmesi\nNe zaman kullanılır?\n\nEğer veri kaybetmek istemiyorsak.\nEğer eksik veri yönetimi stratejileri ile uyumlu bir yaklaşım gerekiyorsa.\n\nUygulanacak Yöntemler:\n\n\n\n\n\n\n\nDurum\nÖnerilen Yöntem\nR Uygulaması\n\n\n\nVerinin merkezi eğilimi korunmalı mı?\nMedyan ile doldurma\nmedian(veri$deger, na.rm = TRUE)\n\n\nVerinin değişkenliği korunmalı mı?\nRandom Forest Imputation\nmissForest::missForest(veri)\n\n\nVeriye yakın değerler baz alınmalı mı?\nkNN İmputation\nDMwR::knnImputation(veri)\n\n\n\n3.2.8 Sonuç ve Özet\n\nÖncelikle veriyi inceleyin (Boxplot, Histogram, Özet İstatistikler).\nEğer tek değişkenli analiz gerekiyorsa, IQR, Z-Skoru veya MAD yöntemlerini kullanın.\nEğer çok değişkenli analiz gerekiyorsa, Mahalanobis Mesafesi, İzolasyon Ormanı veya DBSCAN yöntemlerini tercih edin.\nAykırı değerleri temizlerken, silme, dönüştürme veya impute etme seçeneklerini duruma göre belirleyin.\n\nReferanslar\nhttps://naniar.njtierney.com/\nhttps://www.rdocumentation.org/packages/mice/versions/3.17.0/topics/mice\nhttps://choonghyunryu.github.io/dlookr/ https://rpubs.com/chibueze99/MissingR\nhttps://stefvanbuuren.name/fimd/\nhttps://rmisstastic.netlify.app/tutorials/josse_bookdown_dataanalysismissingr_2020\nhttps://rpubs.com/rpatel40/handling_missing_data_in_R\nhttps://www.youtube.com/watch?v=Akb401i32Oc&ab_channel=yuzaRDataScience\nhttps://ravenfo.com/2021/02/11/aykiri-deger-analizi/",
    "crumbs": [
      "Veri Dönüşümleri",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Veri Temizleme</span>"
    ]
  },
  {
    "objectID": "veri_donusum.html",
    "href": "veri_donusum.html",
    "title": "Veri Dönüşümleri",
    "section": "",
    "text": "Etki değerlendirme süreçleri, bir politika ya da müdahalenin hedeflenen sonuçlara ulaşıp ulaşmadığını belirlemek amacıyla yürütülen sistematik analizleri kapsar. Bu süreçlerin güvenilir olması, doğru yöntemlerin uygulanması kadar, kullanılan veri setlerinin eksiksiz, tutarlı ve analiz için uygun hale getirilmesine de bağlıdır. Veri analizine başlamadan önce, veri manipülasyonu ve veri temizleme aşamalarının dikkatlice yürütülmesi, elde edilen sonuçların doğruluğunu artırarak karar alıcıların daha sağlıklı değerlendirmeler yapmasına olanak tanır. Eksik veya yanlış işlenmiş verilerle yürütülen analizler, politika etkilerinin hatalı hesaplanmasına ve yanıltıcı çıkarımlara neden olabilir.\nVeri manipülasyonu, ham verinin analiz için uygun hale getirilmesi sürecini kapsayan işlemleri ifade eder. Sütun seçimi, satır filtreleme, değişken dönüştürme ve verinin istatistiksel olarak özetlenmesi gibi işlemler, veri setinin daha anlamlı ve analiz edilebilir hale getirilmesini sağlar. Manipülasyon aşaması, özellikle büyük veri setlerinde gereksiz veya tekrarlayan bilgilerin çıkarılması, eksik gözlemlerin yönetilmesi ve veri yapısının modelleme için optimize edilmesi gibi kritik adımları içermektedir.\nBununla birlikte, veri analizinin güvenilirliği açısından veri temizleme süreci büyük önem taşımaktadır. Veri setlerinde eksik değerler, aykırı gözlemler, yanlış formatlanmış değişkenler veya hatalı kayıtlar bulunabilir. Bu tür veriler, analiz süreçlerini olumsuz etkileyerek yanlış sonuçlara yol açabilir. Eksik verilerin doldurulması, aykırı değerlerin yönetilmesi ve verinin standardizasyonu gibi işlemler, analiz sürecinin daha sağlıklı ilerlemesini sağlar. Veri temizleme aşamasında eksik değerlerin nasıl ele alınacağı, hatalı girişlerin nasıl tespit edileceği ve veri türlerinin nasıl dönüştürüleceği gibi konular titizlikle ele alınmalıdır.\nBu çalışma, etki değerlendirme süreçlerinde veri manipülasyonu ve veri temizleme adımlarının nasıl gerçekleştirileceğini ele alarak, R programlama dili ve Quarto platformu aracılığıyla uygulanabilir bir rehber sunmayı amaçlamaktadır. Eksiksiz, temizlenmiş ve analiz için optimize edilmiş veri setleri, etki değerlendirme süreçlerinin güvenilirliğini artırarak politika yapıcıların daha sağlam kararlar almasına katkı sağlayacaktır.",
    "crumbs": [
      "Veri Dönüşümleri"
    ]
  }
]